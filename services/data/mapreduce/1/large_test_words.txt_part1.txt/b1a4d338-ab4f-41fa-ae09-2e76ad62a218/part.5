≠ô√Ã3u¢S∏qi˜Ifb]ÎπæfÆ^G	Ä#[Int is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and kåÙ`KÊöU?,ÙóÒí1 ÎüªRîæÈ÷ñûÆË[œºnown to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detectioAµœéBÑz◊·u◊·SåO%îﬁ˜~É}¬ a‘_œ.U_}n 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyzeù˚óîúY“ΩœgÌxÆgπ
˙ÌúØ’”Îér*®A the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign
M A N N I N G
Dominik Tornow
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
A distributed system is a collection of collaborating concurrent components that communicate
by sending and receiving messages over a network. Each component has exclusive access to its own
local state, which other components cannot access. The network has exclusive access to
its own local state, including messages that are in flight.
State State' State'' ...
Step Step Step
The behavior of the system is represented as a sequence of states, with each step transitioning
the system from one state to the next. Each step is taken by either a component
or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Think Distributed Systems

M A N N I N G
Shelter ISland
Dominik Tornow
Think Distributed Systems
For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
For more information, please contact
Special Sales Department
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
Email: orders@manning.com
¬© 2025 Manning Publications Co. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the
publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed
as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning‚Äôs policy to have the books
we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our
responsibility to conserve the resources of our planet, Manning books are printed on paper that is at
least 15 percent recycled and processed without the use of elemental chlorine.
‚àû
Manning Publications Co.
20 Baldwin Road
PO Box 761
Shelter Island, NY 11964
ISBN 9781633436176
Printed in the United States of America
The author and publisher have made every effort to ensure that the information in this book was correct
at press time. The author and publisher do not assume and hereby disclaim any liability to any party for
any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result
from negligence, accident, or any other cause, or from any usage of the information herein.
Development editor: Becky Whitney
Technical editor: Arun Saha
Review editor: Radmila Ercegovac
Production editor: Andy Marinkovich
Copy editor: Keir Simpson
Proofreader: Jason Everett
Typesetter: Tamara ≈†veli√ä Sablji√ä
Cover designer: Marija Tudor
To Jihyun
vi
brief contents
1 ‚ñ† Thinking in distributed systems: Models, mindsets, and
mechanics 1
2 ‚ñ† System models, order, and time 23
3 ‚ñ† Failure tolerance 40
4 ‚ñ† Message delivery and processing 55
5 ‚ñ† Transactions 67
6 ‚ñ† Distributed transactions 79
7 ‚ñ† Partitioning 90
8 ‚ñ† Replication 106
9 ‚ñ† Consistency 117
10 ‚ñ† Distributed consensus 131
11 ‚ñ† Durable executions 145
12 ‚ñ† Cloud and services 156
vii
contents
preface xii
acknowledgments xiii
about this book xiv
about the author xvi
about the cover illustration xvii
1 Thinking in distributed systems: Models, mindsets, and
mechanics 1
1.1 Software engineering and mental models 2
Mental models: The foundation of reasoning 3 ‚ñ† Correct mental
models 4 ‚ñ† Complete mental models 4
1.2 Mental model of software systems 4
1.3 Different types of models 5
Different models describing the same aspects 5
Different models describing different aspects of a system 6
1.4 Thinking about distributed systems 8
Correctness 9 ‚ñ† Scalability and reliability 12
Responsiveness 12
1.5 Two big ideas 13
Systems of systems 13 ‚ñ† Global view vs. local view 15
1.6 Distributed Systems Incorporated 16
viii contentsviii
1.7 Navigating complexity 18
Simple yet complex 18 ‚ñ† Emergent behavior 19
Changing perspective 19 ‚ñ† Think globally; act locally 20
1.8 Thinking above the code 21
2 System models, order, and time 23
2.1 System models 23
Theory and practice 24 ‚ñ† Synchronous distributed systems 25
Asynchronous distributed systems 25 ‚ñ† Partially synchronous
systems 26 ‚ñ† Component and network behavior 27
Realistic system models 30
2.2 Order and time 30
The happened-before relationship 32 ‚ñ† Time and clocks 35
Physical time and physical clocks 35 ‚ñ† Logical time and logical
clocks 37 ‚ñ† Physical clocks vs. logical clocks 38
3 Failure tolerance 40
3.1 In theory 41
3.2 Types of failure tolerance 42
Masking failure tolerance 42 ‚ñ† Nonmasking failure tolerance 42
Fail-safe failure tolerance 43 ‚ñ† None of the above 43
3.3 In practice 44
System model 44 ‚ñ† Failure handling 46 ‚ñ† Failure
classification 47 ‚ñ† Failure detection 50 ‚ñ† Failure
mitigation 52 ‚ñ† Putting everything together 52
4 Message delivery and processing 55
4.1 Exchanging messages 56
4.2 The uncertainty principle of message delivery and
processing 58
Before sending the request 59 ‚ñ† After sending the request and
before receiving a response 59 ‚ñ† After receiving a response 59
4.3 Silence and chatter 60
4.4 Exactly-once processing semantics 62
4.5 Idempotence 62
4.6 Case study: Charging a credit card 64
ixcontents ix
5 Transactions 67
5.1 Abstractions 68
5.2 The magic of transactions 70
Concurrency 70 ‚ñ† Failure 71
5.3 The model of transactions 72
Correctness 74 ‚ñ† Serializability 75 ‚ñ† Completeness 77
Application-level abort 77 ‚ñ† Platform-level abort 78
6 Distributed transactions 79
6.1 Atomic commitment: From a single RM to multiple RMs 79
Transaction on a single RM 81 ‚ñ† Transaction on multiple
RMs 81 ‚ñ† Blocking and nonblocking 82
6.2 The essence of distributed transactions 82
6.3 Two-Phase Commit protocol 83
In the absence of failure 83 ‚ñ† In the presence of failure 85
Improvement 86
7 Partitioning 90
7.1 Encyclopedias and volumes 90
7.2 Thinking in partitions 92
7.3 The mechanics of partitioning and balancing 93
7.4 (Re)partitioning 94
Types of partitioning 94 ‚ñ† Data item to partition assignment
strategies 97
7.5 Common item-based assignment strategies 99
Range partitioning 100 ‚ñ† Hash partitioning 100
7.6 Repartitioning 101
Range partitioning 101 ‚ñ† Hash partitioning 102
7.7 Consistent hashing 103
7.8 (Re)balancing and overpartitioning 103
8 Replication 106
8.1 Redundancy 107
8.2 Thinking about replication and consistency 109
8.3 Replication 110
x contentsx
8.4 The mechanics of replication 111
System model 111 ‚ñ† Replication lag 112 ‚ñ† Synchronous
vs. asynchronous replication 113 ‚ñ† State-based vs. log-based
replication 114 ‚ñ† Single-leader, multileader, and leaderless
systems 114
9 Consistency 117
9.1 Consistency models 117
Common consistency models 119 ‚ñ† Virtues and limitations 119
9.2 Linearizability 121
Queue and stack 122 ‚ñ† Formal definition of linearizability 123
9.3 Eventual consistency 124
The shopping cart 124 ‚ñ† Variants of eventual consistency 125
Implementation 125
9.4 Consistency, availability, and partition tolerance 126
History 126 ‚ñ† Conjecture vs. theorem 127
CAP theorem 128
10 Distributed consensus 131
10.1 The challenge of reaching agreement 132
10.2 System model 132
10.3 State machine replication 133
10.4 The origin‚Äîand irony‚Äîof consensus 134
10.5 Implementing consensus 135
Leader-based consensus 135 ‚ñ† Quorum-based consensus 136
Combining leader and quorum 136
10.6 Raft 136
The log 137 ‚ñ† Terms 138 ‚ñ† Leader Election protocol 139
Log Replication protocol 140 ‚ñ† State machine safety 141
10.7 Raft puzzles 141
Puzzle 1 141 ‚ñ† Puzzle 2 142 ‚ñ† Puzzle 3 142
11 Durable executions 145
11.1 The pitfalls of partial executions 145
11.2 System model 147
Process definition 148 ‚ñ† Process execution 148
xicontents xi
11.3 The concept of failure-transparent recovery 149
11.4 Strategies of failure-transparent recovery 150
Restart 150 ‚ñ† Resume 151
11.5 Implementation of failure-transparent recovery 152
Application-level implementation: Sagas 152
Platform-level implementation: Durable execution 153
12 Cloud and services 156
12.1 From proactive to reactive 157
12.2 Cloud computing 157
12.3 Cloud-native computing 158
12.4 Serverless computing 159
Traditional 160 ‚ñ† Serverless 160 ‚ñ† Cold path vs. hot path 161
12.5 Service 161
Global view vs. local view 162 ‚ñ† Example recommendation
service 163
12.6 Final thoughts 166
index 168
xii
preface
As an engineer, I am never more confident than when I truly understand a complex
concept. When you deeply understand a system‚Äôs underlying mechanics, you can
explain where the system is coming from and predict where the system is going.
Over the course of my career working with distributed systems, I‚Äôve focused on sys-
tems thinking and modeling to develop dependable mental models‚Äîmodels that cut
through doubt and bring distributed systems into focus.
In this book, I want to share the mental models I‚Äôve developed over 20 years. More
important, I want to show you how to construct your own, replacing confusion with clar-
ity, and hesitance with confidence so that you can experience the deep satisfaction of
distributed systems engineering along with me.
xiii
acknowledgments
To my wife, Jihyun, whose encouragement and dedication made this book possible.
Thank you for being on this journey with me.
To my friends, who listened to me talk about distributed systems far more than any-
one should have to and still wanted to spend time with me.
To my colleagues and collaborators, whose challenging and insightful discussions
over the years helped forge the mental models presented in these pages.
To the publishers, editors, and technical staff at Manning, whose commitment and
constructive feedback transformed rough ideas into clear explanations‚Äîthis book is
immeasurably better for your contributions.
To all the reviewers: Abhay Paroha, Advait P, Ajay Thakur, Alankrit Kharbanda, Alok
Ranjan, Anu Engineer, Anurag Kumar Jain, Arijit Dasgupta, Arjun Chakraborty, Ash-
win Das Gururaja, Bhala Ranganathan, Bhavin Thaker, Bhupendra Singh, Carlos J.
Cela, Deep Bodra, Dinesh Chitlangia, Eric Normand, Hari Mani, Jayasekhar Konduru,
Karthik Penikalapati, Koushik Vikram, Krishna Kumaar, Luis Soares, Maneesh Karnati,
Manikandan Vellore Muneeswaran, Marcos Oliveira, Michaeljon Miller, Naga Rishy-
endar Panguluri, Narendra Reddy Sanikommu, Neha Shetty, Ori Pomerantz, Pradeep
Kumar Goudagunta, Prasann Pradeep Patil, Prit Sheth, Pronnoy Goswami, Raja Rao
Budaraju, Raju Ansari, Ramprasad C, Ravi Laudya, Ravi Soni, Ravi Teja Thutari, Saket
Chaudhari, Samarth Shah, Sandeep Guggilam, Sandhya Vinjam, Shankar, Shubham
Malhotra, Siddhartha Singh, Suresh Bysani Venkata Naga, Tharun Mothukuri, Tony
Bernardino, Vaibhav Mahindroo, Vaibhav Tupe, Vishal Chaurasia, VVS Sundeep
Akella, and Zhihao Yao; your thoughtful questions, comments, and evaluations pushed
me to strengthen every chapter.
xiv
about this book
Every modern application is a distributed system. Yet despite their ubiquity, distributed
systems remain notoriously difficult to understand, design, and implement. This book
bridges that gap, taking you from novice or intermediate to expert, by providing the
understanding and mental models you need to reason about and architect complex
distributed systems with confidence.
Although many resources teach specific technologies, frameworks, or platforms,
Think Distributed Systems focuses on enduring principles and patterns that go beyond any
single implementation. This book is not just another catalog of algorithms or protocols.
Instead, it teaches you to think like a distributed systems engineer, understanding the
core concepts and mechanics that define real-world distributed systems.
Who should read this book?
This book is for software engineers who want to build a solid foundation in distributed
systems. You don‚Äôt need experience; the journey begins from first principles and builds
toward advanced topics. If you‚Äôve worked with distributed systems but found yourself
frustrated by fuzzy mental models and vague intuitions, this book will transform uncer-
tainty into clarity and bring complex ideas into focus.
How this book is organized: A road map
The book is organized into 12 chapters, taking you from fundamental to advanced
concepts:
¬° Chapter 1 introduces distributed systems and their characteristics.
¬° Chapter 2 examines system models and assumptions about processes, networks,
and time.
xvabout this book xv
¬° Chapter 3 covers failure, failure tolerance, and failure transparency.
¬° Chapter 4 explores message delivery and message processing guarantees.
¬° Chapter 5 covers transactions and atomic commitment.
¬° Chapter 6 covers distributed transactions and protocols like Two-Phase Commit.
¬° Chapter 7 examines partitioning to improve scalability.
¬° Chapter 8 examines replication to improve reliability.
¬° Chapter 9 discusses consistency models and the CAP theorem.
¬° Chapter 10 explores state machine replication and distributed consensus.
¬° Chapter 11 discusses durable execution.
¬° Chapter 12 explores cloud computing, cloud-native computing, serverless com-
puting, and microservices.
About the code
This book deliberately features few code examples. The challenges in distributed sys-
tems arise from the interactions between components, not from the implementation
of one component. Instead of presenting code fragments and expecting you to deduce
the distributed behavior, I use system models and illustrations to show you exactly how
components communicate, collaborate, and compete, revealing the essential behav-
iors that characterize distributed systems with clarity.
liveBook discussion forum
Purchase of Think Distributed Systems includes free access to liveBook, Manning‚Äôs online
reading platform. Using liveBook‚Äôs exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It‚Äôs a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/think-distributed-systems/discussion.
Manning‚Äôs commitment to our readers is to provide a venue where meaningful dia-
logue between individual readers and between readers and the author can take place. It
is not a commitment to any specific amount of participation on the part of the author,
whose contribution to the forum remains voluntary (and unpaid). We suggest you try
asking the author some challenging questions lest their interest stray! The forum and
the archives of previous discussions will be accessible on the publisher‚Äôs website as long
as the book is in print.
xvi
about the author
Dominik Tornow is the founder and CEO of Resonate
HQ, Inc. He studied software systems engineering at
the Hasso Plattner Institute in Potsdam, Germany, and
brings more than two decades of industry experience to
his work. Dominik specializes in systems engineering,
systems thinking, and systems modeling with a focus
on formal and conceptual techniques that ensure that
concurrent distributed systems are correct by construc-
tion. He is a prolific writer, a frequent speaker at leading
industry conferences, and the holder of multiple soft-
ware patents.
xvii
about the cover illustration
The figure on the cover of Think Distributed Systems, captioned ‚ÄúLiburnienne,‚Äù or
‚ÄúLiburnian,‚Äù was published by Nepveu (Paris) in 1815 and is taken from a collection
provided by Biblioth√®que nationale de France. The Liburnians were an ancient tribe
inhabiting the district called Liburnia, a coastal region of the northeastern Adriatic
between the rivers Arsia (Ra≈°a) and Titius (Krka) in what is now Croatia. Each illustra-
tion is finely drawn and colored by hand.
In those days, it was easy to identify where people lived and what their trade or station
in life was just by their dress. Manning celebrates the inventiveness and initiative of the
computer business with book covers based on the rich diversity of regional culture cen-
turies ago, brought back to life by pictures from collections such as this one.

1
1Thinking in
distributed systems:
Models, mindsets,
and mechanics
This chapter covers
¬° The difference between knowing and
understanding
¬° The role of mental models
¬° The structure and behavior of a distributed
system
¬° The correctness, scalability, and reliability of a
distributed system
¬° The need to build distributed systems
Every modern application is a distributed application. Whether you are building
a web app, mobile app, wearable app, cloud service, or game, the question is not
whether an application is distributed but to what degree the application is distrib-
uted. A distributed system is a collection of collaborating, concurrent components that
communicate with one another by sending and receiving messages over a network:
¬° The behavior of a distributed system emerges from the behavior of its compo-
nents and their interactions.
2 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° The complexity of a distributed system emerges from the complexity of its compo-
nents and the intricacy of their interactions.
Why do we distribute applications? After all, distributed applications are notoriously
hard to build, so why go through the trouble?
As engineers, we assess the fitness of a system in terms of correctness, scalability, and
reliability. In other words, a system must provide its intended function even in the pres-
ence of increasing load and inevitable failures. Although a single component can be
functional, no single component can handle unbounded loads or survive inevitable
failures. We need more than one component, such as a process or node. We need a
distributed system.
True, distributed systems are complex, but for most software engineers, understand-
ing distributed systems is no longer optional. This book gives you the foundation to
reason about distributed systems with confidence so that you can build functional, scal-
able, and reliable distributed systems.
NOTE During my journey to understanding distributed systems, I had many
‚Äúaha!‚Äù moments of realization that left me excited and increased my confi-
dence. In this book, I‚Äôll share my aha! moments with you. I hope they‚Äôll be
helpful.
1.1 Software engineering and mental models
Software engineering can be a delightful profession; at the same time, it can be a
dreadful profession. Nowhere does this tension become more visible than in complex
distributed systems.
For me, software engineering is most dreadful when I experience confusion. Unfor-
tunately, in this industry, confusion is a constant state of mind. Other professions have
developed robust mental models and means of communication, but software engineer-
ing is still catching up. Civil engineering, for example, relies on mental models rooted
in physics, expressed mathematically, and communicated by standardized diagrams.
But in software engineering, we struggle with the concepts and terms we use every day.
Take a moment to ask an endless list of questions, such as ‚ÄúWhat is a microservice?‚Äù
‚ÄúWhat is the cloud?‚Äù ‚ÄúWhat is cloud-native,‚Äù and ‚ÄúWhat is serverless?‚Äù Do you know the
answers, and if you do, would other engineers agree with your notion?
Often, we have a fuzzy notion, a vague idea that we cannot communicate effectively
and efficiently. For me, this situation is a source of great frustration. I do not want to
probe the system, poke it with a stick, and then observe and rationalize its behavior only
for the next poke to inadvertently invalidate what I thought I knew.
I want an accurate, concise mental model‚Äîa concise internal representation that
enables me to reason confidently about the system. In this book, you will develop accu-
rate and concise mental models to reason confidently about distributed systems. You
will also learn to construct mental models to replace confusion with certainty and hesi-
tance with confidence.
3Software engineering and mental models
Aha! moment: From knowledge to understanding
Knowing is not the same as understanding. There is a difference between knowing
about complex distributed systems and understanding complex distributed systems.
The distinction between knowing and understanding is evident in many areas, such
as board games, in which learning the rules is straightforward but mastering the strat-
egies and tactics requires time and effort.
The game of chess perfectly showcases the divide between knowing and understand-
ing. Though it takes only a short amount of time to learn the rules of the game, it
takes much longer to truly understand the strategies and tactics necessary to excel
at it. This is why chess is often seen as a measure of intelligence, for both humans
and machines. Despite its simple setup of a board, six types of pieces, and two play-
ers, chess is a remarkably complex and sophisticated game.
The same concept applies to distributed systems. As we progress through this book,
our goal is to build not only a deeper knowledge but also a deeper understanding of
distributed systems. Our goal is to build dependable mental models.
1.1.1 Mental models: The foundation of reasoning
The book title Think Distributed Systems emphasizes the significance of mental models,
which are the very foundation of reasoning. Before we dive into the mental models of
distributed systems, let‚Äôs explore mental models themselves. What are mental models,
and what makes a good mental model?
A model is a representation of a target system. A mental model is the internal represen-
tation of the target system and the basis of comprehension (ability to understand) and
communication (ability to convey understanding).
Figure 1.1, for example, illustrates a simple electric circuit, with the system on the
right and the mental model‚Äîthe corresponding internal representation‚Äîon the left.
The mental model is the basis for our reasoning about the system.
3.0Œ©
1.5V
Mental model System
+
Figure 1.1 Mental model and system
A tangible way to think of mental models and systems is as a set of facts:
4 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° System‚ÄîA system can be understood as a set of facts that constitutes the ground
truth.
¬° Mental model‚ÄîA mental model can be understood as a set of facts that constitutes
our perceived truth.
A good mental model is a mental model that is correct and complete:
¬° Correctness‚ÄîIn a correct mental model, every fact of the model is a fact of the sys-
tem. In other words, a correct mental model does not contain falsehoods.
¬° Completeness‚ÄîIn a complete mental model, every relevant fact of the system is a fact
of the model. In other words, a complete mental model does not contain omissions.
1.1.2 Correct mental models
A correct mental model ensures that every fact of the model is a fact of the system it
represents. If the system illustrated in figure 1.1 consists of a light bulb with a resistance
of 3.0 ohms, and the model states that the light bulb has a resistance of 3.0 ohms,
the model is correct. If the model instead stated that the resistance was 2.0 ohms, the
model would be incorrect.
1.1.3 Complete mental models
No model captures every fact of a system. Therefore, we define completeness as captur-
ing every relevant fact of the system. Relevance is application-specific. In other words,
relevance lies in the eye of the beholder.
If I am interested in determining how much current will flow through the light bulb
illustrated in figure 1.1, the mental model is correct and complete. If, however, I am
interested in the brightness of the light bulb, the mental model is incomplete because I
am missing the luminous efficiency (ability to convert electrical energy to visible light).
1.2 Mental model of software systems
Software systems are virtual constructs, not physical constructs. We might think of
distributed software systems as a set of components that communicate by exchang-
ing messages via a network. However, these constructs are virtual; no matter where
we look, we won‚Äôt find them as physical constructs. Instead, a software system behaves
as though it were made from these constructs. The observable behavior of a software
system and our model of a software system are equivalent, but we will see multiple
equivalent models that don‚Äôt describe the observable behavior better than the others.
Different models are valid!
NOTE I could argue that software phenomena ultimately reduce to physical
phenomena, such as electrons moving in silicon. But I subscribe to Edward A.
Lee‚Äôs point of view in Plato and the Nerd: The Creative Partnership of Humans and
Technology (The MIT Press, 2017): ‚ÄúAlthough software is ultimately electrons in
silicon, there are so many layers between the physics and the software that the
connection to the physical becomes meaningless.‚Äù
5Different types of models
Beware the analogy
Analogies‚Äîcomparisons between two concepts based on perceived similarities‚Äîare
popular teaching and learning tools for mental models. Analogies promise transfer-
ability, allowing us to reason about a domain in terms of a familiar one. The analogy of
an electrical circuit being similar to water flowing through pipes, for example, can help
clarify concepts such as current, voltage, and resistance.
A word of caution, however: teachers who employ analogies typically are aware of
where the similarities end and the differences begin, but students may not have this
insight. This can lead to misconceptions, such as mistakenly believing that electricity
‚Äúflows‚Äù like water, which oversimplifies and distorts physical reality.
Although analogies can be useful for understanding a concept, by definition they are a
model of a different concept. Therefore, it is crucial to critically assess the limitations
of an analogy to avoid drawing incorrect conclusions.
1.3 Different types of models
Over the years, I have read extensively about distributed systems and have become
increasingly frustrated that every blog, book, and paper defines and describes distrib-
uted systems differently. I‚Äôve felt lost in a sea of inconsistencies and struggled to recon-
cile the differences.
This experience led me to realize that there is no single way to describe a distrib-
uted system. Instead, there are countless ways: informally or formally, for example, or
as action machines, state machines, state-action machines, or process algebras. The list
goes on and on.
This diversity adds complexity but also provides depth, with each mental model pro-
viding a unique perspective. Studying multiple mental models helps us gain a holistic
understanding and allows us to identify omissions or inconsistencies in our thinking.
1.3.1 Different models describing the same aspects
My first realization was that sometimes, different ways of describing a distributed sys-
tem are indeed equivalent because a description of one kind can be transformed into a
description of another kind (see figure 1.2).
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.2 Different
models describing the same
aspects of a system. (The
sets of facts for each model
completely overlap.)
6 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
According to our previous definition, a distributed system is a set of concurrent com-
ponents that communicate with one another by sending and receiving messages over
a network. We can view the network as a communication bus and a buffer to track
in-flight messages. We can also view the network only as a communication bus, with
each component having a buffer to track in-flight messages. These descriptions are
equivalent; both can express message loss, message duplication, and message reorder-
ing. On one hand, we can describe a distributed system with the network as the buffer
for in-flight messages (see figure 1.3).
State
S1
State
S3
State
S4
Component
C 1
Component
C 3
Component
C 5
Component
C 2
Component
C 4
Component
C 6
State
S2
State
S4
State
S6
Buffer
Network
Figure 1.3 The network as the buffer of in-flight messages
On the other hand, we can describe a distributed system with the components serving
as the buffer for in-flight messages (see figure 1.4).
Neither model is incorrect. Both models are capable of capturing the relevant
aspects of the distributed system, such as messages being in flight or messages get-
ting lost, duplicated, or rearranged. The choice of model often comes down to
preference.
1.3.2 Different models describing different aspects of a system
My second realization was that sometimes, different ways of describing a distributed sys-
tem are not equivalent because they have a different focus. Some descriptions express
certain aspects of a system that others deliberately omit (see figure 1.5).
7Different types of models
Component
C1
Component
C3
Component
C5
Component
C 2
Component
C 4
Component
C 6
Network
Buffer
S 5
Buffer
S 3
Buffer
S 1
Buffer
S 6
Buffer
S 4
Buffer
S 2
Figure 1.4 The components serve as the buffer for in-flight messages.
Aspects captured
by description 1
Aspects captured
by description 2
Figure 1.5
Different models
describing
different aspects
of a system.
(The sets of
facts for each
model partially
overlap.)
In his video course ‚ÄúIntroduction to TLA+, the Temporal Logic of Actions‚Äù (https://
lamport.azurewebsites.net/video/videos.html), Leslie Lamport describes the Trans-
action Commit protocol, which is an abstraction of all distributed transaction proto-
cols, and the Two-Phase Commit protocol, which is an implementation of a distributed
transaction protocol. (Chapter 6 explores distributed transaction protocols and the
Two-Phase Commit protocol in depth.)
The first model ignores message exchange, whereas the second model takes message
exchange into account. As a result, the first model is unable to represent messages that
are lost, duplicated, or rearranged, and the second model is able to do so.
NOTE In chapters 6 and 10, we will delve deeper into distributed transaction
protocols, consensus protocols, and different models of distributed systems,
examining their respective characteristics in greater detail.
8 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
The blind men and the elephant
The well-known parable of the blind men and the elephant is often used as an allegory
to illustrate the importance of considering multiple perspectives when trying to under-
stand a complex or abstract topic. In this story, six blind men come across an ele-
phant for the first time. Each person touches a different part of the elephant‚Äîsuch
as the trunk, ear, or tail‚Äîand each describes the elephant based on the part they
touched. The moral of the story is that each person‚Äôs understanding of the elephant
is incomplete and limited by their perspective; we can gain a more complete under-
standing only by considering all the perspectives together.
In the same way, studying multiple mental models or different ways of thinking can
help you gain a more holistic understanding of the topic. Also, being exposed to multi-
ple perspectives can help you identify misconceptions in your thinking.
If you encounter different models of a distributed system, that doesn‚Äôt mean that some
of them are incorrect. When you read posts, articles, or papers, try to understand not
only the model chosen by the author but also the reasons behind their choice. What
makes the model suitable for the point the author is trying to convey?
Please keep in mind that this approach can be quite challenging and even outright
frustrating. You may have to set aside your hard-earned mental models in favor of an
author‚Äôs mental model. But you will gain a more holistic understanding in return.
1.4 Thinking about distributed systems
I‚Äôll begin this section with an informal discussion of building a comprehensible mental
model of distributed systems and their mechanics. In later chapters, I‚Äôll formally discuss
building a comprehensive mental model of distributed systems and their mechanics.
A distributed system is a collection of collaborating, concurrent components that com-
municate with one another by sending and receiving messages over a network. Each
component has exclusive access to its own local state, which other components cannot
access. The network has exclusive access to its own local state, including messages that
are in flight (see figure 1.6).
An accurate, concise way to think about the behavior of a distributed system is as a state
machine (also called a state action machine if the author intends to highlight the steps that
transition between states). The behavior of the system is represented as a sequence of
states, with each step transitioning the system from one state to the next (see figure 1.7).
In the most general terms, the system proceeds in discrete steps. Each step is taken by
either a component or the network. These steps can be categorized as follows:
¬° External steps‚ÄîActions such as receiving a message or sending a message
¬° Internal steps‚ÄîActions such as performing local computations or accessing the
local state
To summarize, we will think about the system as one in which at any moment, exactly
one component or the network will complete exactly one step.
9Thinking about distributed systems
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Figure 1.6 A distributed system as a set of concurrent, communicating components (local state of
network not shown)
State State' State'' ...
Step Step Step
Figure 1.7 Behavior of a system as a sequence of states
NOTE In various blog posts, papers, and books, components are often referred
to as actors, agents, processes, or nodes. Similarly, steps are often referred to as
actions or events.
1.4.1 Correctness
As I progressed in my career, I realized that my understanding of the correctness of
a software system was fuzzy at best: a system was correct if it did what it was supposed
to do (of course). But this definition provides no objective criteria for determining
whether a system is correct. I wanted to be able to think rigorously about the system‚Äôs
guarantees, capabilities, and limitations.
Leslie Lamport coined my favorite definition in ‚ÄúProving the Correctness of Mul-
tiprocess Programs‚Äù (https://lamport.azurewebsites.net/pubs/proving.pdf). Lamport
defines the correctness of a system in terms of its safety and liveness properties:
10 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Safety‚ÄîInformally, a safety property guarantees that something bad will never
happen.
¬° Liveness‚ÄîInformally, a liveness property guarantees that something good will
eventually happen.
A system is correct if every possible behavior of the system is correct‚Äîthat is, if every
possible behavior meets the system‚Äôs safety and liveness guarantees (see figure 1.8).
Safety Liveness
All possible behaviors
All behaviors
satisfying safety
All behaviors
satisfying liveness
All correct behaviors, including
scalability and reliability
Figure 1.8 Safety and liveness
In the context of distributed transactions, a safety guarantee ensures that no two par-
ticipants in a transaction arrive at a conflicting decision, whereas a liveness guarantee
asserts that every participant will eventually arrive at a decision. Let‚Äôs assume that a dis-
tributed transaction spans two participants, which are two key value stores: P1 and P2.
Listing 1.1 A distributed transaction
Begin Transaction
AT participant P1 SET b TO "foo"
AT participant P2 SET a TO "bar"
Commit Transaction
Figure 1.9 illustrates the behavior space‚Äîthat is, all possible behaviors of the distrib-
uted transaction. Here, a state reflects the status of a transaction at participants P1 and
P2. The transaction can be in one of three possible states: working, committed, and
11Thinking about distributed systems
aborted. A state transition updates one participant from working to committed or
aborted (remember‚Äîexactly one component, exactly one step at a time).
Violates liveness
(‚Äúcan‚Äôt stay here‚Äù)
Violates safety
(‚Äúmust not get here‚Äù)
P1 = working
P 2 = working
P1 = committed
P 2 = committed
P1 = committed
P 2 = working
P1 = aborted
P 2 = aborted
P 1 = aborted
P 2 = working
P1 = working
P 2 = committed
P1 = working
P 2 = aborted
P1 = aborted
P 2 = committed
P 1 = committed
P 2 = aborted
Figure 1.9 Behavior space of a distributed transaction with two participants
The safety guarantee of the system asserts that neither P1 nor P2 will commit while the
other aborts. The liveness guarantee asserts that both P1 and P2 will eventually com-
mit or abort. The combination of safety and liveness asserts that eventually, P1 and P2
will reach the same decision, either committing or aborting. In other words, the safety
guarantee prevents the system from reaching an inconsistent state, and the liveness
guarantee prevents the system from getting stuck.
At first, I found the concept of safety and liveness guarantees to be overly academic
(which is a polite way of asking ‚ÄúCould you be more annoying?‚Äù). But I quickly came to
appreciate the value of safety and liveness as a thinking tool that allowed me to reason
about systems effectively and efficiently.
12 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
NOTE Consider how we think about the system. We have two key-value stores,
yet we do not think in terms of keys and values. We are interested only in the
status of a transaction at participants P1 and P2 and ignore all other aspects. We
ignore the keys involved in the transaction, for example, and we ignore the
values that are proposed by the transaction. We could think about the system
in terms of keys and values, of course, and we could include other aspects,
such as locks on the keys involved in the transaction. By presenting this model,
I‚Äôm asserting that other aspects are irrelevant to this discussion. You may dis-
agree. Neither of us is wrong; our mental models, focus, and interests are simply
different.
1.4.2 Scalability and reliability
In ‚ÄúDefining Liveness‚Äù (https://mng.bz/jZdz), Bowen Alpern and Fred Schneider
provide a proof that every property of a system is an intersection of a safety property
and a liveness property. But I do not think about all aspects of a system in terms of
safety and liveness. That is, I do not think about scalability and reliability guarantees in
terms of safety and liveness guarantees.
That situation is not uncommon: different mental models compete for our atten-
tion. Even when these mental models are equivalent, capable of expressing the same
aspects but in different ways, we often gravitate toward one over the other. So I find
myself thinking about scalability and reliability guarantees in terms of responsiveness,
informally defined as a system‚Äôs ability to meet its service-level objectives:
¬° Scalability of a system is defined as its ability to be responsive in the presence of
load.
¬° Reliability of a system is defined as its ability to be responsive in the presence of
failure.
1.4.3 Responsiveness
Responsiveness is formally defined by four related concepts:
¬° Service-level indicator‚ÄîA quantitative observation about the behavior of a system
¬° Service-level objective‚ÄîA predicate (a function that yields true or false) on a
service- level indicator that determines whether the behavior of a system meets
an objective
¬° Error rate‚ÄîThe ratio of the number of observations that do not meet their objec-
tives to the total number of observations for a fixed time interval
¬° Error budget‚ÄîAn acceptable (to us) upper limit on the error rate
Based on these factors, we can define the responsiveness of a system as the ability of the
system to keep its error rate below its error budget.
TIP For detailed discussions of these concepts, I recommend the Google Site
Reliability Engineering book collection (https://sre.google/books).
13Two big ideas
Aha! moment: Application-specific
Correctness guarantees, including scalability and reliability, are application-specific.
As a software engineer, you are able to define the guarantees of your system and
decide what behavior is desirable, tolerable, or intolerable. In other words, what you
may consider to be broken, I may consider to be acceptable.
Aha! moment: Emergent
Correctness guarantees, including scalability and reliability, are emergent properties:
correctness, scalability, and reliability don‚Äôt trace back to an individual component. A
single component cannot mitigate infinite load, and a single component cannot miti-
gate a single crash failure. Instead, scalability and reliability are the result of a set of
components and their interactions.
1.5 Two big ideas
At this point, I want to emphasize two big ideas that have greatly influenced my think-
ing about distributed systems: systems of systems and global versus local viewpoints.
These concepts will serve as the foundation for a clear, precise definition of services
and, dare I say, microservices later in this book.
1.5.1 Systems of systems
A distributed system is a set of concurrent, communicating components. Though it
doesn‚Äôt explicitly say so, I argue that this definition motivates us to think of compo-
nents as atomic‚Äîthat is, indivisible entities. In software systems, however, most compo-
nents are not atomic entities but higher-order entities, or subsystems (see figure 1.10).
State of
C 1
State of
C 3
State of
C 5
C 1
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Network
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Subsystem
Figure 1.10 A distributed system as a set of concurrent, communicating subsystems
14 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
How should we approach the idea that a group of concurrent and communicating
components exhibits a specific behavior and that when it‚Äôs viewed at a higher level
of abstraction, the group behaves as though it were a single component? In his 1967
book The Ghost in the Machine, Hungarian philosopher Arthur Koestler introduced the
term holon to describe an entity that is both whole in itself and part of a larger whole.
Depending on the level of observation, a holon can be viewed as either an atomic
entity or an organization of holons, a higher-order entity. This hierarchical structure
composed of holons is referred to as a holarchy (see figure 1.11).
A whole in
and of itself Also, looking up, composing
into higher-level holons
Also, looking down, composing
into lower-level holons
‚ÄúMessy‚Äù hierarchy
Figure 1.11 Holons and holarchies
Holons and holarchies can be found in various fields, such as physics, biology, and
sociology, and are often used to describe complex systems composed of smaller, inter-
connected parts. Though they‚Äôre not as common, I‚Äôve found holons and holarchies
to be incredibly useful tools for thinking about distributed systems. Most complex
distributed systems lack clean-cut hierarchies; instead, they exhibit complex forms of
organization. Holons and holarchies can express these complex and ‚Äúmessy‚Äù forms
of organization, allowing us to group, zoom in, or zoom out to capture the relevant
aspects of a system accurately and concisely.
As an example, let‚Äôs consider a multitenant replicated database management system.
This database management system replicates data through a consensus algorithm, such
as Paxos, Raft, or Viewstamped Replication. Let‚Äôs assume that the system has two ten-
ants, each of which is provisioned one database, and three database nodes.
15Two big ideas
Depending on our perspective or the point we want to make, we may view the system
in different ways. On one hand, we may think in terms of one atomic entity: the data-
base cluster. Alternatively, we may think of the database cluster as a higher-order entity
composed of three interacting database nodes. On the other hand, we may think in
terms of two atomic entities: two databases, one per tenant. Alternatively, we may think
of each database as a higher-order entity composed of the interacting database nodes.
We choose which components to consider atomic or higher-order entities and which
components to include in our analysis. This choice can vary even for the same distrib-
uted system, depending on the context (see figure 1.12).
Database
nodes
Database
cluster
Database
nodes
Database
(tenant 1)
Database
(tenant 2)
Figure 1.12 Two different holarchies representing the same system
1.5.2 Global view vs. local view
Let‚Äôs return to the idea of a global view and a local view. Recall that we are considering
the system from the perspective of an all-knowing observer, meaning that we are able
to observe the state of the entire system: we have a global view. A component does not
have the same luxury; a component can observe only its own state and its own channel
to the network, giving it a limited local view. From a global point of view, we can observe
the entire system‚Äîthat is, determine the system state (see figure 1.13).
State of
C 1 C 1 C 2 State of
C 2
Network
Figure 1.13 Global point of view
16 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
From a local point of view, a component can observe only itself‚Äîthat is, determine
only its own state. To determine a global system state, a component must have the
cooperation of other components that must send their recorded local states via the
network (see figure 1.14).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 1.14 C1‚Äôs point of view
Typically, we think about communication as a direct message exchange between two
components: C1 and C2. It is important to understand, however, that C1 is not com-
municating directly with C2 but with the rest of the system. C1 sends to the network a
message that is addressed to C2. The network may or may not forward the message to
C2 or any other component.
1.6 Distributed Systems Incorporated
As we dive into the topic of distributed systems, I encourage you to picture them as a
corporation situated in an office building (see figure 1.15). We‚Äôll call this entity Dis-
tributed Systems Incorporated.
The building represents the system, with each room representing a concurrent
(independent) component. (We will revisit concurrency in greater detail later in the
book.) These rooms are connected indirectly by pneumatic tubes running through the
mailroom, which act as the network for communication. The office building is con-
nected to the rest of the world via a mailbox that processes all incoming and outgoing
messages.
DEFINITION Pneumatic tubes are systems that use compressed air to transport
containers of small objects or documents through a network of tubes. These
systems were commonly used in offices from the late 19th century to the mid-
20th century to send items such as messages between different parts of a build-
ing or complex.)
I find this mental model helpful because it allows me to think and talk about distrib-
uted systems. Distributed Systems Incorporated takes an intangible, abstract cybersys-
tem and maps it to a tangible, concrete physical system while faithfully capturing the
core mechanics of the system:
17Distributed Systems Incorporated
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
The entire corporation
represents the
distributed system.
The tubes and mailroom
represent the network.
One team member
represents a concurrent
component.
C 4C 1 C 2 C 3 C 6C 5
Figure 1.15 Distributed Systems Incorporated
¬° Components as rooms‚ÄîEach room and its occupant represent a concurrent com-
ponent in the distributed system. Components have a local state, send and receive
messages, and perform a set of tasks that contribute to overall system behavior.
¬° The network as pneumatic tubes‚ÄîThe office building‚Äôs pneumatic tubes that connect
the rooms represent the network through which messages are sent and received.
¬° The external interface as a mailbox‚ÄîThe office building‚Äôs mailbox, which handles
all incoming and outgoing messages, represents the system‚Äôs external interface,
where communication with the outside world occurs.
In addition, this model can represent a wide range of concerns. What happens to
Distributed Systems Incorporated if Bob, the mailroom attendant, loses messages,
duplicates messages, or rearranges messages? What happens when employees take a
15-minute break, go on vacation, or leave the company? These examples represent
concerns common to distributed systems:
18 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
¬° Crash semantics‚ÄîConsider what happens when employees take breaks, go on
vacation, or leave the company. This represents various failure modes in a distrib-
uted system:
‚Äì Short absences‚ÄîEmployees taking a short break represent a transient failure.
‚Äì Extended absences‚ÄîEmployees taking a vacation represent an intermittent
failure.
‚Äì Permanent departures‚ÄîEmployees leaving the company represent a permanent
failure.
¬° Message-delivery semantics‚ÄîConsider what happens if the mailroom attendant
loses, duplicates, or rearranges messages. This represents various message-
delivery semantics in a distributed system:
‚Äì Lost messages‚ÄîMessages are not delivered.
‚Äì Duplicate messages‚ÄîMessages are delivered more than once.
‚Äì Reordered messages‚ÄîMessages are delivered out of order.
With these examples, you can analyze the consequences of these actions and explore
potential countermeasures. Consider a scenario in which one employee sends a request
to another employee but does not receive a response. How can the first employee
determine whether the message was lost and the second employee never received the
request or whether the second employee received the request but their response was
lost? If the first employee sends another request, there‚Äôs a risk that the request will
be processed twice, leading to potential inconsistencies. In each case, the company‚Äôs
ability to provide the desired customer service may be compromised unless these situa-
tions are mitigated carefully.
You may be surprised by how far this model can take you. If you work with Kuber-
netes (https://kubernetes.io), a popular distributed container orchestration platform,
try modeling Kubernetes Incorporated. If you work with Apache Kafka (https://kafka
.apache.org), a popular distributed streaming platform, try modeling Kafka Incorpo-
rated. Each technology can be mapped to the building analogy, helping you visualize
and reason about the complexities of distributed systems by making abstract concepts
more concrete.
1.7 Navigating complexity
Even without diving deeply into the complexities of distributed systems, we can begin
having some aha! moments. Let‚Äôs preview a few of these moments informally and
explore them further in later chapters.
1.7.1 Simple yet complex
Although each member of our staff may handle only a simple set of tasks and follow a
simple set of rules, the resulting behavior of Distributed Systems Incorporated can be
complex. In other words, composing simple components does not necessarily result in
simple systems.
19Navigating complexity
1.7.2 Emergent behavior
The interesting behavior of Distributed Systems Incorporated cannot be traced back to
individual employees. A single employee cannot be held responsible for the scalability
of the company, for example, because there is a limit to how much work one employee
can accomplish in a day. Also, a single employee cannot be held responsible for the
reliability of the company because that employee may be absent due to illness, time off,
or departure from the company.
Instead, the interesting behavior of Distributed Systems Incorporated is emergent,
resulting from the actions of individual employees and their interactions. This is the
very foundation of the idea of building reliable systems from unreliable components.
1.7.3 Changing perspective
Often, we effortlessly and instinctively think about systems from both a black-box and
a white-box point of view. But moving from a black-box to a white-box point of view, or
vice versa, is a change in resolution, not a change in perspective.
As figure 1.16 shows, whether we are looking at the black-box or the white-box side,
we are considering the system from the perspective of an all-knowing observer. We are
able to observe the state of the entire system. We have a global view.
White-box modelBlack-box model
Distributed Systems
Incorporated
Mailbox
Room C 1
Room C 3
Room C 5
Room C 2
Room C 4
Room C 6
Mail
Distributed Systems
Incorporated
C 4C 1 C 2 C 3 C 6C 5
Figure 1.16 Black-box versus white- box models: a global point of view
20 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
In figure 1.17, however, we see that a component within the system does not have the
same luxury. A component can observe only its own state, giving it a limited local view.
Shifting perspective, we can accurately and concisely identify the core challenge of dis-
tributed systems: thinking globally while acting locally.
NOTE Although the depiction in figure 1.17 may seem lonely and depressing,
I like to think that Distributed Systems Incorporated has a positive work cul-
ture, strong team dynamics, and the highest levels of employee satisfaction.
Room C1
Figure 1.17 Local point of view
1.7.4 Think globally; act locally
The core challenge of creating distributed systems is designing a global algorithm with
each component of the system implementing a local algorithm. Figure 1.18 illustrates
a common example of the difficulty of ensuring global correctness with limited knowl-
edge: splitbrain. In this scenario, global correctness depends on one person being the
leader and making decisions. But how can you ensure that only one person believes that
they are the team lead? Who are the participants in this process, what local knowledge
do they possess, and what local steps do they take to ensure that global guarantees are
met? In other words, the challenge is creating a system that functions as a cohesive whole
even though each component is aware of and able to access only local information.
Room C 1 Room C2
I am the team leader;
I make the decisions.
I am the team leader;
I make the decisions.
Figure 1.18
Splitbrain
21Thinking above the code
1.8 Thinking above the code
If you flip through this book, you‚Äôll notice a distinct lack of code samples. This lack
is not an oversight but a deliberate choice because simple, static code snippets are
poor at capturing complex, dynamic systems. Instead, we‚Äôll think above the code and
construct tailored mental models that capture the essence of the systems we aim to
understand.
Let‚Äôs look at race conditions. Race conditions are frequently introduced and defined
in terms of a code snippet, illustrating two threads updating the same variable simulta-
neously. This approach has a problem: code snippets make race conditions appear lim-
ited to single-variable updates. Readers are left to extrapolate the fundamental concept
on their own. We place the burden of comprehension on the reader instead of clearly
communicating a complete mental model.
Can we do better? Let‚Äôs construct our own minimalistic model, one that‚Äôs not rooted in
code. A process is a sequence of atomic actions. Process P is the sequence of atomic actions
a, b, c, and process Q is the sequence of atomic actions x, y, z. A process executes by per-
forming the first action in the sequence and then continues with the remaining actions.
The concurrent composition of two processes (P | Q) is the set of all possible inter-
leavings of the actions from P and Q. This framework is sufficient to define race condi-
tions. A race condition is a situation in which a subset of (P | Q)‚Äîthat is, a subset of the
set of all possible interleavings‚Äîis considered incorrect (see figure 1.19). Most import-
ant, you are the arbiter of correctness, as I explain in greater detail later in the book.
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... A race condition refers to the situation in which
a subset of (P | Q) is considered incorrect.
Figure 1.19 Reasoning about race conditions
This model applies not only to threads accessing a single variable but also up and
down the software stack to cooperative multitasking, preemptive multitasking, a sin-
gle machine with a single core, a single machine with multiple cores, and multiple
machines. In database systems, for example, we state that the concurrent composition
of P and Q (P | Q) is correct if, for every possible interleaving, the result of the execu-
tion is equivalent to one of these (see figure 1.20):
¬° (P ‚Ä¢ Q)‚ÄîThe sequential composition of P and Q (that is, P executing before Q)
¬° (Q ‚Ä¢ P)‚ÄîThe sequential composition of Q and P (that is, Q executing before P)
22 chapter 1 Thinking in distributed systems: Models, mindsets, and mechanics
(P | Q)
a ‚ãÖ b ‚ãÖ c ‚ãÖ x ‚ãÖ y ‚ãÖ z
a ‚ãÖ x ‚ãÖ b ‚ãÖ c ‚ãÖ y ‚ãÖ z
x ‚ãÖ y ‚ãÖ a ‚ãÖ z ‚ãÖ b ‚ãÖ c
x ‚ãÖ y ‚ãÖ z ‚ãÖ a ‚ãÖ b ‚ãÖ c
... Serializability demands that concurrent compostion
have the same effect as a sequential composition:
(P | Q) = (P ‚ãÖ Q) v (Q ‚ãÖ P)
Figure 1.20 Reasoning about serializability
When we find the right mental model, we are able to reason about the systems we cre-
ate with unparalleled clarity.
Summary
¬° A mental model is the internal representation of the target system and the basis
of comprehension and communication.
¬° Striving for a deep understanding of distributed systems is better than merely
knowing their concepts.
¬° A distributed system is a set of concurrent components that communicate by
sending and receiving messages over a network.
¬° The core challenge in designing distributed systems is creating a coherent system
that functions as a whole even though each component has only local knowledge.
¬° Ultimately, we are interested in the guarantees that a system provides. We reason
about these guarantees in terms of correctness‚Äîthat is, in terms of safety and
liveness guarantees as well as scalability and reliability guarantees.
¬° Distributed systems can be visualized as a corporation, with rooms representing
concurrent components, pneumatic tubes representing the network, and a mail-
box representing the external interface.
23
2System models,
order, and time
This chapter covers
¬° Synchronous and asynchronous systems
¬° Component and network behavior
¬° Order, physical time, and logical time
Thinking about distributed systems requires thinking about system models, such
as synchronous and asynchronous system models, and also the concepts of order,
physical time, and logical time. Consider this contrast: in a synchronous system with
timing guarantees, if a message does not arrive within a known bound, we know
that a failure has occurred. In an asynchronous system with no timing guarantees,
if a message does not arrive, we know nothing. A failure may have occurred, or the
message may be delayed. Our goal in this chapter is to understand the state of affairs
or forces at play that determine and constrain the behavior of distributed systems.
2.1 System models
We can‚Äôt reasonably talk about a distributed algorithm or protocol without talking
about the assumptions we made about the underlying distributed system. This set of
24 chapter 2 System models, order, and time
assumptions, known as the system model, is about the system‚Äôs components, its network,
and its timing behavior (see table 2.1).
Table 2.1 Examples of assumptions about the system
One system model Another system model
Components may not fail. Components may fail.
Messages may not get lost. Messages may get lost.
Clocks are perfectly synchronized. Clocks are not perfectly synchronized.
Why are system models crucial for our discussions? Algorithms and protocols that are
correct under one system model may not be correct under another system model; any
deviation may render an algorithm or a protocol incorrect.
Aha! moment: Board game
You can think of a system model as a board game. The game sets the stage and the
rules, and the players must devise a strategy to achieve the game‚Äôs objective within
the constraints of the rules. Even a slight change in the rules may render the players‚Äô
strategy ineffective. Your carefully devised strategy, executed over many rounds, may
unravel in that instant.
2.1.1 Theory and practice
System models can take many forms, from theoretical to practical and everything in
between (see figure 2.1).
Theoretical Practical
System models
Often used for reasoning
about impossibilities
Often used for reasoning
about possibilities
Practical with theoretical foundations
(e.g., consensus algorithms)
Figure 2.1
System models
25System models
Theoretical system models are commonly used to reason about impossibilities. Com-
puter scientists explore the theoretical limitations of a problem by investigating the
feasibility of a solution and assessing the minimum and maximum bounds of the possi-
ble solutions in terms of the numbers of components involved or messages exchanged.
Practical system models are commonly used to reason about possibilities. Engineers
explore the practical considerations by assessing possible solutions, taking into account
critical factors such as development costs, operational costs, and scalability and reliabil-
ity properties.
Finally, there is the intersection of practical and theoretical system models‚Äîthat is,
system models that are equally relevant for theory and practice. Examples include sys-
tem models that underlie popular consensus algorithms, such as Viewstamped Replica-
tion, Paxos, and Raft.
2.1.2 Synchronous distributed systems
Synchronous distributed systems are systems in which each component operates with access
to a clock that is perfectly synchronized with other clocks or deviates by a known upper
bound. Although common synchronous system models may vary in their details, each
model assumes a strong notion of physical time. Furthermore, internal and external
events unfold in strict time or occur with a bounded delay. In effect, some models pres-
ent a synchronous system as one with no uncertainty about its timing; other models
present a synchronous system as one with some bounded uncertainty about its timing.
NOTE The synchronous system model does not predicate component failure
or message loss. I will discuss these assumptions in section 2.1.5.
2.1.3 Asynchronous distributed systems
Like synchronous systems, commonly presented asynchronous system models vary in
their details, but the difference is significant:
¬° Some models assume no notion of time.
¬° Some models assume a weak notion of time.
Some problems have no solution assuming no notion of time, yet they have a solution
assuming a weak notion of time. When you encounter the term asynchronous system, first
try to determine the model‚Äôs notion of time; then try to determine whether that notion
of time is significant in that context.
no notion of time
In this theoretical system model, no component has access to any kind of clock. Also,
internal and external events occur in arbitrary time, or internal and external events
occur with an arbitrary delay.
In this model, the notion of time does not exist; the most significant consequence
is that a component cannot use timeouts. Timeouts are crucial mechanisms for failure
detection and mitigation.
26 chapter 2 System models, order, and time
Weak notion of time
In this theoretical system model, a component has access to a clock that is not synchro-
nized with other clocks and has an unknown deviation. Also, internal and external
events occur in arbitrary time, or internal and external events occur with an arbitrary
delay. In this model, the notion of time does exist; the most significant consequence is
that a component can use timeouts.
NOTE As with synchronous systems, the asynchronous system model does not
predicate component failure or message loss. A different set of assumptions is
involved; I will discuss these assumptions in section 2.1.5.
2.1.4 Partially synchronous systems
Both synchronous and asynchronous system models are theoretical system models. In
reality, no distributed system displays entirely synchronous or entirely asynchronous
behavior; the system‚Äôs behavior lies somewhere in between (see figure 2.2).
Synchronous
system model
Partially (a)synchronous
system model
Asynchronous
system model
Theoretical
system model
Practical
system model
Theoretical
system model
Figure 2.2 From synchronous to asynchronous system models
The partially synchronous system model acknowledges this reality and postulates that
a distributed system is synchronous most of the time and asynchronous at other times.
This model matches our experience and expectations. Components and networks are
generally well behaved but sometimes act up.
NOTE A relatable example of alternating behavior is our home internet. Most
of the time, accessing the web is quick and meets our expectations. Sometimes,
however, home internet becomes slow or results in timeouts before returning
to normal.
A typical approach to designing protocols is to ensure safety and liveness guarantees
when the system operates synchronously and to prioritize safety over liveness when the
system operates asynchronously, thus ensuring that nothing bad will happen even if
something good won‚Äôt happen. Chapter 3 talks about failure tolerance (failure detec-
tion and mitigation) in greater detail.
27System models
2.1.5 Component and network behavior
Components and networks are frequently characterized by their failure behavior.
Chapter 3 discusses failure, so I will save the definition for that chapter.
component failure
System models for components vary
depending on the types of failures that
may occur in a component. Figure 2.3
illustrates common failure models.
crash-stop failure
When a component experiences a Crash-
Stop failure, which is the most basic failure
model, the component stops to execute
any internal or external steps at an arbi-
trary moment in time and performs no
further internal or external steps (see
figure 2.4). Put simply, the component
ceases to exist.
Step 1 Step 2
State 1 State 2
Figure 2.4 Crash-Stop failure
omission failure
When a component experiences an Omission failure, the component stops to perform
any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes from the state where the failure occurs, again per-
forming internal and external steps (see figure 2.5). Put simply, the component takes
a break.
Step 1 Step 2
State 1 State 2
Does not cause memory loss! Step 3
State 3
Figure 2.5 Omission failure
Byzantine failure
Crash-Recovery failure
Omission failure
Crash-Stop failure
Figure 2.3 Component failures
28 chapter 2 System models, order, and time
crash-recovery failure
When a component experiences a Crash-Recovery failure, the component stops to per-
form any internal or external steps for an arbitrary duration at an arbitrary moment in
time. Then the process resumes, again performing internal and external steps. Unlike
in Omission failure, however, the component may lose state (see figure 2.6). Put sim-
ply, the component may suffer partial or total memory loss.
Step 1 Step 2
State 1 State 2
Causes memory loss! Step 3
State 1
Figure 2.6 Crash-Recovery failure
byzantine failure
A Byzantine failure occurs when a component behaves in an arbitrary manner, including
acts of deception such as intentionally deviating from its intended algorithm (see fig-
ure 2.7). Put simply, anything might happen.
Figure 2.7 Byzantine failure
netWork failure
Like components, system models for a network differ according to the failures that
may occur in the network.
message reordering
A network may not respect the order of messages. That is, a network may deliver
messages in a different order from the order in which they were are received (see
figure 2.8).
message duplication
A network may send a message to the receiving component more than once, so a com-
ponent may receive duplicate messages (see figure 2.9).
message loss
A network may not send a message to the receiving component at all (see figure 2.10).
29System models
Send a Send b
Send b Send a
Receive aReceive b
Receive a Receive b
Component 1
Network
Component 2
Figure 2.8 Message reordering
Send a
Send a Send a
Receive aReceive a
Receive a
Component 1
Network
Component 2
Figure 2.9 Message duplication
Send a
Receive a
Component 1
Network
Component 2
Figure 2.10 Message loss
30 chapter 2 System models, order, and time
We assume, however, that in general, a network cannot lie. If a network delivers a mes-
sage to a component, we presume that the message was sent by a component in the sys-
tem. A network that experiences message reordering, message duplication, or message
loss is commonly referred to as an unreliable network.
2.1.6 Realistic system models
With the knowledge we‚Äôve acquired thus far, we can identify practical system mod-
els‚Äîa set of practical assumptions regarding distributed systems. In practical terms,
distributed systems are synchronous most of the time and asynchronous at certain
times. Components may fail due to crashes but may also recover, forgetting some state
(volatile) but remembering other state (durable). The network may reorder, drop,
or duplicate messages. More formally, distributed systems are partially synchronous,
consisting of components subject to Crash-Stop failure, Omission failure, and Crash-
Recovery failure communicating over an unreliable network.
Although Byzantine failures are relevant in some domains, such as cryptocurrency,
this book largely ignores them. This system model is the one I have in mind when I
think about distributed systems.
2.2 Order and time
Let‚Äôs take a moment to revisit a definition. A distributed system is a set of concurrent
components that communicate by sending and receiving messages over a network. But
components are not merely communicating; they are also collaborating and coordi-
nating. Here, collaboration and coordination refer to the management of dependencies
between the steps of components.
In general, actions are not commutative. The result of collaboration and coordina-
tion depends on the order in which actions are applied. Therefore, establishing the
correct order of actions is crucial for correct results. This question arises: how do we
determine this correct order?
NOTE Although this section on order and time is not the most exciting read,
it‚Äôs essential that you grasp the concepts of event ordering, physical time, and
logical time because they will accompany you throughout your journey in the
realm of distributed systems, both in theory and in practice. A solid founda-
tional understanding of these concepts is invaluable for success. I‚Äôve included
two of my favorite aha! moments: a clear definition of race conditions and a
definition of concurrency versus parallelism.
Let‚Äôs examine a practical example. Consider two proposers, P1 and P2, and two accep-
tors, A1 and A2, with both acceptors starting in the initial state of A1 = A2 = 0. The pro-
posers broadcast requests to apply simple arithmetic operations, such as (+2) or (√ó2),
to the acceptors. The acceptors apply the requests to their local state in the order in
which they receive the requests.
If both P1 and P2 broadcast (+2), the order of application does not affect the
result because the operations are commutative. But if P1 broadcasts (+2) and P2
31Order and time
broadcasts (√ó2), the order of application affects the result because the operations are
not commutative.
Take a close look at figure 2.11. Even as all-knowing observers, we cannot immedi-
ately tell which order of application is correct because both P1 and P2 broadcast their
requests at the same time. But this result is unintuitive; it feels wrong. Intuitively, we
expect A1 and A2 to show the same value, and we expect A1 and A2 to apply the opera-
tions in the same order. Because P1 and P2 broadcast their requests at the same time, the
order of the operations does not matter as long as the same order is guaranteed.
Accepter 1
Proposer 1
Proposer 2
Accepter 2 √ó2
+2
+2
+2
√ó2
√ó2
A1 = 0 A1 = 2 A1 = 2 A1 = 4
A 2 = 0 A2 = 0 A2 = 0 A 2 = 2
Figure 2.11 Proposers and acceptors
Aha! moment: Race conditions
The preceding paragraph is a helpful example of a race condition. Although we may
have a general understanding of a race condition, providing an accurate, concise
definition can be challenging. Typically, we tend to rely on examples that involve pro-
cesses, threads, fibers, and shared memory.
In the context of this book, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some are
considered incorrect. If proposers P1 and P2 can broadcast only (+n) or only (√ón), the
system does not contain a race condition because the delivery order does not affect
the outcome at the acceptors. But if P1 and P2 can broadcast both (+n) and (√ón), the
system contains a race condition because only specific delivery orders can produce
the same result at the acceptors. I think this is the most abstract, broadly applicable
definition of a race condition.
A potential solution to the race condition in this example introduces a coordinator
C, as illustrated in figure 2.12. Instead of broadcasting their requests to A1 and A2, P1
32 chapter 2 System models, order, and time
and P2 send their requests to C, which initially has a state of C = 0. When C receives
a request, C tags each request with its current state, broadcasts the request to A1 and
A2, and increments the value by 1. A1 and A2 keep track of the latest request they have
applied for. If an acceptor receives a request with a larger tag, they realize that they
have skipped some steps and can delay applying the requests until they receive the
missing requests.
Accepter 1
Proposer 1
Proposer 2
Accepter 2
Coordinator
+2
+2
+2
+2
√ó2
√ó2
√ó2
0 1
0 1 2
1
0 1
1 0
reorder
√ó2
√ó2 +2
Figure 2.12 Proposers, acceptors, and a coordinator
Take a closer look at figure 2.12. As all-knowing observers, we see that P1 sends its
request (+2) before P2 sends its request (√ó2). But the network delivers P2‚Äôs request (√ó2)
before P1‚Äôs request (+2). Is that a problem? P1 and P2 did not coordinate their requests,
and neither can expect to be first because they can observe only themselves, not each
other. Because C cannot observe P1 or P2 sending their requests, any delivery order is
acceptable. C tags P2‚Äôs request with 0 and tags P1‚Äôs request with 1 before broadcasting
<(√ó2), 0> and <(+2), 1> to A1 and A2. This way, regardless of the order in which the
requests arrive at A1 and A2, the acceptors can reorder them before applying them to
their local state. Both order and the ability to establish order are fundamental to many
distributed system problems and their solutions.
2.2.1 The happened-before relationship
In his seminal 1978 paper ‚ÄúTime, Clocks, and the Ordering of Events in a Distrib-
uted System‚Äù (https://mng.bz/WwY4), Leslie Lamport recognizes the importance
of the order of events and explores its relationship to physical time and logical time.
33Order and time
In this paper, Lamport introduces a formal framework that is widely used today: the
happened-before relationship. Lamport defines the happened-before relationship in
terms of intra- and intercomponent relations of events.
intracomponent
If events a and b occur at the same component and the occurrence of a precedes the
occurrence of b, event a happened-before event b (see figure 2.13).
Component 1
Component 2
ba
Figure 2.13 Happened-before relationship, intracomponent
intercomponent
If event a and b occur at different components, and a is a Send Message event and b
is the corresponding Receive Message event, event a happened before event b (see
figure 2.14).
Component 1
Component 2
a
b
Figure 2.14 Happened-before relationship, intercomponent
The happened-before relationship is a partial order. For each pair of events a and b,
a happened before b, b happened before a, or a and b are concurrent (see table 2.2).
Table 2.2 Happened-before relationships
Situation Notation
a happened before b. a ‚Üí b
b happened before a. b ‚Üí a
a and b are concurrent. a || b = b || a
34 chapter 2 System models, order, and time
transitivity
Unsurprisingly, a happened-before relationship is transitive. If event a happened
before event b, and event b happened before event c, event a also happened before
event c (see figure 2.15).
Component 1
Component 2
ba
c
Figure 2.15 Happened-before, transitively
causality
The happened-before relationship captures the causal relationship between events.
Here, causal refers to the fact that an event a potentially influenced an event b, not that
a actually influenced event b. When you think about a distributed system, always think
about the causality of events and carefully determine the possibility that the system will
violate that causality.
Aha! moment: Concurrency vs. parallelism
We can rely on the happened-before relationship to define concurrency and parallel-
ism accurately and concisely, as shown in the following figure.
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Begin
event
End
event
Concurrent but not parallel Concurrent and partially parallel
Concurrency vs. parallelism
First, however, we must address a mismatch between the entities involved in concur-
rency and parallelism. Concurrency is defined in terms of events, which are moments
in time, whereas parallelism is defined in terms of operations, which are spans of time.
To bridge this gap, we can think of an operation as a pair of start and end events.
35Order and time
By definition in a happened-before relationship, two events, a and b, are concurrent
if a did not happen before b and b did not happen before a. By extension, two opera-
tions, o1 and o2, are concurrent if the end event of o1 did not happen before the begin
event of o2 and the end event of o2 did not happen before the begin event of o1. Two
operations are parallel if the operations overlap in time:
¬° Concurrency is determined by a logical clock and logical time.
¬° Parallelism is determined by a physical clock and physical time.
All this brings us straight to time and clocks. See section 2.2.2.
2.2.2 Time and clocks
Now that we understand that events are ordered and know the importance of that
order, we need to consider how the components of a distributed system can capture
that order. Distributed systems rely on clocks as a source of timestamps. To ensure that
clocks can be used to order events accurately, clocks must be able to guarantee clock
consistency. Clock consistency states that if event a happened before event b, the time-
stamp of a is less than the timestamp of b:
a ‚Üí b ‚áí C(a) < C(b)
If clocks are consistent, they can be used to reason about the causal ordering of events
in the system, even if those events occur on different components.
2.2.3 Physical time and physical clocks
There are two types of clocks: physical and logical. Both are used in distributed systems
for ordering events, but they differ in their properties.
Physical clocks are physical devices that use physical time to timestamp events. But
physical clocks in distributed systems face a major challenge: comparing timestamps
from different clocks can be difficult due to clock skew and drift. These problems arise
because physical clocks are neither perfect nor perfectly synchronized.
Aha! moment: Wall clock
As all-knowing observers with a global point of view, we can assume that we have
access to a wall clock, a hypothetical perfect physical clock. In reality, no component
has access to this wall clock. Instead, if a component needs to measure time, it
needs access to its own physical clock, an actual clock that is imperfect. The wall
clock is a hypothetical device that establishes a reference point for our discussions.
clock skeW
Clock skew refers to the difference in time between two clocks in a distributed system.
Figure 2.16 illustrates clock skew. Clock 1 and Clock 2 start with a 5-minute difference
and maintain this fixed offset as time progresses.
36 chapter 2 System models, order, and time
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.16 Clock skew
clock drift
Clock drift refers to the difference in the frequency of ticks between two clocks in a
distributed system. Figure 2.17 illustrates clock drift. Clock 1 and Clock 2 start with the
same time value and gradually diverge as time progresses.
Clock 1
Clock 2
Clock 1
Clock 2
Figure 2.17 Clock drift
mitigation
Clock skew and clock drift complicate comparing timestamps and time intervals across
components in a distributed system. Clock synchronization protocols such as the widely
used Network Time Protocol (NTP) mitigate this challenge. NTP works by having one
component, called the NTP server, broadcast its clock reading to other components in
the system, known as NTP clients. The clients adjust their clocks to match the reading of
the server to mitigate the effects of clock skew and drift.
time-of-day clock vs. monotonic clock
Adjusting clocks can cause problems as well. If a clock needs to adjust backward, time
seems to move backward. To address this problem, hardware and software provide two
types of clocks:
¬° Time-of-day clocks provide a timestamp as close as possible to wall-clock time but
may move backward due to clock synchronization. Time-of-day clocks may violate
clock consistency even when compared to themselves.
37Order and time
¬° Monotonic clocks provide a timestamp independent of wall-clock time but guaran-
tee that time will not move backward. Monotonic clocks guarantee clock consis-
tency when compared to themselves.
Programming environments usually provide functions for both time-of-day and mono-
tonic clocks, as shown in the following listing.
Listing 2.1 Time-of-day and monotonic clocks
// Time of day clock
var timestamp1 = System.TimeOfDay
var timestamp2 = System.TimeOfDay
// Monotonic clock
var timestamp3 = System.Monotonic
var timestamp4 = System.Monotonic
Because monotonic clocks are independent of wall time, they can be compared only
as intracomponent, not intercomponent. In conclusion, physical clocks are crucial in
distributed systems, but they pose significant challenges, such as clock skew and drift.
Being aware of these challenges and implementing appropriate measures to minimize
their effect is essential.
2.2.4 Logical time and logical clocks
In the paper mentioned in section 2.2.1, Lamport proposed logical time and logical
clocks to establish the order of events. Logical time allows events to be ordered by their
happened-before relationship.
Logical clocks are logical devices that use logical time to timestamp events. One of
the best-known logical clocks is the Lamport clock, proposed by‚Äîyou guessed it‚Äî
Leslie Lamport. Each component in the distributed system maintains a Lamport clock,
a counter that is incremented with every internal or external step. Steps‚Äîrather, the
corresponding events‚Äîare tagged with the current value of the Lamport clock. When a
component sends a message to another component, the receiving component updates
its own Lamport clock value to the maximum of its current timestamp and the received
timestamp plus 1. This ensures that causally related events are assigned strictly, increas-
ing logical-time values (see figure 2.18).
Lamport clocks are widely known and widely discussed but not necessarily widely
implemented. I‚Äôve never used Lamport clocks to capture the ordering of events. But
Lamport clocks are not the only manifestations of logical clocks; we can find logical
clocks and timestamps in lots of places.
Clock synchronization may happen
immediately after this statement.
timestamp 2 may be
less than timestamp 1.
Clock synchronization may happen
immediately after this statement,
but monotonic clocks are unaffected.
timestamp 4 is always
greater than timestamp 3.
38 chapter 2 System models, order, and time
41 5
max(1, 3) + 1
21 3
Figure 2.18 Lamport clocks
Consider Apache Kafka, a messaging and streaming platform that organizes data in
topics, which are further divided into partitions. Each message in a partition is assigned
an offset that indicates its position within that partition. This offset allows consumers
to track their progress through the partition.
Kafka ensures that messages are ordered within each partition. In other words, you
can think of the partition as a logical clock and the message offset as a logical time-
stamp. If message a was enqueued before message b within the same partition (intra-
partition), the offset of message a is less than the offset of message b, but message offsets
cannot be compared across partitions (interpartition).
We can find another example of logical clocks in etcd, a distributed, linearizable
key-value store that uses a sequence number for each key. When a key is updated, the
sequence number is incremented, allowing operations such as compare-and-swap.
In etcd, a key can be considered a logical clock and the sequence number a logical
timestamp. For each key, if update operation a happened before update operation b,
the sequence number assigned to a is less than the sequence number assigned to b. As
in Kafka, however, sequence numbers across keys in etcd are not comparable.
Even if you don‚Äôt see Lamport clocks or vector clocks everywhere, if you squint a lit-
tle, you may see something that resembles logical clocks and logical timestamps.
2.2.5 Physical clocks vs. logical clocks
Physical clocks and logical clocks are commonly used in tandem to create a sense of
order and time in distributed systems. Physical clocks are employed to measure the
duration between events within a component, whereas logical clocks are employed to
establish the order of events across components.
Physical clocks can suffer various issues in distributed systems, including clock drift,
clock skew, and synchronization problems, which may lead to inconsistencies in the
ordering of events across the system. Logical clocks are designed to provide a consistent
ordering of events even if physical clocks are out of sync.
Summary
¬° System models encode assumptions about components, network, and timing
behavior; different system models affect algorithm correctness.
¬° Synchronous systems have strict timing guarantees, whereas asynchronous sys-
tems operate with no timing guarantees. Partially synchronous systems combine
39Summary
the properties of synchronous and asynchronous systems, operating synchro-
nously most of the time but tolerating asynchronous behavior occasionally.
¬° Component failures include Crash-Stop, Omission, Crash-Recovery, and Byzan-
tine. Network failures include message reordering, duplication, and loss.
¬° Order of events is crucial for correctness. Logical clocks such as Lamport clocks
are used to establish event order and capture causality.
40
3Failure tolerance
This chapter covers
¬° Failure and failure tolerance
¬° Failure detection and mitigation
¬° Application- and platform-level failures
¬° Transient, intermittent, and permanent failures
¬° An ideal failure-handling strategy
Now that we have defined the notion of system models, discussed widely used system
models including synchronous and asynchronous distributed systems, and explored
the concepts of order, physical time, and logical time, we can explore failure, failure
tolerance, and failure handling‚Äîin short, ways to think about failure. While read-
ing this chapter, keep in mind that the primary objective of thinking about failure
is to ensure failure tolerance, which refers to the guarantee that a distributed system
functions in a well-defined manner even when failures occur.
The topic of failure, failure tolerance, and failure handling in distributed com-
puting is broad, encompassing a significant body of theoretical and practical work.
Therefore, this chapter is divided into two main sections to provide a well-rounded
41In theory
perspective: The first main section explores thinking about failure in theoretical terms;
the second main section explores thinking about failure in practical terms.
3.1 In theory
Informally, a failure is an unwanted but possible state transition of a system. On failure,
the system transitions from a good state to a bad state. Failure tolerance is the ability of a
system to behave in a well-defined manner when the system is in a bad state.
A formal approach to defining failure is based on the observation that a system can
be in one of three states: legal state (the good state), illegal state (the bad state), and
intolerable state (the ‚Äúeverything is lost‚Äù state). Also, a formal approach to defining fail-
ure is based on the observation that a system changes its state as a result of two types of
transitions: normal and failure.
NOTE The terms fault, error, and failure are subject to considerable ambiguity.
What one author may refer to as a fault, another may label as an error or a fail-
ure. Although some software engineers attempt to distinguish between these
terms, there is no universally accepted definition. This book uses the term fail-
ure. By using this term, I aim to reduce confusion and provide clarity in discuss-
ing these complex systems. I will also use less frequently used terms, such as
failure tolerance, instead of the more common fault tolerance.
These observations provide a comprehensive framework for understanding the con-
cept of failure in distributed systems, as illustrated in figure 3.1.
4. State = Illegal state, transition = Failure transition
If a system is in an illegal state and executes a failure
transition, the system will transition to another illegal state.
3. State = Illegal state, transition = Normal transition
If a system is in an illegal state and (repeatedly) executes a
normal transition, the system will transition to a legal state.
2. State = Legal state, transition = Failure transition
If a system is in a legal state and executes a failure transition,
the system will transition to an illegal state
1. State = Legal state, transition = Normal transition
If a system is in a legal state and executes a normal
transition, the system will transition to another legal state.
Legal
state
Illegal
state
Intolerable
state
Figure 3.1 System states and state transitions
42 chapter 3 Failure tolerance
The sequence of transitions that leads from an illegal state to a legal state is also called
failure recovery.
NOTE Where are intolerable states and transitions? Fundamentals of fault-
tolerant distributed computing in asynchronous environments exclude intol-
erable states and intolerable transitions from discussions because by definition,
we cannot tolerate‚Äîand do not attempt to tolerate‚Äîthat situation. The often-
cited, more than drastic earth-gets-hit-by-a-meteor example comes to mind;
there are simply no contingencies.
3.2 Types of failure tolerance
Legal and illegal states are characterized by their safety and liveness properties. Recall
these statements from chapter 1:
¬° Safety guarantees that something bad will never happen.
¬° Liveness guarantees that something good will eventually happen.
We can define failure tolerance in terms of guaranteeing safety and liveness properties
(see table 3.1). In the absence of failure, a system always guarantees both its safety and
liveness properties. In the presence of failure, however, we may have to compromise.
Table 3.1 Types of failure tolerance
Safe Not safe
Live Masking Nonmasking
Not live Fail-safe üò≠
3.2.1 Masking failure tolerance
If a system guarantees both safety and liveness in the presence of failure, it provides
masking failure tolerance. Masking (as in hiding or covering) failure tolerance, the most
desirable form of failure tolerance, amounts to failure transparency. But masking fail-
ure tolerance may be too costly or impossible to achieve, forcing us to make choices.
Chapter 9 discusses impossibility results, such as the Consistency or Availability under
Partitioning (CAP) conjecture, that prevent us from guaranteeing failure transparency
in certain scenarios.
3.2.2 Nonmasking failure tolerance
If a system guarantees liveness but does not guarantee safety in the presence of fail-
ure, it provides nonmasking failure tolerance. Informally speaking, the system doesn‚Äôt
guarantee that it won‚Äôt make any mistakes, but it guarantees that it will make progress.
Consider a queue that guarantees fully ordered (first in, first out) message delivery
in the absence of failure. Now suppose that the queue is nonmasking failure-tolerant.
43Types of failure tolerance
In that case, it will continue delivering messages in the presence of a failure, but mes-
sages may be delivered out of order for the duration of the failure.
3.2.3 Fail-safe failure tolerance
If a system guarantees safety but does not guarantee liveness in the presence of failure,
it guarantees fail-safe failure tolerance. Informally speaking, the system guarantees
that it won‚Äôt make any mistakes, but it doesn‚Äôt guarantee that it will make progress.
Continuing with the preceding example, if the queue is fail-safe failure-tolerant,
it will stop delivering messages in the presence of a failure to prevent messages from
being delivered out of order for the duration of the failure.
3.2.4 None of the above
If a system does not guarantee safety or liveness in the presence of failure, it is not
failure-tolerant at all. Arguably, this is unacceptable in most cases.
Let‚Äôs explore these concepts with the help of an example. The following listing and
figure 3.2 illustrate a simple nondistributed system consisting of one component, here
called process P. The process has one variable x with the possible values {0, 1, 2, 3}.
x=3
x=1 x=2
x=0
Figure 3.2 An illustration of
the states and state transitions
defined in listing 3.1
Listing 3.1 Normal and failure transitions
process P
var x ‚àà {0, 1, 2, 3} init 1
transitions
// normal transition
Per definition, there are two legal
states x=1 and x=2, an illegal state
x=0, and an intolerable state x=3.
44 chapter 3 Failure tolerance
‚ù∂ x = 1 ‚Üí x := 2
‚ù∑ x = 2 ‚Üí x := 1
// normal transition (repair)
‚ù∏ x = 0 ‚Üí x := 1
// failure transition
‚ùπ x ‚â† 0 ‚Üí x := 0
end
end
As the creators of the system, we specify its correctness (its desired safety and liveness
properties):
¬° Safety‚ÄîHere, we define the system‚Äôs safety property as the value of x is either 1
or 2.
¬° Liveness‚ÄîSimilarly, we define the system‚Äôs liveness property as this: there is a
point in the future when x will be 1, and there is a point in the future when x will
be 2.
This system is safe and live if x = 1 or x = 2. The system is not safe but still live if x = 0.
Therefore, this system is nonmasking failure-tolerant.
Note that the state x = 3 is intolerable. Should the system ever find itself in this state,
there is no guarantee that it will ever return to a legal or illegal state.
3.3 In practice
Now that we have established the theoretical foundations for thinking about failures,
we can establish the practical foundation for thinking about failures. In addition, we
will lay out an outline for a failure-handling strategy in a service-oriented distributed
system.
NOTE Although retry mechanisms are a common approach to handling miti-
gation in (micro)services-oriented distributed systems, I will not cover them in
depth here. Chapter 4 explores retries in more detail with regard to message
delivery and processing semantics.
3.3.1 System model
For the remainder of this chapter, we will think in terms of service orchestration, which
involves a service-consumer component and one or more service-provider components
(see figure 3.3). These components interact through a request‚Äìresponse-style message
exchange.
The service consumer executes a process‚Äîa sequence of steps in which every step is
a request to a service provider (see figure 3.4).
When a process is started, two scenarios are possible:
¬° Total application‚ÄîIn the absence of failures, the process successfully executes
each step and transitions the system from a correct state to another correct state.
Normal transitions alternating
between the legal states x=1 and x=2
Normal transition moving from the
illegal state x=0 to the legal state x=1.
This models recovery from a failure.
Failure transition moving from the
legal states x=1 or x=2 to the illegal
state x=0. This models a failure.
45In practice
State of
C 1
State of
C 3
State of
C 5
C 3
C 5
C 2
C 4
C 6
State of
C 2
State of
C 4
State of
C 6
Process
Service consumer Service providers
Figure 3.3 Service orchestration
cba
Figure 3.4 A process as a sequence of steps
¬° Partial application‚ÄîIn the presence of a failure, the process may execute only
certain steps, transitioning the system from a correct state to a possibly incorrect
state.
In summary, a process is a sequence of steps in which partial execution is undesirable.
Therefore, in the event of a failure, we need to ensure that the process executes in one
of two ways:
¬° Correct and desirable‚ÄîObservably equivalent to a successful, total application
(spoiler: forward recovery)
¬° Correct but less desirable‚ÄîObservably equivalent to no application (spoiler: back-
ward recovery)
TIP Completeness (that is, total application) is to distributed systems what
ACID (atomicity, consistency, isolation, durability) is to databases: a prerequi-
site for the proper execution of a sequence of steps. If the sequence of steps is
incomplete, it‚Äôs incorrect.
46 chapter 3 Failure tolerance
To illustrate this point, we will use the quasi-canonical example of an e-commerce
checkout process. One step in this process involves charging the customer‚Äôs credit card
(see figure 3.5).
...Charge credit card...
Figure 3.5 E-commerce process
NOTE We must ensure that the credit card
is charged exactly once in the event of a
successful checkout or not charged at all if
the checkout fails. Under no circumstances
should the credit card be charged if goods
are not shipped or tickets are not issued.
3.3.2 Failure handling
Now let‚Äôs handle some failures. Failure han-
dling involves two main steps: failure detection
and failure mitigation (see figure 3.6)‚Äîthat is,
detecting an unwanted event and then taking
steps to mitigate it.
Aha! moment: Failure detection and mitigation
The relationship between failure detection and safety, as well as the relationship
between failure mitigation and liveness, initially caught me by surprise. In general,
failure detection is the basis for guaranteeing safety. A commonly employed strategy
is to detect a failure and subsequently inhibit dangerous actions to ensure that the
system maintains its safety.
Similarly, failure mitigation is the basis for guaranteeing liveness. A commonly employed
strategy is to mitigate a failure to ensure that the system resumes its liveness.
This relationship is demonstrated by consensus algorithms such as Paxos, Raft, and
Viewstamped Replication. Consensus algorithms rely on quorums to reach consen-
sus (ensure their safety and liveness properties).
When a node is unable to participate in the quorum (detection), for example, it must
refrain from responding to requests to ensure safety. When the node rejoins the quo-
rum (mitigation), it can resume responding to requests and resume liveness. The
relationship between these concepts is fascinating, yet it was not immediately obvi-
ous to me.
Failure
handling
Failure
detection
Failure
mitigation
Ensures
safety
Ensures
liveness
Figure 3.6 Failure handling
47In practice
3.3.3 Failure classification
To detect and mitigate failures efficiently and effectively, we must first understand
their nature: their classification. Failure can be classified in countless ways, of course,
but here, I want to use the classification that guides my thinking, illustrated in figure
3.7. I will focus on two orthogonal dimensions: spatial and temporal.
spatial dimension
On one hand, failures may be classified by where it occurs‚Äîby location (see figure 3.8).
First, however, we must add another model to our repertoire: thinking in layers.
Failure
classification
Spatial
dimension
Temporal
dimension
Figure 3.7 Failure classification
The mental model of layered architecture
arranges components in layers. Compo-
nents at a higher layer make a downcall
to components at a lower layer, generally
expecting a response. Less frequently,
components at a lower layer make an
upcall to components at a higher layer,
generally via a previously registered call-
back (see figure 3.9).
Aha! moment: Application layer and platform layer
‚ÄúSeek to characterize things by the role they play in context rather than by some
intrinsic characteristic.‚Äù ‚ÄîDr. Eugenia Cheng, The Joy Of Abstraction: An Exploration of
Math, Category Theory, and Life (Cambridge University Press, 2022)
From the point of view of any given layer, that layer is considered the application layer,
and all lower layers are considered the platform layer. An operating system sees itself
as the application layer and the hardware as the platform layer. Conversely, a data-
base management system regards itself as the application layer and views the oper-
ating system and hardware as the platform layer.
Layer n
Layer n‚àí1
Layer 0
...
Figure 3.9 Thinking in layers
Spatial
dimension
Application-level
failure
Platform-level
failure
Figure 3.8 Spatial classification
48 chapter 3 Failure tolerance
(continued)
Sometimes, we reason across three layers: application, platform, and infrastructure.
So from that point of view, the database management system is the application layer,
the operating system is the platform layer, and the hardware is the infrastructure
layer.
As discussed in chapter 1, when you think in distributed systems, keep in mind that
various authors may conceptualize the layers of a software system differently. Also,
authors may associate the same components with different layers, depending on the
argument they are trying to make.
application-level and platform-level failures
The end-to-end argument states that in a
layered system, failure handling should
be implemented in the lowest layer possi-
ble (viewed from the top down) that can
handle failure detection and failure miti-
gation correctly and completely. A failure
can be classified as an application-level or
platform- level failure, depending on the
lowest layer that can detect and mitigate
the failure (see figure 3.10).
An InsufficientFunds failure message,
for example, indicates an application-level
failure. The application level is the lowest
layer capable of resolving this failure correctly and completely; the failure is meaningless
(cannot be interpreted) at the platform level.
Conversely, CouldNotConnect failure message indicates a platform-level failure.
Although the application could potentially mitigate the failure, the lowest layer capable
of mitigating that failure correctly and completely is the platform level, which can do so
by trying to reestablish the connection.
temporal dimension
On the other hand, failures may be classified by when they occur‚Äîby time (see figure
3.11).
Failures can be classified as transient, intermittent, or permanent. These classifica-
tions can be expressed as conditional probabilities‚Äîspecifically, the probability that a
second failure will occur, given that a first failure has already occurred.
In addition, transient, intermittent, and permanent failures can be distinguished by
how they are repaired or resolved. Transient and intermittent failures often repair auto-
matically, whereas permanent failures usually require manual repair.
InsufficientFunds
Can be handled
only here
Application
Platform
CouldNotConnect
May be handled
here
Figure 3.10 Application-level vs. platform-
level failure
49In practice
Temporal
dimension
Intermittent
failure
Permanent
failure
Manual repair
Transient
failure
Autorepair Autorepair Figure 3.11 Temporal dimension
transient failures (failures that come and go)
If a failure is transient, we can reasonably assume that the probability of a second fail-
ure is not elevated. Formally, a transient failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is the same as
the probability that F2 will occur on its own (see figure 3.12).
¬° Transient failures are autorepairing and resolve without intervention.
P(F2 | F1) = P(F2)
Figure 3.12 Transient failure
In our example, if the cause of the failure is a router restart, CouldNotConnect may be
a transient failure. This type of failure autorepairs quickly; when the router restarts,
the connection can be made.
intermittent failures (failures that linger)
If a failure is intermittent, we can reasonably assume that the probability of a second
failure is elevated. Formally, an intermittent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is higher than
the probability that F2 will occur on its own (see figure 3.13).
¬° Intermittent failures are also autorepairing and resolve without intervention.
P(F2 | F1) > P(F2)
Figure 3.13 Intermittent failure
50 chapter 3 Failure tolerance
In our example, if the cause of the failure is an outdated routing table, CouldNotConnect
may be classified as an intermittent failure. This type of failure autorepairs after some
delay; when the router updates its routing table, the connection can be made.
permanent failures (failures that are here to stay)
If a failure is permanent, we can reasonably assume that a second failure is certain. For-
mally, a permanent failure is defined by two characteristics:
¬° The probability that a failure F2 will occur after another failure F1 is 1 (see figure
3.14).
¬° Permanent failures require manual intervention to resolve.
P(F2 | F1) = 1.0
Figure 3.14 Permanent failure
In our example, if the cause of the failure is an expired certificate when the connec-
tion is attempted, CouldNotConnect may be classified as a permanent failure. This type
of failure does not autorepair and requires manual intervention; an operator must
provide a new, valid certificate.
3.3.4 Failure detection
The first component of failure handling is failure detection, which refers to a mechanism
that detects whether a failure has occurred. But I struggle with the notion of failure
detectors in distributed systems. Most authors of related books focus on detecting com-
ponent failures such as crashes (Crash-Stop, Omission, and Crash-Recovery failures;
see chapter 2).
In their paper ‚ÄúUnreliable failure detectors for reliable distributed systems‚Äù (https://
dl.acm.org/doi/10.1145/226643.226647), Tushar Deepak Chandra and Sam Toueg
describe failure detectors this way: ‚ÄúWe consider distributed failure detectors: each pro-
cess has access to a local failure detector module. Each local module monitors a subset
of the processes in the system, and maintains a list of those it currently suspects to have
crashed.‚Äù That is how most blog posts, papers, and books about distributed systems that
I am aware of present failure detectors. But when I think about failure detection and
failure detectors, I prefer to cast a wider net.
Let‚Äôs recall the informal definition of failure: a failure is an unwanted yet possible state
transition that leaves the system in an illegal state. In my perspective, a failure detector
is a witness (predicate) that confirms the occurrence of a failure‚Äîan unwanted yet pos-
sible state transition that leaves the system in an illegal state. Often, these witnesses are
integrated into the underlying algorithm or protocol itself.
51In practice
Detecting an application-level failure may be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, for example, an
InsufficientFunds response indicates a failure on the application level.
Similarly, detecting a platform-level failure can be as simple as examining the return
code of a step. When an attempt is made to charge a credit card, a CouldNotConnect
response indicates a failure on the platform level.
Nonetheless, we must detect complete unavailability. In this case, failure detection
pertains to the widely discussed mechanisms mentioned previously: identifying whether
a particular component is experiencing a component failure. The failure detector has
various implementations, but all of them function by exchanging messages at set inter-
vals between the observer and the observed component:
¬° In a pull scenario, the observer sends a request to the observed component at a
set frequency and anticipates a response within a designated time interval.
¬° In a push scenario, the observed component sends messages, typically called
heartbeats, to the observer at a set frequency.
In this context, the witness of a crash is a timeout. If the observing component fails to
receive a message from the observed component within a specified timeframe, it pre-
sumes that the observed component has crashed. This assumption, however, may be
incorrect. Consider the following scenarios:
¬° In a synchronous system that communicates over a reliable network, a missing mes-
sage indicates a failure with certainty.
¬° In a partially synchronous or asynchronous system communicating over an unreliable
network, a missing message does not necessarily indicate a failure. The message
may have been delayed (in the case of a partially synchronous or asynchronous
system) or lost (in the case of an unreliable network).
Failure detection is complete if the failure detector eventually identifies every actual fail-
ure, and it‚Äôs accurate if every identified failure is an actual failure. In other words, com-
pleteness implies no misses (false negatives), and accuracy implies no mistakes (false
positives).
In a partially synchronous or asynchronous system communicating over an unreli-
able network, it is impossible to achieve a complete and accurate failure detector due to
the inherent uncertainty about whether a message was delayed or lost.
Aha! moment: Who supervises the supervisor?
When one component assumes the role of the observer and monitors another com-
ponent acting as the observed, it raises the question of who monitors the observer.
In practice, we can use many observation (supervision) strategies, with the program-
ming environment Erlang/Open Telecom Platform (OTP; https://mng.bz/8XBZ) being
arguably the most famous for its elaborate supervision trees.
52 chapter 3 Failure tolerance
(continued)
But there comes a point where we reach the end of the road and enter intolerable ter-
ritory. Human intervention is required.
3.3.5 Failure mitigation
The second component of failure handling is fail-
ure mitigation, which refers to a mechanism that
resolves a (suspected) failure. Broadly speaking,
there are two failure management techniques:
backward recovery and forward recovery (see fig-
ure 3.15).
backWard recovery
Backward failure recovery refers to a failure-mit-
igation strategy that transitions the system from
the intermediary, illegal state to a state that is
equivalent to the initial, legal state: it moves the
process backward. As a rule of thumb, backward
failure recovery does not require repairing the underlying cause of the failure because
we do not try to push past it. Backward failure recovery is a common application-level
failure mitigation strategy in the form of compensation.
forWard recovery
Forward failure recovery refers to a failure-mitigation strategy that transitions the system
from the intermediary, illegal state to a state that is equivalent to the final, legal state: it
moves the process forward. As a rule of thumb, forward failure recovery requires repair-
ing the underlying cause of the failure because we try to push past it. Forward failure
recovery is a common platform-level failure mitigation strategy in the form of retries.
3.3.6 Putting everything together
This section outlines an ideal failure-handling strategy with the highest likelihood of
completing a process execution successfully in the event of a failure (see figure 3.16).
application-level failure mitigation
When a failure may unambiguously be classified as an application-level failure, the
application layer is responsible for mitigating the failure. The InsufficientFunds
Exception in our e-commerce example, for example, is an application-level failure
that can be tackled with backward recovery by compensating for the steps that have
already occurred.
platform-level failure mitigation
If a failure cannot be unambiguously classified as an application-level failure, or if the
failure can be unambiguously classified as a platform-level failure, the platform layer is
Failure
mitigation
Backward
recovery
Forward
recovery
Figure 3.15 Failure mitigation
53Summary
Failure Assuming
‚Ä¢ Platform-level
‚Ä¢ Transient
‚Ä¢ Autorepair
Retry Assuming
‚Ä¢ Platform-level
‚Ä¢ Intermittent
‚Ä¢ Autorepair
Retry
Retry
Repair
retry
Repair
retry
Assuming
‚Ä¢ Platform-level
‚Ä¢ Permanent
‚Ä¢ Manual repair
Assuming
‚Ä¢ Application-level Failure
Success
Figure 3.16 Outline of failure-handling strategy in an orchestration scenario
responsible for mitigating the failure. A possible failure-mitigation strategy consists of
three escalating steps:
1 We assume that a failure is a platform-level, transient, autorepair failure; to miti-
gate the failure, we issue an immediate retry.
2 We assume that a failure is a platform-level, intermittent, autorepair failure; to
mitigate the failure, we schedule multiple retries with a backoff strategy.
3 We assume that a failure is a platform-level, permanent, manual-repair failure; to
allow for mitigation, we suspend the process, repair the underlying condition of
the failure, and resume the process to retry.
If these mitigation efforts are ultimately unsuccessful, we have to elevate the failure to
an application-level failure, presenting the failure to the application.
be vigilant
What happens if backward recovery encounters a failure? Suppose that in the course of
backward recovery, we need to refund a credit card charge, but the refund fails. What
can we do?
We must elevate the failure from the application layer to the human layer. The oper-
ators of the software system are responsible for detecting and mitigating this failure. In
the worst-case scenario, someone must sign and mail a refund check to the customer.
Summary
¬° Failure tolerance is the goal of failure handling.
¬° Failure handling involves two key steps: failure detection and failure mitigation.
¬° Failures can be classified across two dimensions: spatial and temporal.
¬° Spatially, failures are classified as application-level or platform-level.
¬° Temporally, failures are classified as transient, intermittent, or permanent.
54 chapter 3 Failure tolerance
¬° Failure tolerance strategies such as masking, nonmasking, and fail-safe address
the safety and liveness of the system.
¬° Failure detection and mitigation strategies vary based on the classification of the
failure and the desired class of failure tolerance.
55
4Message delivery
and processing
This chapter covers
¬° Message delivery and message processing
¬° At-least-once, at-most-once, and exactly-once
delivery and processing
¬° Equivalence and idempotence
When we talk about message delivery and processing, we may come across numerous
misconceptions and instances of borderline deception, which can be detrimental to
forming accurate, concise mental models and can cloud our understanding. Some
distributed system engineers use the term exactly-once processing, for example, but
what they really mean is exactly-once processing semantics. Exactly-once processing
refers to the ideal scenario in which each message is processed only once, whereas
exactly-once processing semantics means that the system behaves as though each
message is processed only once despite potential underlying retries or duplicates.
Another example is marketing material that claims a software product guarantees
exactly-once processing semantics to make the product more appealing while failing
to disclose the limitations of that guarantee.
56 chapter 4 Message delivery and processing
If you have already established an accurate, concise mental model, misconcep-
tions and misinformation will not affect you. But if you are still developing that mental
model, misconceptions or misinformation may lead you to design a system based on
assumptions of guarantees that were never provided. In this chapter, we will be rigor-
ously accurate and concise as we develop a mental model that is also rigorously accurate
and concise.
4.1 Exchanging messages
As defined in chapter 1, a distributed system is a set of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state. A change in a component‚Äôs local state is known
as a side effect.
As outlined in chapter 1, we adopt a partially synchronous system model: unreliable
components communicating over an unreliable network. The components and the net-
work operate synchronously most of the time but asynchronously sometimes. Compo-
nents may fail due to crashes, but they may also recover, forgetting some state (volatile)
and remembering other state (durable). The network may reorder, drop, or duplicate
messages. We exclude Byzantine failures. That is, we do not allow components to behave
arbitrarily, including performing actions of deception such as intentionally deviating
from the intended algorithm.
For the remainder of this chapter, we will think in terms of two components hav-
ing a dialogue. The sender and receiver interact through a request-response-style mes-
sage exchange. Although both components send and receive messages, we refer to the
component submitting the request as the sender and the component submitting the
response as the receiver. In this scenario, the sender sends a request to the receiver, which
processes the message, potentially causing a side effect, before sending a response to
the sender.
Figure 4.1 depicts a Petri net capturing one request-response dialogue. Petri nets
model the dynamic aspects of concurrent systems. Places (circles) represent the control
state of the system, and transitions (rectangles) represent the events of the system. A
transition is enabled if each input place contains a token and each output place does
not contain a token. When a transition fires, the transition consumes the tokens from
its input places and produces tokens in its output places.
Figure 4.2 depicts the step-by-step progression of the request-response dialogue,
illustrating how the dialogue unfolds:
1 The sender submits a request to the network.
2 The network delivers the request to the receiver.
3 The receiver processes the request, potentially causing a side effect.
4 The receiver submits a response to the network.
5 The network delivers the response to the sender.
57Exchanging messages
Certain
Send
request
Uncertain
Certain
Recv
request
Recv
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
Figure 4.1 Request-response dialogue between two components
5
1
3
4
2
Figure 4.2 Step-by-step progression of the request-response dialogue
58 chapter 4 Message delivery and processing
We can think of message exchange as being two separate aspects:
¬° Message delivery refers to the transfer of a message within a network, as well as the
presentation of the message from the network to the component.
¬° Message processing refers to both the generation of a response and the creation of
a side effect.
This chapter explores message delivery and processing that occurs at most once, at
least once, and exactly once, as described in table 4.1 and table 4.2. These categories
cover all possible delivery and processing behaviors, each imposing distinct guarantees
that systems must account for.
Table 4.1 Message delivery
Message delivery On the receiver side
At most once The network delivers the message zero times or one time.
At least once The network delivers the message one or more times.
Exactly once The network delivers the message once.
Table 4.2 Message processing
Message processing On the receiver side
At most once The receiver processes the message zero times or one time.
At least once The receiver processes the message one or more times.
Exactly once The receiver processes the message once.
Generally, in terms of at-most-once, at-least-once, and exactly-once semantics, our focus
is on message processing rather than message delivery. Message processing embodies
the application logic, which includes any side effects.
4.2 The uncertainty principle of message delivery and processing
A component can observe only its own state and channel to the network, as discussed
in chapter 1 (global view versus local view). This gives the component a limited local
view (see figure 4.3).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 4.3 C1‚Äôs point of view: there is only C1 plus the rest of the system.
59The uncertainty principle of message delivery and processing
This limited view causes the sender to transition from a state of certainty to a state of
uncertainty, with the ideal goal of returning to a state of certainty. See the Sender side
of figure 4.1.
4.2.1 Before sending the request
At this point, the sender can be certain that no work has been done and no side effects
have occurred.
4.2.2 After sending the request and before receiving a response
At this point, the sender cannot be certain whether the work has been done, is about
to be done, or will never be done. The sender cannot distinguish among these cases:
¬° The request was lost in the network.
¬° The receiver crashed before processing the message.
¬° The receiver crashed after processing the message.
¬° The response was lost in the network.
¬° The network or the receiver is slow.
4.2.3 After receiving a response
At this point, the sender can again be certain that the response indicates a success (so
the work has been done) or the response indicates a failure (so the work has not been
done). Remember that there are no Byzantine failures: components are not allowed to
deceive.
Aha! moment: Distributed systems and Transmission Control Protocol
Q: Doesn‚Äôt TCP prevent message loss in the network?
A: Although TCP is considered a reliable protocol, TCP will not save you.
TCP is a reliable delivery protocol that guarantees the accurate, complete transmis-
sion of data between the sender and receiver, achieved through acknowledgments
and retransmissions. TCP is implemented in the operating system kernel and made
available to applications via the socket interface. On both the sender and receiver
sides, TCP writes data to a local buffer and then acknowledges receipt of the data.
If something goes awry, TCP retransmits data that has already been transmitted and
deduplicates data that has already been received.
So are you safe? Unfortunately, no. The kernel acknowledges receipt before attempt-
ing to deliver data to the application, transitioning from at-least-once delivery seman-
tics to at-most-once delivery semantics.
At-least-once delivery semantics on a lower layer does not entail at-least-once deliv-
ery semantics on a higher layer. If we trip after acknowledging receipt but before deliv-
ering and processing the message, the message will be lost. In other words, TCP
cannot guarantee that it will prevent message loss.
60 chapter 4 Message delivery and processing
(continued)
Application Application
Sender Receiver
Component
layer
Network
layer
Kernel
(sockets)
Buffer
Kernel
(sockets)
Buffer
At least once
At most once Message
loss
User spaceKernel space
User space Kernel space
TCP message exchange
4.3 Silence and chatter
After sending a request but before receiving a response, we are in a state of uncertainty
that we ultimately need to resolve. Timeouts (discussed in chapter 2) are common
failure detection mechanisms. When sending a request, the sender sets a time limit for
an expected response. If the sender does not receive a response within the time limit‚Äî
that is, if a timeout occurs‚Äîthe sender suspects that the request or response was lost
or that the receiver crashed. Because our system model is only partially synchronous,
however, the system provides limited timing guarantees:
¬° If the system currently operates synchronously, it provides timing guarantees,
and a timeout is an accurate witness (true positive).
¬° If the system currently operates asynchronously, it cannot provide any timing
guarantees, and a timeout may not be an accurate witness (true positives or false
positives).
In the absence of a response, the sender truly does not know and has no way of know-
ing whether a side effect occurred or will occur in the future. But we have challenges
beyond the sender not receiving a response from the receiver. Another challenge is
the receiver receiving duplicate requests from the sender. Our partially synchronous
system model does not allow for message-loss failures or for message-duplication fail-
ures. At this point, we begin to understand that guaranteeing exactly-once delivery or
processing is impossible.
In the absence of a failure or a delay, the sender sends a request and receives a
response. In the presence of a failure or a delay, the sender sends a request but does
61Silence and chatter
not receive a response. The sender has exactly two choices: move on or try again. If the
sender moves on, the sender intends to deliver and process a message at most once‚Äî
that is, zero times or one time. Messages may get duplicated in the network, however. If
the sender tries again, the sender intends to deliver and process a message at least once:
one or more times. But messages may continuously get lost in the network. Understand-
ing that guaranteeing exactly-once processing is impossible, can we guarantee exactly-
once processing semantics?
Aha! moment: Two failed attempts
To solidify our understanding of the fundamental impossibility of exactly-once pro-
cessing, let‚Äôs look at two implementations that come up short. On the receiver side,
we will keep track of the requests that were previously delivered or processed.
The following code illustrates an algorithm that tracks a request before processing the
request. If a duplicate gets delivered, the if statement skips the message handler:
requests := { }
on message request do
if request in received then
return
end
received := received + request
process(request) // e.g. transfer balance
return
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after tracking the request but before processing the request, when a retry gets deliv-
ered, the if statement shorts the message handler. The message is not processed,
amounting to at-most-once processing.
Let‚Äôs try the other way around. The following code illustrates an algorithm that pro-
cesses a request before tracking the request. If a duplicate gets delivered, the if
statement short-circuits the message handler:
requests := { }
on message request do
if request in received then
Variable to track requests
Event handler to be called
on receiving a request
If this request is in the set of
previously received requests, exits
Tracks the request as received
before processing the request
If the message handler crashes
here, the message is already
marked, although we have not
processed the message. A retry
will not be processed either.
62 chapter 4 Message delivery and processing
(continued)
return response
end
process(request) // e.g. transfer balance
received = received + request
return response
end
Does this guarantee exactly-once processing? No. If the message handler crashes
after processing the request but before tracking the request, when a retry gets deliv-
ered, the if statement will not short-circuit the message handler. The message is
processed again, amounting to at-least-once processing.
4.4 Exactly-once processing semantics
Exactly-once processing semantics focus on the processing outcome rather than the process-
ing itself. These semantics guarantee that the state of the system processing a message
once is equivalent to the state of the system processing the message two or more times:
P(m) ‚â° P(m) ‚Ä¢ ‚Ä¶ P(m)
Note that we are not talking in terms of equality but in terms of equivalence. In general,
we cannot guarantee that the state of the system processing a message once is equal to
the state of the system executing the message two or more times. The simplest example
is logging. A system may log that it processed a message. In the case of two or more pro-
cessing operations, there are two or more log entries. We generally regard the duplica-
tion of log entries as inconsequential, however. The system is not equal but equivalent.
Aha! moment: Equivalence is in the eye of the beholder
Like correctness guarantees, equivalence is specific to the application. As a software
engineer, you can define equivalence. In other words, what you consider equivalent, I
may consider different. Equivalence is in the eye of the beholder.
Exactly-once processing semantics are an end-to-end guarantee. Both the sender and
the receiver play important roles.
Self-evidently, guaranteeing exactly-once processing semantics requires guarantee-
ing at-least-once delivery and processing: The sender must not stop submitting requests
until it receives a response, and the receiver must not skip processing.
4.5 Idempotence
Now we run into the situation in which message processing may happen multiple
times. How can we guarantee that the state of the system is equivalent to processing
If the message handler crashes
here, the message is already
processed, although we have
not tracked the message. A
retry will be processed again.
Marks the request as received
after processing the request
63Idempotence
happening exactly once? The answer is idempotence. Idempotence refers to the property
of an operation where applying the operation multiple times yields the same value as
applying the operation once:
f(f(x)) = f(x)
Similarly, in the context of distributed systems, idempotence refers to the property of
an operation where executing the operation multiple times has an equivalent effect to
executing the operation once. If message processing consists of an idempotent opera-
tion, at-least-once message processing guarantees have exactly-once processing seman-
tics guarantees.
The bad news is that guaranteeing idempotence can be extremely challenging. Like
exactly-once processing semantics, idempotence is specific to the application. Also like
exactly-once processing semantics, idempotence is an end-to-end guarantee: Both the
sender and the receiver play important roles in ensuring idempotence.
Some operations on some data structures are inherently idempotent. Consider a
write-once register that starts with the initial value NULL. The first request to set the value
locks in its value forever. Any subsequent requests do not alter the value. The operation
is inherently idempotent: If a request is duplicated but the request has already been
processed, the register is in exactly the same state.
Most operations on most data structures are not inherently idempotent, however. Con-
sider charging a credit card. This operation is inherently not idempotent. If a request gets
duplicated but the request has already been processed, the credit card gets charged twice.
A common pattern in implementing idempotence uses an idempotence key: The
sender generates a universally unique identifier (UUID) and attaches the UUID to the
request and every retry of the request. The receiver promises to deduplicate the request
and its retries based on the UUID.
A common pattern in implementing receiver-side deduplication transactionally
records and processes the request, as shown in the following listing. An application
might use a separate database table to track idempotence keys and commits if‚Äîand
only if‚Äîa new entry for the idempotence key can be made without violating a unique-
ness constraint alongside the side effect.
Listing 4.3 Transactional recording and processing
requests := { }
on message request do
if request in received then
return response
end
begin transaction
process(request)
received = received + request
Transactionally processes
and tracks the message
Either both happen or none happens.
64 chapter 4 Message delivery and processing
commit transaction
return response
end
4.6 Case study: Charging a credit card
To illustrate exactly-once processing semantics and idempotence, we‚Äôll use the exam-
ple of booking an airline ticket. One step in this process involves charging the custom-
er‚Äôs credit card (see figure 4.4).
...Charge credit card....
Figure 4.4 Booking an airline ticket
We want to guarantee that the credit card is charged exactly once per purchase‚Äînot
zero times or two or more times. In the absence of a failure or a delay, the sender sends
a request and receives a response. In the presence of a failure or a delay, the sender
sends a request but does not receive a response (see figure 4.5).
Send
request
Receive
request
Receive
response
Send
response
Charge
credit card.
Failure before
processing
Failure after
processing
Figure 4.5 Request-response dialogue charging a credit card
65Summary
In the absence of a response, the web app or mobile app may retry a bounded num-
ber of times, ultimately informing the user if all retries fail to yield a response. In that
case, the user may have to contact support to determine whether a ticket was issued,
although most likely, a confirmation email or a refresh of the app‚Äôs My Trips tab will
bring some certainty.
How could the app ensure exactly-once processing semantics and idempotence?
Because exactly-once processing semantics and idempotence are in the eye of the
beholder, there are many possibilities and different user experiences.
Some payment processors, such as Stripe, support idempotence keys in their APIs,
for example. Consider this passage from the Stripe website (https://docs.stripe.com/
api/idempotent_requests):
Stripe‚Äôs idempotency works by saving the resulting status code and body of the first
request made for any given idempotency key, regardless of whether it succeeded or
failed. Subsequent requests with the same key return the same result, including 500
errors.
Stripe‚Äôs idempotency, however, relies on the sender‚Äôs ability to generate idempotency
keys. Here, assuming that the reservation number is itself a UUID, the sender may use
the reservation number as an idempotence key.
Other options exist. Following is a recounting of a particularly frustrating experience.
While I was using an airline‚Äôs mobile app to book a ticket, the app repeatedly crashed
upon checkout. I restarted the app multiple times to resume the checkout process,
eventually giving up after four or five tries.
When I checked my credit card activity, to my dismay, I found four or five pending
charges. I contacted the airline‚Äôs support team, which assured me that within 24 hours,
only one pending charge would take effect.
The airline‚Äôs software engineers considered this situation to be exactly-once process-
ing semantics. In their eyes, the state of the system processing a request exactly once was
equivalent to the state of the system processing a request two or more times. I disagree. I
was in a very different state of mind; seeing four or five charges on my credit card made
me uneasy.
Consider my experience a cautionary tale. When you implement exactly-once pro-
cessing semantics, consider the resulting experience for your users.
Summary
¬° In the absence of failure, exactly-once message delivery and processing are trivial.
¬° In the presence of failure, exactly-once message delivery and processing are
impossible.
¬° We can guarantee exactly-once processing semantics if we can guarantee at-least-
once delivery, at-least-once delivery processing, and an idempotent processing
step.
66 chapter 4 Message delivery and processing
¬° Guaranteeing idempotence is one of the most complex aspects of distributed
systems, and a one-size-fits-all solution does not exist.
¬° Guaranteeing exactly-once processing semantics and idempotence requires all
your skill and all your wits.
¬° Various challenges include message reordering, message duplication, message
loss, crashes, and delays.
67
5Transactions
This chapter covers
¬° The concept of abstractions
¬° The concept of transactions
¬° The significance of transactions
¬° The implementation of transactions
Transactions‚Äîthe most popular, most powerful abstractions in the history of soft-
ware engineering‚Äîare rooted in the world of database systems, not in the world of
distributed systems. So why am I covering transactions in this book? Transactions
were swiftly and broadly adopted in the context of distributed systems and are rec-
ognized as the benchmark for an exceptional developer experience.
Transactions provide certainty in an uncertain world: Transactions allow you to
pretend that concurrency or failure does not exist!
Transactions stand as some of the most ubiquitous abstractions in the field of
software engineering. Almost every software engineer knows about transactions and
their fundamental principles, encapsulated by ACID (atomicity, consistency, isola-
tion, and durability). Most authors introduce and explain transactions in terms of
68 chapter 5 Transactions
ACID guarantees, but ACID presents the properties of transactions as separate, inde-
pendent concepts.
Chapter 1 emphasized the value of exploring different mental models and adopting
alternative thinking approaches to gain a holistic understanding of a topic. Building on
that notion, this chapter introduces and explains transactions from a different perspec-
tive, not through the lens of ACID guarantees but through the lens of correctness and
completeness guarantees.
5.1 Abstractions
In arguably the most fundamental work in computer science‚ÄîStructure and Interpreta-
tion of Computer Programs, by Harold Abelson and Gerald Jay Sussman (The MIT Press),
the term abstraction appears on page 1 of chapter 1 in headline 1. Abelson and Suss-
man identify abstractions as one of the three fundamental building blocks of computer
programs:
¬° Primitive expressions, which represent the simplest entities of the language
¬° Means of combination, by which compound elements are built from simpler
elements
¬° Means of abstraction, by which compound elements are named and manipulated
as a unit
Although this definition accurately cap-
tures the nature of abstractions, it does
not stress their essence: abstractions
have the power to transform domains of
discourse, effectively reshaping entire
worlds. In their book Modern Operating
Systems (Pearson), Andrew Tanenbaum
and Herbert Bos created a variation of
figure 5.1 to illustrate the transforma-
tive nature of abstractions.
The bottom level presents a set of
entities (ugly entities) that the higher
level consumes and transforms into a
different set of entities (beautiful enti-
ties). From the perspective of an oper-
ating system, hardware components such as the CPU, RAM, and disks represent the
ugly interface. The operating system takes in these entities and presents a beautiful
interface, processes, memory, and files.
Beauty lies in the eye of the beholder, however. From the point of view of database
systems, the operating system presents its ugly interface, and now processes, memory,
and files are undesirable. The database system consumes these entities and presents its
beautiful interface, tables consisting of rows and columns, and transactions.
Beautiful
interface
Ugly
interface
Level n+1
Level n
Figure 5.1 The transformative nature of
abstractions, according to Tanenbaum and Bos
69Abstractions
This example of databases and oper-
ating systems also emphasizes the recur-
sive relationships that exist between
abstractions. Higher-level abstractions
are built on lower-level abstractions until
we reach the fundamental primitives,
which represent the simplest entities in
the system.
Each time we transition across these
boundaries, moving from a database sys-
tem to an operating system and from an
operating system to hardware, we enter
a distinct world. This entails encoun-
tering different entities, relationships among those entities, and constraints on those
relationships. Each transition presents a different semantics. In his paper ‚ÄúLife beyond
Distributed Transactions: an Apostate‚Äôs Opinion‚Äù (https://ics.uci.edu/~cs223/papers/
cidr07p15.pdf), Pat Helland uses a variation of figure 5.2 to highlight the transforma-
tive nature of abstractions.
Instead of the terms beautiful and ugly, Helland uses the terms agnostic and aware.
More important, he emphasizes the API that maps the higher-level agnostic concepts
to the lower-level aware concepts. The API translates between worlds. I prefer figure
5.3 to highlight higher-level and lower-level abstractions and to reason about their
relationships.
Composition
Reduction
Figure 5.3 Equivalence between higher and lower levels
From a top-down perspective, we observe a reduction: an entity using the abstractions
of the higher level is reduced, transformed, or compiled into entities using the abstrac-
tions of the lower level. Conversely, from a bottom-up perspective, we see a composi-
tion: entities using the abstractions of the lower level are composed into an entity using
the abstractions of the higher level, and the entity on the higher level emerges.
Whether we look at this system from top down or bottom up, a notion of equiva-
lence exists. Whether we reason about the system in terms of higher-level abstractions
Agnostic
Aware
API
Figure 5.2 The transformative nature of
abstractions, according to Helland
70 chapter 5 Transactions
or lower-level abstractions, we come to the same conclusion. Simply stated, whether we
reason in terms of the higher or lower level, the system returns the same result.
This chapter explores transactions as abstractions that transform concurrency-
agnostic and failure-agnostic definitions into concurrency-aware, failure-aware, and
failure-tolerant executions. In other words, we will explore transactions as abstractions
that enable us to pretend that concurrency and failure are nonexistent.
5.2 The magic of transactions
Web applications are distributed applications. When I started developing web applica-
tions, I did not realize that they are distributed applications; I didn‚Äôt connect the dots.
How could I have missed that crucial connection? The answer is transactions.
Transactions guarantee correctness and completeness. If you build your application
on top of a transactional database system, transactional guarantees extend from the
database to your entire system. Transactions obfuscate the fact that you are building a
concurrent, partially synchronous distributed system consisting of components that are
subject to failure and communicating over an unreliable network.
The following listing illustrates a simplified money-transfer web API. The /transfer
endpoint accepts a post request containing three parameters: a source identifier, a
target identifier, and an amount. The endpoint transfers money from the source
account to the target account regardless of overdraft.
Listing 5.1 Simplified money transfer
app.post('/transfer', async (req, res) => {
const { source, target, amount } = req.body;
const query = `
BEGIN;
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT;`;
await db.none(query, [amount, source, target]);
res.status(200).end();
});
Do you see what is not part of this handler? The handler has no visible guardrails. There
is no visible detection or mitigation of concurrency, and there is no visible detection or
mitigation of failure.
5.2.1 Concurrency
As we learned in chapters 1 and 2, a system contains a race condition if the system has
multiple possible executions, some of which are considered correct and some of which
are considered incorrect. An update to a single account balance is inherently vulnera-
ble to race conditions. The process of updating an account balance consists of a read
No visible failure
detection or mitigation
71The magic of transactions
operation followed by a write operation. The results may differ based on the interleav-
ing of these operations for concurrent transfers.
Consider the concurrent transfers from the source account depicted in figure 5.4.
There are two concurrent transfers of 10 units each from the source account that initially
holds 50 units. The interleaving of read and write operations from transfer t1 and transfer
t2 results in a final balance of 40 units, which differs from our expected result of 30 units.
t1 (‚àí10)
t2 (‚àí10) wr
r=50 w(40)
wr
r=50 w(40)
balance = 40balance = 50
Figure 5.4 Possible
effects of concurrency
Transactions guarantee correctness, however. Although the query does not display any
visible attempt to detect, mitigate, or prevent race conditions, transactions execute as
though concurrency does not exist.
5.2.2 Failure
Recall from chapter 2 that a system is subject to component and network failures. An
update to two account balances is inherently vulnerable to failure. The process of
updating two account balances consists of a sequence of two write operations. (For this
example, we may ignore the reads.) If the execution fails after the first write but before
the second write, the transfer is partially applied.
Consider the transfers from the source account and target account depicted in fig-
ure 5.5. There is a transfer of 10 units from the source account to the target account,
both initially holding 50 units. If the execution fails after the first write but before the
second write, the transfer results in a final balance of 40 units and 50 units, which differs
from our expected result of 40 units and 60 units.
ww
w(40) w(60)
t(‚àí10)
balance = 50balance = 50
balance = 50 balance = 40
Figure 5.5 Possible
effects of failure
But transactions guarantee completeness. Although the query does not display any vis-
ible attempt to detect, mitigate, or prevent failures, transactions execute as though
failure does not exist.
In summary, although there are no visible concurrency, failure-detection, or mitiga-
tion steps, transactions guarantee correctness and completeness even in the presence
72 chapter 5 Transactions
of concurrency and failure. A concurrency and failure-agnostic definition guarantees
a concurrency and failure-aware and tolerant execution. It is a remarkable result and a
delightful developer experience.
5.3 The model of transactions
A transaction is a sequence of operations on a set of objects that ends with exactly one
commit or exactly one abort. A transaction guarantees both correctness and complete-
ness. These guarantees can be attributed to the combination of four fundamental prin-
ciples famously referred to as the ACID guarantees.
Fun fact about ACID
According to James ‚ÄúJim‚Äù Gray, the 1998 Turing Award winner for his seminal con-
tributions to database and transaction processing, his colleague Andreas Reuter
coined the acronym ACID because his wife, Christiane Reuter, ‚Äúhates sweet things
and loves vinegar.‚Äù
ACID characterizes transactions as follows:
¬° Atomicity guarantees that a transaction executes observably equivalent to com-
pletely or not at all.
¬° Consistency guarantees that a transaction transitions the database from one con-
sistent state to another consistent state.
¬° Isolation guarantees that a transaction executes as though it is the only transac-
tion executing, preventing interference.
¬° Durability guarantees that when a transaction commits, its effects are permanent,
preventing it from going back on its word.
Aha! moment: Consistency
Consistency sets itself apart from atomicity, isolation, and durability because it is an
application-level guarantee, whereas the latter three are platform-level guarantees. To
illustrate this distinction, consider two SQL snippets that define the structure of an
accounts table.
In the following listing, the balance is defined as an integer with no additional con-
straints. As a result, the balance can hold both positive and negative integer values.
1. Application that does allow overdraft
CREATE TABLE accounts (
...
balance INT
...
);
No constraint on balance
73The model of transactions
By contrast, the next listing defines the balance as an integer with an additional con-
straint in the form of a CHECK clause. This constraint ensures that the balance can
hold only non-negative integer values.
2. Application that does not allow overdraft
CREATE TABLE accounts(
...
balance INT CHECK(balance >= 0)
...
);
Consistency, as an application-level guarantee, can differ based on the requirements
of the application. Developers have the flexibility to establish and enforce consis-
tency rules that align with their needs. In the first case, the database system com-
mits a transaction if the balance of the account falls below zero. In the second case,
the database system aborts a transaction if the balance of the account falls below
zero. By contrast, atomicity, isolation, and durability are platform-level guarantees
that are not affected by application-level semantics.
NOTE Originally, isolation guaranteed that a transaction executes as though
no other transactions were executing, a concept referred to as serializability
(explained in section 5.3.2). Modern databases provide many isolation levels
that extend beyond the scope of this book.
In general, we think of a database as a col-
lection of tables consisting of rows and col-
umns. For our purposes, we‚Äôll adopt a more
abstract perspective, thinking of a database
as a collection of objects, each represented
by a name-value pair.
Figure 5.6 illustrates the relationship be-
tween program and execution. A program
is a sequence of operations performed on
a set of names. When executed, a program
results in a transaction. A transaction is a sequence of actions executed on a set of objects,
concluding with exactly one commit or exactly one abort. Essentially, a program is the
definition of a transaction, and a transaction is the execution of a program (see table 5.1).
Table 5.1 Design time and run time
Design time/definition Run time/execution
Program P Transaction t
Operation A Action a
Name N Object o
The balance must be non-negative.
P = [<A1, N1>, <A2, N2>, <A3, N2>]
t = [<a1, o1>, <a2, o2>, <a3, o2>]
Definition
Execution
Figure 5.6 Definition and execution
74 chapter 5 Transactions
We model a transaction as a trace, which consists of a sequence of triples in the form of
‚ü®tx, ai, oi‚ü©, where
¬° tx represents the transaction.
¬° ai represents the operation.
¬° oi represents the object modified by the operation.
The trace of a transaction tx can be expressed as tx = [‚ü®tx, ai, oi‚ü© | i = 1 .. n]. The inclu-
sion of the transaction tx in the triple allows us to analyze the interleaving of multiple
transactions.
The database may interleave the execution of two or more transactions. This exe-
cution of a set of transactions is referred to as a history. A history is denoted by this
sequence:
history = [‚ü®tij, ai, oi‚ü© | j = 1 .. m, i = 1 .. n]
The database system translates the failure-agnostic definition into a failure-aware and
failure-tolerant execution to guarantee correctness and completeness (see figure 5.7).
Database
system
Failure-agnostic
Definition
Failure-tolerant
Execution
Figure 5.7 Translation of failure-agnostic definitions into failure-aware and failure-tolerant executions
5.3.1 Correctness
In this section, we‚Äôll explore how a database system ensures correctness‚Äîthat is, how a
database system translates a concurrency-agnostic definition into a concurrency-aware
and concurrency-tolerant execution. In other words, the database system handles the
detection and mitigation of concurrency anomalies without burdening the developer.
A transaction transitions the database from a consistent state to a consistent state. We
can express that idea as a Hoare triple, which states the precondition, transaction, and
postcondition:
{ C } t‚Çì { C }
It‚Äôs important to note, however, that consistency may be violated temporarily during
the execution of a transaction (see figure 5.8). In the case of a funds-transfer trans-
action between source and target accounts, the temporary inconsistency arises when
the money is debited from the source account before being credited to the target
account.
75The model of transactions
<name1, value1>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3>
<name1, value1'>
<name2, value2'>
<name3, value3'>
<t, a1> <t, a3><t, a2>
Consistency
predicate holds.
Consistency predicate
may not hold.
Consistency
predicate holds.
t = [<t, a1, name1>, <t, name2>, <t, a3, name3>]
Figure 5.8 Temporary consistency violation
In a nonconcurrent setting, each transaction begins with a consistent state and pro-
duces a consistent state. When transactions are executed concurrently, one transaction
may observe inconsistencies introduced by another transaction.
This observation leads to a straightforward conclusion: if transaction t1 transitions
the database from a consistent state to a consistent state and transaction t2 also transi-
tions the database from a consistent state to a consistent state, the sequential composi-
tion of t1 and t2 also ensures the transition from a consistent state to a consistent state.
In other words, a history without concurrency, known as a serial history, does not exhibit
concurrency anomalies:
{ C } t1 { C } ‚àß { C } t2 { C } => { C } t1;t2 { C }
As a result, two concurrent transactions, t1 and t2, execute correctly if the combined
effects of their concurrent execution are equivalent to the effects of executing t1 before
t2 or t2 before t1:
{ C } t1||t2 { C } => { C } t1;t2 { C } ‚à® { C } t2;t1 { C }
This guarantee of correctness is referred to as serializability, in which concurrency is
allowed only if it does not introduce inconsistencies in the database.
5.3.2 Serializability
Serializability is a consistency model. A consistency model is a predicate on execution
histories that groups execution histories into good, legal, or valid and bad, illegal, or
invalid categories (see figure 5.9).
Consistency models represent correctness guarantees, and as a software engineer,
you have the authority to define the consistency model for your system, as discussed in
chapter 2. Introduced by Edgar Codd, serializability acts as a consistency model for the
concurrent execution of a set of transactions. Serializability is based on the concept
76 chapter 5 Transactions
of equivalence of a concurrent execution
to a sequential execution. In the concur-
rent execution of two transactions, t1 and
t2, the execution is serializable if the tuple
of (transaction result, database state) is
equivalent to either of the following:
¬° The sequential execution of t1 ‚Ä¢ t2
¬° The sequential execution of t2 ‚Ä¢ t1
In figure 5.10, the history is serializable
if the interleaved execution of actions a
‚Ä¢ c ‚Ä¢ b ‚Ä¢ d is equivalent to either of the
following:
¬° (a ‚Ä¢ b) ‚Ä¢ (c ‚Ä¢ d)
¬° (c ‚Ä¢ d) ‚Ä¢ (a ‚Ä¢ b)
NOTE Serializability does not require equivalence to a specific sequential
execution‚Äîonly to some sequential execution.
CommitBegin C D
CommitBegin A B
C DA B
t1
t 2
time
Figure 5.10 Serializability
Aha! moment: Implementing serializability
Implementing serializability is trivial. Implementing serializability efficiently is hard.
Because this book is not about databases, I‚Äôll outline the general idea.
To implement serializability, we could guard access to all objects with one lock.
Only the transaction holding the lock is allowed to proceed. Here, we are forcing a
A consistency model
guarantees good histories.
Good
histories
Bad
histories
Figure 5.9 Good and bad histories
77The model of transactions
sequential schedule. We can start using more fine-grained locks to guard the access
to subsets of all objects. Also, we are allowing a concurrent schedule that still exhib-
its sequential semantics. Our goal must be to refine our implementation to allow the
highest degree of concurrency while guaranteeing sequential semantics.
5.3.3 Completeness
This section explores how a database system ensures completeness‚Äîthat is, how a
database system translates a failure-agnostic definition into a failure-aware and failure-
tolerant execution. The database system handles the detection and mitigation of fail-
ure without burdening the developer.
Database systems employ various strategies for failure detection and mitigation. As
discussed in chapter 3, there are two broad techniques: backward recovery and forward
recovery. In the context of database systems, backward recovery is referred to as Undo,
and forward recovery is referred to as Redo. Although actual databases may combine
both Undo and Redo approaches, this section focuses on Undo.
The concept of Undo is straightforward: every operation is accompanied by an Undo
operation. The database manages a data structure called the Transaction Undo Log,
represented by an object ou. Before executing an operation on an object, the system
records the corresponding Undo operation in the Transaction Undo Log.
Thus, instead of directly executing an operation ‚ü®t, ai, oi‚ü©, the system performs two
operations: ‚ü®t, w, ou + ‚ü®¬¨ai, oi‚ü©‚ü© (representing the recording of the Undo operation) and
‚ü®t, ai, oi‚ü© (representing the original operation). Now there are two valid traces for a
transaction:
¬° Commit trace is the execution that unfolds in the absence of failure, executing all
regular operations:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..n]
¬° Abort trace is the execution that unfolds in the presence of failure, executing some
regular operations and their Undo operations, effectively restoring the system to
its state before the transaction:
tx = [‚ü®tx, ai, oi‚ü© | i = 1..k] [‚ü®tx, ¬¨ai, oi‚ü© | i = k..1]
We have two scenarios to consider: application-level abort and platform-level abort.
Application-level abort is straightforward. Platform-level abort introduces certain chal-
lenges that demand careful deliberation and consideration.
5.3.4 Application-level abort
An application-level abort occurs when a transaction is explicitly or implicitly aborted.
You trigger an explicit abort by explicitly issuing an abort command indicating the
termination of the transaction. An implicit abort is triggered by conditions such as a
78 chapter 5 Transactions
consistency violation that result in the automatic termination of the transaction. In an
application-level abort, the database system performs the Undo operations associated
with the transaction.
5.3.5 Platform-level abort
A platform-level abort occurs when a crash failure and subsequent restart occur at an arbi-
trary point during the execution of a transaction. In this case, when the database system
restarts, the database system has to perform recovery‚Äîthat is, examine its Transaction
Undo Log for any transaction that has not been committed and execute its Undo opera-
tions. When a platform-level abort occurs and the database system restarts, the database
system has to perform recovery. The database system examines its Transaction Undo
Log to identify any transactions that had not yet committed at the time of the crash fail-
ure. Then the database system performs Undo operations on those transactions.
There is one problem, however: recording an Undo operation and executing the oper-
ation are two separate steps. If the system crashes and restarts after the Undo operation is
recorded but before the operation is performed, the Undo operation is applied to the old
value of the object, and not to the new value. Also, if the system crashes and restarts during
Undo operations, Undo operations are performed multiple times. To prevent this problem,
Undo operations must be both idempotent and restartable (noop + idempotent):
¬° Noop (no operation)‚ÄîApplying an operation and its corresponding Undo opera-
tion on an object should be equivalent to applying the Undo operation only on
the object:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ¬¨ai, oi‚ü©
¬° Idempotent‚ÄîApplying an operation and its Undo operation multiple times should
have the same effect as applying them once:
‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© = ‚ü®tx, ai, oi‚ü© ‚ü®tx, ¬¨ai, oi‚ü© ... ‚ü®tx, ¬¨ai, oi‚ü©
Summary
¬° Abstractions are illusions, turning the ugly into the beautiful.
¬° Abstractions are layered. An entity using the abstractions of a higher level is
reduced to entities using the abstractions of the lower level.
¬° Transactions are abstractions that allow an application to pretend that concur-
rency and failure do not exist.
¬° The database system translates the concurrency and failure-agnostic definitions
into a concurrency and failure-aware and failure-tolerant executions.
¬° Transactions are commonly introduced and discussed in the context of ACID.
¬° By embracing a holistic viewpoint, we recognize that transactions create an
encompassing world in which the challenges of concurrency and failure become
virtually nonexistent.
79
6Distributed
transactions
This chapter covers
¬° The concept of distributed transactions
¬° The Two-Phase Commit protocol in the absence
of failure
¬° The Two-Phase Commit protocol in the presence
of failure
¬° Possible improvements of the Two-Phase Commit
protocol
Distributed transactions span changes across multiple systems. The participants in a
distributed transaction are often referred to as resource managers (RMs). The term
resource manager encompasses not only database systems but also other systems, such
as message queuing systems, that can participate in a distributed transaction. From
here on, I will use the terms resource manager and database system interchangeably.
6.1 Atomic commitment: From a single RM to multiple RMs
Let‚Äôs revisit the example money transfer discussed in chapter 5. The following list-
ing illustrates a transaction that executes on a single RM. It transfers money from
the source account to the target account regardless of overdraft.
80 chapter 6 Distributed transactions
Listing 6.1 Money transfer on one RM executing in one transaction
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
If the source and target accounts are hosted on two separate resource managers, how
do we guarantee completeness? The next listing illustrates two transactions that exe-
cute on two RMs, RM1 and RM2, to transfer money from the source account hosted on
RM1 to the target account hosted on RM2.
Listing 6.2 Money transfer on two RMs executing in two transactions
RM1:
BEGIN
UPDATE accounts SET balance = balance $1 WHERE id = $2;
COMMIT
RM2:
BEGIN
UPDATE accounts SET balance = balance + $1 WHERE id = $3;
COMMIT
How can we prevent disagree-
ment and guarantee that both
transactions either commit or
abort? In other words, how can
we guarantee that the composi-
tion of two (sub)transactions is
a transaction in itself (see figure
6.1)? The answer is atomic com-
mit protocols.
A transaction is a sequence of
operations on a set of objects
that ends with exactly one com-
mit or one abort. A transaction is
atomic, meaning that if it ends in
a commit, it executes observably
equivalent to once. If it ends in an abort, it executes observably equivalent to not at all.
Recall that a distributed system is a collection of concurrent components that com-
municate by sending and receiving messages over a network. Each component has
exclusive access to its own local state, which no other components can access. We may
think about the distributed system as though it proceeds in discrete steps, with either a
component or the network taking a step. On a single RM, atomic commit is guaranteed
Client
RM
Transaction
Source
account
Target
account
Distributed transaction
RM
Source
account
Client
Target
account
RM
Figure 6.1 From a single RM to multiple RMs
81Atomic commitment: From a single RM to multiple RMs
via writes to the local state. On multiple RMs, atomic commit is guaranteed via atomic
commit protocols (see figure 6.2).
Single
RM
Multiple
RMs
ww c
w c
w c
Coordinated
commits
Nondistributed transaction
Distributed transaction
Figure 6.2 How do we
coordinate and guarantee
multiple commits?
NOTE Transactions are commonly introduced and discussed in terms of ACID
(atomicity, consistency, isolation, durability) guarantees. We will focus our dis-
cussion of distributed transactions on atomicity, however, and not explicitly
discuss consistency, isolation, and durability.
6.1.1 Transaction on a single RM
For nondistributed transactions that execute on a single RM, atomicity is guaranteed
via one atomic write to its local state. When the RM commits or aborts a transaction,
it takes a single step, writing a commit or abort entry to its local state. If the RM fails
before writing a commit or abort entry, on recovery, it proceeds as though an abort
entry exists (see the top of figure 6.2).
6.1.2 Transaction on multiple RMs
For distributed transactions that execute on multiple RMs, atomicity is guaranteed
via an atomic commit protocol. Atomic commit ensures that a distributed transaction
either commits or aborts. That is, subtransactions unanimously commit or abort (see
the bottom of figure 6.2). The correctness of an atomic commit protocol is specified
via its safety and liveness guarantees:
¬° The safety guarantee asserts that no two participants in a transaction arrive at a
conflicting decision:
S: ‚àÑ rm‚ÇÅ, rm‚ÇÇ: state[rm‚ÇÅ] = commit & state[rm‚ÇÇ] = abort
82 chapter 6 Distributed transactions
In other words, there is no pair of RMs such that one is in the commit state and
the other is in the abort state.
¬° The liveness guarantee asserts that every participant will eventually arrive at a
decision:
L: ‚àÄ rm: <>[] state[rm] = commit | <>[] state[rm] = abort
In other words, the state of an RM is eventually always (forever) committed or
eventually always (forever) aborted.
6.1.3 Blocking and nonblocking
There are two types of atomic commit protocols: blocking and nonblocking. Blocking
protocols guarantee safety but not liveness in the presence of participant failure. Non-
blocking protocols guarantee both safety and liveness in the presence of a single partic-
ipant failure. For a commit protocol to be nonblocking, a single participant‚Äôs failure
does not prevent other participants from deciding whether the transaction is commit-
ted or aborted.
6.2 The essence of distributed transactions
Reasoning about atomic commit protocols,
including their safety and liveness guaran-
tees, is a challenging task, especially in the
presence of failures, requiring an accurate
and concise mental model of distributed
transactions. A distributed transaction,
also referred to as a global transaction, con-
sists of two or more nondistributed trans-
actions, also referred to as local transactions
(see figure 6.3).
We can think of a nondistributed trans-
action as being in one of four states: work-
ing, prepared, committed, or aborted (see
figure 6.4):
¬° Working‚ÄîThe transaction is executing operations. From here, it may decide to
transition to aborted (if a consistency constraint is violated, for example) or tran-
sition to prepared and wait for the request to commit or abort.
¬° Prepared‚ÄîThe transaction is not executing operations but has not transitioned
to committed or aborted yet. From here, the transaction may transition to com-
mitted on receiving a commit request or transition to aborted on receiving an
abort request.
¬° Committed or aborted‚ÄîThe transaction has reached its final, irreversible state by
being committed or aborted.
Distributed
transaction
Nondistributed
transaction
Nondistributed
transaction
consists of
.
.
.
Figure 6.3 A distributed transaction consists
of two or more nondistributed transactions.
83Two-Phase Commit protocol
Committed
Aborted
Working Prepared
Figure 6.4 State
machine of nondistributed
transactions
The state of a distributed transaction consists of a state vector that contains the states
of nondistributed transactions, as well as the outstanding messages in the network (see
figure 6.5).
RM 1 = working
RM 2 = working
RM 1 = prepared
RM 2 = working
RM 1 = prepared
RM 2 = prepared
RM 1 = committed
RM 2 = prepared
RM 1 = committed
RM 2 = committed
Figure 6.5 Global transaction (outstanding messages not illustrated)
6.3 Two-Phase Commit protocol
The Two-Phase Commit (2PC) protocol is the best-known and most-studied atomic
commit protocol. The 2PC protocol guarantees safety and liveness in the absence of
participant failure. In the presence of even one (specific) participant failure, however,
the protocol guarantees only safety, not necessarily liveness. Therefore, the protocol is
considered a blocking atomic commit protocol.
6.3.1 In the absence of failure
The 2PC protocol partitions the system into exactly one transaction coordinator (TC)
and two or more RMs. The protocol executes in two phases: Prepare and Commit.
A component called the client initiates a transaction with each participating RM and
executes standard read and write operations. But instead of asking the RMs directly to
commit or abort the transactions, the client asks the TC to commit or abort the transac-
tions, marking the start of 2PC protocol execution (see figure 6.6).
During the protocol‚Äôs first phase, the TC asks all participating RMs whether they vote
to commit or abort their local transactions. In the second phase, the TC instructs all
participating RMs to commit or abort.
84 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.6 2PC protocol execution
phase 1: prepare
1 The TC persistently records a ‚ü®Prepare, Timeout‚ü© entry to its log and sends a
‚ü®Prepare‚ü© request to all participating resource managers.
2 If RMi decides to commit, it persistently records a ‚ü®Vote to Commit‚ü© entry to its
log and sends ‚ü®Vote to Commit‚ü© to the TC.
or
If RMi decides to abort, it persistently records an ‚ü®Abort‚ü© entry to its log, sends
‚ü®Abort‚ü© to the TC, and aborts the transaction t1.
85Two-Phase Commit protocol
phase 2: commit
1 If the TC receives a ‚ü®Vote to Commit‚ü© response from all managers RMis, it per-
sistently records a ‚ü®Commit‚ü© entry to its log and sends a ‚ü®Commit‚ü© request to all
participating RMs.
2 If the TC receives an ‚ü®Abort‚ü© response from at least one participating RMi or a
timeout, it persistently records an ‚ü®Abort‚ü© entry to its log and sends an ‚ü®Abort‚ü©
request to all participating RMs.
3 Each participating RM receives the ‚ü®Commit‚ü© or ‚ü®Abort‚ü© request, persistently
records a ‚ü®Commit‚ü© or ‚ü®Abort‚ü© entry to its log, and commits or aborts its local
transaction ti.
Aha! moment: Vote-to-Commit vs. Abort
To this day, I occasionally find myself perplexed by the asymmetry inherent in the
Preparation phase of the protocol. The Preparation phase does not consist of a ‚ü®Vote
to Commit‚ü© and ‚ü®Vote to Abort‚ü© pair or a ‚ü®Commit‚ü© and ‚ü®Abort‚ü© pair. Instead, the
protocol consists of a ‚ü®Vote to Commit‚ü© and ‚ü®Abort‚ü© pair. In other words, during this
phase, an RM cannot unilaterally commit, but it can unilaterally abort.
6.3.2 In the presence of failure
In the absence of failure, the 2PC protocol guarantees both safety and liveness. In the
presence of failure, the 2PC protocol guarantees only safety, not liveness. As defined
in chapter 2, we assume a partially synchronous system model consisting of unreliable
components communicating over an unreliable network that acts synchronously most
of the time and asynchronously sometimes.
rm failure
The 2PC protocol guarantees both safety and liveness even when at least one RM fails.
A failure of one or more RMs does not block the protocol. If an RMi fails before per-
sistently recording ‚ü®Vote to Commit‚ü© or ‚ü®Abort‚ü© to its log, on recovery, the RMi per-
sistently records an ‚ü®Abort‚ü© and sends an ‚ü®Abort‚ü© to the TC (see figure 6.7).
begin ...
begin ... abort
We haven‚Äôt promised yet
on recovery
Send abort to transaction coordinator
Figure 6.7 An RM fails
before persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Vote to Commit‚ü© to its log, on recovery, the
RMi must ask the TC whether to commit or abort (see figure 6.8).
86 chapter 6 Distributed transactions
begin ... vote-to-commit
begin ... vote-to-commit
We already promised
on recovery
Inquiry about decision
Figure 6.8 An RM fails
after persistently recording
‚ü®Vote-To-Commit‚ü© or
‚ü®Abort‚ü©.
If an RMi fails after persistently recording ‚ü®Commit‚ü© to its log, on recovery, it must per-
form a REDO (see figure 6.9).
begin ... vote-to-commit . commit
begin ... vote-to-commit . commit
We already commited
on recovery
REDO to ensure operations take effect
Figure 6.9 An RM
fails after persistently
recording ‚ü®Commit‚ü©.
If an RMi fails after persistently recording ‚ü®Abort‚ü© to its log, on recovery, it must per-
form an UNDO (see figure 6.10).
begin ... vote-to-commit . abort
begin ... vote-to-commit . abort
We already aborted
on recovery
UNDO to ensure that no operation takes effect
Figure 6.10 An RM
fails after persistently
recording ‚ü®Abort‚ü©.
tc failure
The 2PC protocol guarantees safety but not liveness in case the TC fails. A failure of the
TC may block the protocol.
If the TC fails after a participating RMi persistently records ‚ü®Vote to Commit‚ü©, the
RMi is stuck. If the TC fails after a participating RM persistently recorded ‚ü®Vote to
Commit‚ü©, that RM is blocked until the TC is repaired.
Figure 6.11 illustrates a scenario in which all participating RMs voted to commit in
the Prepare phase but the TC failed in the Commit phase after informing the RM1 to
commit. As a result, RM2 can neither commit nor abort the transaction because it does
not know whether any other RMs have already committed or aborted.
6.3.3 Improvement
Many variations of the 2PC protocol have the goal of improving its liveness guarantees.
These variants have fewer blocking executions but still have some blocking executions.
87Two-Phase Commit protocol
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t 1
begin t 1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t 1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.11 Failure of TC after the first commit
Let‚Äôs revisit the scenario in which the TC fails. If the TC fails after a participating RMi
persistently recorded ‚ü®Vote to Commit‚ü©, the RMi is stuck.
One way to improve the protocol is to allow the RMi to communicate with other
RMjs. If the RMj has already committed or aborted, it may safely commit or abort as well.
But what happens if all participating RMs vote to commit but none has yet received a
‚ü®Commit‚ü© or ‚ü®Abort‚ü© request (see figure 6.12)? You might think, ‚ÄúNo problem. In that
case, we can simply time out and abort.‚Äù But that approach may lead to a safety vio-
lation. Because clocks are not perfectly synchronized, if the TC recovers and sends a
‚ü®Commit‚ü©, some RMs may believe that they received the ‚ü®Commit‚ü© within their timeout
window; others may believe that they received the ‚ü®Commit‚ü© outside their time window
and have already aborted. This example is a helpful reminder of how subtle and intri-
cate distributed protocols can be.
88 chapter 6 Distributed transactions
Client Transaction
coordinator
Resource
manager1
Resource
manager2
begin t1
begin t1
Usual transaction execution
commit t 1
prepare t 1
vote to commit
prepare t1
vote to commit
commit
ack
commit
ack
ack
Prepare
phase
Commit
phase
Figure 6.12 Failure of the TC before the first commit
Summary
¬° Distributed transactions extend nondistributed transactions to span multiple
RMs.
¬° A distributed transaction (global transaction) consists of two or more nondistrib-
uted transactions (local transactions).
¬° Atomic commit protocols ensure that distributed transactions achieve a unani-
mous commit or abort decision, upholding atomicity across RMs.
¬° Blocking commit protocols guarantee safety but not liveness in the presence of
failure.
89Summary
¬° Nonblocking commit protocols guarantee safety and liveness in the presence of
failure.
¬° The Two-Phase Commit (2PC) protocol is the best-known and most-studied
atomic commit protocol.
¬° 2PC divides participants into a TC and RM and operates in two phases: Prepare
and Commit.
¬° 2PC guarantees safety and liveness in the case of an RM failure.
¬° 2PC guarantees safety in the case of a TC failure.
90
7Partitioning
This chapter covers
¬° The concept of partitioning
¬° Static and dynamic partitioning
¬° Vertical and horizontal partitioning
¬° Item-based lookup versus directory-based lookup
¬° Common strategies
Two techniques are essential for guaranteeing the scalability and reliability of a dis-
tributed system: partitioning and replication. Partitioning seeks to overcome the
scalability limitations of a single component; replication seeks to overcome the reli-
ability limitations of a single component. First up: partitioning. Chapter 8 covers
replication.
7.1 Encyclopedias and volumes
When I was a teenager, one of my most cherished possessions was a small encyclo-
pedia. An encyclopedia is a compilation of entries that are sorted alphabetically. My
encyclopedia was too extensive to fit into a single book; instead, it was divided into
volumes (see figure 7.1).
91Encyclopedias and volumes
A B C D E F G H I J K L X Y Z
‚Ä¢ ‚Ä¢ ‚Ä¢
Figure 7.1 Encyclopedias and volumes
The encyclopedia represents a logical object, and the volumes represent physical
objects. Logically, the encyclopedia contains all entries, but physically, the volumes con-
tain disjoint subsets of entries.
The authors could have placed any item in a randomly selected volume, but doing
so would have made the encyclopedia useless. If I needed to find an item, I would be
forced to scan the entire encyclopedia to locate the item or come to the realization that
the item doesn‚Äôt exist.
Therefore, the authors, the producer of the encyclopedia, and I (the consumer)
agreed on a straightforward system. Each volume is assigned a letter of the alphabet and
contains entries starting with that letter, sorted alphabetically. If the producer wants to
add (place) an entry, they place it in the corresponding volume. Likewise, if I want to
find (fetch) an entry, I search for and fetch it from the corresponding volume.
Although the practice of dividing an encyclopedia into volumes alphabetically is a
tried-and-true approach, this example highlights various challenges that may arise:
¬° Uneven distribution‚ÄîNot all letters in the English language have an equal num-
ber of corresponding entries. The letter E may have a bulky volume of entries,
whereas the letter X may have a slim volume.
¬° Uneven demand‚ÄîFor this reason, we may end up reaching for the volume with
the letter E more often than the one with the letter X.
¬° Cross-references‚ÄîSome entries refer to other entries to provide a comprehensive
picture. A detailed entry about partitioning, found under P, may reference repli-
cation, found under R, necessitating access to two volumes.
Just as an encyclopedia may be too extensive to fit in a single book, a software system
may be too extensive to fit in a single component, called a node in this context. In such
cases, the system can be divided into disjoint subsets or partitions.
Partitioning a software system can present challenges similar to those of partitioning
an encyclopedia, such as uneven distribution, uneven demand, and cross-referencing
between partitions. Optimizing your partition strategy depends on your requirements.
92 chapter 7 Partitioning
7.2 Thinking in partitions
The term partitioning refers to representing a single logical object by multiple, disjoint
physical objects (see figure 7.2). Partitioning is used to improve the scalability of a dis-
tributed system. By dividing a logical object into multiple physical objects, called parti-
tions, and distributing them across multiple components, the nodes, we can distribute
demand across multiple nodes and avoid hitting the scalability limits of a single node.
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Next chapter
Figure 7.2 Thinking about partitioning
Aha! moment: Distributed systems and partitions
A distributed system, as defined in chapter 1, is a set of concurrent, communicating
components. Each component has exclusive access to its own local state, which no
other components can access.
Although I usually don‚Äôt conceptualize distributed systems this way, a distributed sys-
tem is inherently and by definition partitioned: each component and its local state
can be understood as a partition. Most often, we think of partitioning as a dataset
that is too large to store on one node. Therefore, we split that dataset into multiple
smaller datasets and distribute them across multiple nodes.
Partitioning, however, is not only about growing beyond data volume; it is also
about growing beyond any limitation imposed by a single resource by using multiple
resources. A common example is data volume‚Äîan intrinsic property. Another com-
mon example is request volume‚Äîan extrinsic property.
Aha! moment: Big data is relative
A pragmatic definition of big data is ‚Äúanything that doesn‚Äôt fit on one node.‚Äù In other
words, big data refers to data storage and processing that requires partitioning. If, as
93The mechanics of partitioning and balancing
shown in the figure, a node is a single key-value register, a dataset with two or more
key-value pairs is considered big data.
Raspberry Pi
MacBook Pro
Anything more is
big data on Raspberry Pi.
Anything more is
big data on MacBook Pro.
The relativity of big data
7.3 The mechanics of partitioning and balancing
For the remainder of this chapter, we
will think in terms of a key-value store.
Our dataset consists of a collection of
key-value items. The dataset is split into
partitions, and each partition is hosted
on a node. In figure 7.3, the dataset is
{ x=1, y=2, z=3 }, there are three par-
titions, each containing one key-value
pair, and each partition is replicated
across three nodes.
When we‚Äôre thinking about parti-
tioning, two related yet different rela-
tionships are of interest: the assignment
of a data item to a partition and the
assignment of a partition to a node. To
process more data than a single node
can handle, the system splits the data-
set into partitions and distributes them
across multiple nodes. Figure 7.4 shows
the assignment of data items to parti-
tions and the assignment of partitions to
nodes.
Partitioning
C 1.2
x = 1
C 2.2
y = 2
C 3.2
z = 3
Partitioning
C 1.1
x = 1
C 2.1
y = 2
C 3.1
z = 3
Partitioning
C 1.3
x = 1
C 2.3
y = 2
C 3.3
z = 3
Replication Replication Replication
Figure 7.3 Partitioning a key-value store
94 chapter 7 Partitioning
Data item Partition Node
x = 1 P 1 N 1
y = 2 P2 N 2
z = 3 P 3 N 3
Dataset
Item to partition to node
Item to partition Partition to node
Figure 7.4 Assignment of data items to partitions and the assignment of partitions to nodes
Partitioning may seem straightforward in theory but is quite complex in practice. Par-
titioning requires making carefully calculated tradeoffs and balancing between com-
peting requirements. The design of your partitioning strategy depends on the unique
characteristics of your system.
Consider a social media application like X. In this case, some users post tweets, while
other users read tweets or post replies. One option is to partition your data by user.
Although this strategy may result in well-balanced reads and writes for the average user,
it may become skewed for celebrities or influencers.
Another example to consider is an Internet of Things (IoT) application with sensors
that post measurements periodically. In this case, you could partition your data by date,
which ensures perfect data distribution for a fixed set of sensors. Every day contains the
same amount of data, but writes become skewed to only one partition.
You have other options for partitioning your data, not only by date but also by sensor
type, measurement type, or region. Each option has virtues and limitations. Ultimately,
designing an adequate partitioning strategy depends on the unique characteristics and
requirements of your system.
Partitioning is rarely a one-and-done process. You begin with an initial set of antici-
pated requirements and conditions, and as these requirements and conditions change,
your partitioning strategy has to adapt. This may require a nontrivial and nonobvious tran-
sition as the system moves data between partitions while continuing to service requests.
7.4 (Re)partitioning
Partitioning refers to the assignment of data items to partitions (see the left side of fig-
ure 7.5). Repartitioning refers to the reassignment of previously assigned data items to
different partitions, such as when the number of partitions changes.
7.4.1 Types of partitioning
Partitioning can be categorized along different dimensions. Here, we will look at static
versus dynamic partitioning and horizontal versus vertical partitioning.
95(Re)partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.5 Partitioning‚Äîassignment of data items to partitions
static and dynamic partitioning
Static partitioning is a strategy in which the number of partitions is fixed, whereas dynamic
partitioning is a strategy in which the number of partitions is variable. Note that static and
dynamic partitioning refer only to the number of partitions, not the number of nodes:
¬° Static partitioning‚ÄîThe number of partitions is fixed and cannot be changed
online. Any changes to the number of partitions must be done offline because
the change is considered an administrative operation of the system.
¬° Dynamic partitioning‚ÄîThe number of partitions is variable and can be changed
online. Changing the numbers of partitions is a normal operation of the system.
In other words, the system is elastic.
Static partitioning cannot adapt to changing demand automatically; the system is not
elastic. Dynamic partitioning can adapt to changing demand automatically; the system
is elastic (see table 7.1). Dynamic partitioning adds a lot of complexity, however (see
section 7.6).
Table 7.1 Elasticity and complexity
Strategy Static partitioning Dynamic partitioning
Elasticity (capability to adapt to demand) ‚ùå ‚úîÔ∏è
Complexity Low High
Aha! moment: No change vs. slow change
The term static refers not only to no change but also to slow or infrequent change. If
changing the number of partitions is not part of normal operations but is part of the
administration of a system, the number of partitions can be considered static. Admin-
istrative actions require planning, require the preparation of migrations and rollbacks,
and can cause downtime.
horizontal and vertical partitioning
When we‚Äôre thinking about horizontal and vertical partitioning, the most intuitive and
tangible mental model to follow is that of relational database tables. A relational data-
base table consists of rows and columns (see figure 7.6).
96 chapter 7 Partitioning
a b
1
2
3
4
a b
1
2
3
4
Horizontal partitioning Vertical partitioning
Figure 7.6 Horizontal and vertical partitioning
Horizontal partitioning, also known as sharding, refers to partitioning data based on rows.
On the left side of figure 7.6, one partition contains rows 1 and 2, and the other parti-
tion contains rows 3 and 4. Vertical partitioning refers to partitioning data based on col-
umns rather than rows. On the right side of figure 7.6, one partition contains column
a, and one partition contains column b.
Horizontal and vertical partitioning can be used in combination. Consider an appli-
cation that manages user information. This information includes text-based data such
as a user‚Äôs first and last names, and binary large objects (BLOBs) such as a profile pic-
ture (see figure 7.7).
First Last
2 Jane Smith ...
First Last
1 John Smith ...
Horizontal partition 1.1
Horizontal partition 1.n
Vertical partition 1
by profile data
Vertical partition 1
by profile picture
Horizontal partition 2.1
Horizontal partition 2.m
1
john.jpg
2
jane.jpg
Figure 7.7
Partitioning user
information
First, we can apply vertical partitioning. In this case, we can separate data by type‚Äî
that is, separate the text-based profile data from the profile pictures. This is practical
97(Re)partitioning
because these data types are fundamentally different. The text-based data, such as
names, can be efficiently stored and searched in a relational database; the image files
(profile pictures) are better suited to a file storage system designed to manage large
data files.
Next, we can apply horizontal partitioning. In this case, we can distribute text-based
profile data across multiple relational databases, one per horizontal partition. In addi-
tion, we can distribute profile pictures across multiple file storages‚Äîagain, one per
horizontal partition.
In other words, we divide user information into two parts. First, we split data by type,
text versus images, using vertical partitioning. Then we distribute text and images using
horizontal partitioning. This partitioning strategy helps us scale different aspects of our
application independently and meet demand efficiently.
7.4.2 Data item to partition assignment strategies
When data is partitioned, first we need to determine the partition to which an item
belongs. Item-based assignment and directory-based assignment are two widely used
approaches (see figure 7.8).
Item
Partition
Partition
Partition
Item
Partition
Partition
Partition
Item-based
assignment
Directory-based
assignment
Assignment
Figure 7.8 Item-based and directory-based assignment
The fitness of an assignment strategy is often measured in variance and relocation.
Here, variance refers to the degree to which items are uniformly distributed across par-
titions. Relocation corresponds to the number of items that require relocation when the
number of partitions changes.
item-based assignment strategy
Item-based assignment is a partitioning strategy that assigns each data item to a partition
based solely on its own characteristics‚Äîin case of key-value pairs, for example, the key
or a hash of the key. Item-based assignment is stateless and therefore simple to develop
and operate, as shown in the following listing.
98 chapter 7 Partitioning
Listing 7.1 Item-based assignment strategy
partitions = {i: {} for i in range(5)}
def placement(key):
# Simple placement function that works with integers.
return key % 5
def place(key, value):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Calculate the partition ID based on our hash function.
partition = placement(key)
# Retrieve the item from the appropriate partition.
return partitions[partition].get(key)
directory-based assignment strategy
By contrast, directory-based assignment is a function of the item and a separate compo-
nent called a directory or lookup table. In other words, directory-based assignment is
stateful and therefore more complex to develop and operate.
This strategy uses a directory or mapping system that provides the partition assign-
ment based on both the item and the directory state. Unlike item-based strategy, direc-
tory-based strategy knows the system‚Äôs state, as shown in the following listing.
Listing 7.2 Directory-based assignment strategy
partitions = {i: {} for i in range(5)}
assignment = {}
def place(key, value):
# Instead of calculating the partition,
randomly choose and update the directory.
partition = random.randint(0, len(partitions) 1)
# Add the key to the directory
assignment[key] = partition
# Add the item to the appropriate partition.
partitions[partition][key] = value
def get(key):
# Retrieve the partition ID from the directory.
partition = assignment.get(key)
# Retrieve the item from the appropriate partition.
if partition is not None:
return partitions[partition].get(key)
else:
return None
99Common item-based assignment strategies
Because the directory is a stateful component, however, the directory itself may be
a bottleneck‚Äîthe limiting factor in the system‚Äôs scalability and reliability. In other
words, the directory itself may conflict with the goal of partitioning!
In summary, due to their simplicity, item-based assignment strategies are beneficial
for systems in which coarse-grained control is sufficient. Despite their complexity, direc-
tory-based assignment strategies are beneficial for systems that require fine-grained
control.
Aha! moment: Coarse-grained vs. fine-grained
Item-based assignment strategies work on a coarse-grained level. Item-based assign-
ment strategies are stateless, based on a range or hash function. Although this
assignment strategy can balance items across partitions, it can‚Äôt place items in spe-
cific partitions. There is a risk of collisions when two or more high-demand (hot) items
are assigned to the same partition. This problem has no simple workaround because
the assignment of items is determined by the assignment function, not by manual
intervention.
Directory-based assignment strategies work on a fine-grained level. Directory-based
assignment strategies are stateful, based on a directory or lookup table. This assign-
ment strategy can place items in a specific partition.
7.5 Common item-based assignment strategies
We need an assignment function that enables us to find an item and balance items.
Let‚Äôs run some experiments. UNIX and UNIX-like operating systems provide the file
words, typically located at /usr/share/dict/words, which contains a list of English
words. In this experiment, we will compare the fitness of different assignment func-
tions by assigning each word in the words file to one of five partitions.
We will use the following code to count the items assigned to each partition. At this
writing, the words file on my machine contains 235,976 words. Ideally, each partition
should contain just shy of 47,200 items.
Listing 7.3 Counting the items assigned to each partition
partitions = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
def placement(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
partitions[placement(word)] += 1
100 chapter 7 Partitioning
7.5.1 Range partitioning
One possible assignment strategy is based on key range, with each partition responsi-
ble for hosting items with keys in a specific range of keys. The following code demon-
strates a sample assignment function that assigns keys to partitions based on the first
letter of the key.
Listing 7.4 Range partitioning
def placement(key):
if key[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if key[0] in ['f', 'g', 'h', 'i', 'j']:
return 1
if key[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if key[0] in ['p', 'q', 'r', 's', 't']:
return 3
if key[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
On my machine, running the script in listing 7.3 with the placement function in listing
7.4 yields the following distribution:
0: 67730
1: 33203
2: 35829
3: 73432
4: 25782
This distribution indicates that the partitions are not well-balanced. Partition 3 and par-
tition 0 are obvious hotspots revealing an uneven distribution of items across partitions.
7.5.2 Hash partitioning
Another possible assignment strategy is based on hash value. The following listing
demonstrates a sample assignment function that calculates a hash value for each key
and assigns the key to one of five partitions based on the result.
Listing 7.5 Hash partitioning
def placement(key):
return hash(key) % 5
On my machine, running the script in listing 7.3 with the placement function in listing
7.5 yields the following distribution:
0: 47219
1: 47187
2: 47362
101Repartitioning
3: 47175
4: 47033
This distribution indicates that the partitions are well balanced. There are no hotspots
in terms of uneven distribution of items across partitions.
7.6 Repartitioning
Over time, things change, and the number of partitions may need to decrease or
increase. As we will see, this change is particularly challenging for item-based assign-
ment strategies because the number of partitions has an outsize effect on placement.
We will use the following code to count the items remaining in their partition and
the items that must be relocated to a different partition when the number of partitions
changes from 5 to 6.
Listing 7.6 Counting the items remaining and relocating
same = 0
diff = 0
def placement_5(word):
...
def placement_6(word):
...
with open('/usr/share/dict/words') as f:
words = [word.lower() for word in f.read().split()]
for word in words:
if placement_5(word) == placement_6(word):
same += 1
else:
diff += 1
7.6.1 Range partitioning
On my machine, running the script in listing 7.6 with the placement functions in the
following listing yields an almost-even split. Specifically, 48.6% of items remain in their
current partition, and 51.4% of items must relocate to a different partition.
Listing 7.7 Range partitioning
def placement_5(word):
if word[0] in ['a', 'b', 'c', 'd', 'e']:
return 0
if word[0] in ['f', 'g', 'h', 'i', 'j']:
102 chapter 7 Partitioning
return 1
if word[0] in ['k', 'l', 'm', 'n', 'o']:
return 2
if word[0] in ['p', 'q', 'r', 's', 't']:
return 3
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 4
def placement_6(word):
if word[0] in ['a', 'b', 'c', 'd']:
return 0
if word[0] in ['e', 'f', 'g', 'h']:
return 1
if word[0] in ['i', 'j', 'k', 'l']:
return 2
if word[0] in ['m', 'n', 'o', 'p']:
return 3
if word[0] in ['q', 'r', 's', 't']:
return 4
if word[0] in ['u', 'v', 'w', 'x', 'y', 'z']:
return 5
Same: 114790
Diff: 121186
7.6.2 Hash partitioning
On my machine, running the script in listing 7.6 with the placement function in the
following listing yields a significantly skewed split. Specifically, 16.7% of items remain
in their current partition, and 83.3% of items must relocate to a different partition.
Listing 7.8 Hash partitioning
def placement_5(word):
return hash(word) % 5
def placement_6(word):
return hash(word) % 6
Same: 39443
Diff: 196533
In conclusion, hash partitioning performs well in terms of minimizing variance, but
both range partitioning and hash partitioning perform poorly in terms of minimizing
relocation (see table 7.2).
Table 7.2 Minimizing variance and relocation
Strategy Key range Hashing
Minimal variance ‚ùå ‚úîÔ∏è
Minimal relocation ‚ùå ‚ùå
103(Re)balancing and overpartitioning
7.7 Consistent hashing
Consistent hashing is a technique for achieving both minimal variance and minimal relo-
cation when partitioning data. A consistent hashing algorithm is designed so that given
n items and m partitions, only n/m items need to be relocated to a different partition
on average.
On my machine, after I installed the uhashring Python package, running the scripts
in listing 7.3 and listing 7.6 with the placement functions in the following listing yielded
great results. Specifically, 81.5% of items remained in their current partition, and
18.5% of items relocated to a different partition.
Listing 7.9 Consistent hashing
from uhashring import HashRing
def placement_5(word):
return HashRing(nodes=[0, 1, 2, 3, 4]).get_node(word)
def placement_6(word):
return HashRing(nodes=[0, 1, 2, 3, 4, 5]).get_node(word)
0: 49084
1: 47753
2: 48172
3: 47144
4: 43823
Same: 192243
Diff: 43733
Aha! moment: Thinking about algorithms
I enjoy thinking in systems but not in algorithms. But many reading materials, such as
blog posts, detail algorithms from the bottom up, leaving me to derive my own mental
model and understanding. I have to reverse-engineer the big picture.
I enjoy reading materials that detail the specification of an algorithm from the top
down rather than the details of the implementation. In other words, when thinking
about algorithms, I prefer to think in signatures or interfaces.
In that spirit, when thinking on a system level, I prefer to think about the effect of con-
sistent hashing, not about how consistent hashing is implemented.
7.8 (Re)balancing and overpartitioning
Balancing refers to the assignment of partitions to nodes. Rebalancing refers to the
reassignment of previously assigned partitions to different nodes‚Äîas a response to a
change in demand, for example (see the right side of figure 7.9).
The uhashring package implements
consistent hashing in pure Python.
The package is available at https://
pypi.org/project/uhashring.
104 chapter 7 Partitioning
PartitionData item Node
Partitioning Balancing
Figure 7.9 Balancing‚Äîassignment of partitions to nodes
A twist on partitioning is overpartitioning, which creates excess partitions for the num-
ber of nodes. This subsequently allocates multiple partitions to each node.
Consider a system that has the same number of partitions and nodes. In this setup,
each node gets exactly one partition, as shown on the left side of figure 7.10. Here,
the system has no flexibility to accommodate fluctuating demand. Because the nodes
and partitions are locked in a one-to-one relationship, we cannot make adjustments to
accommodate increasing or decreasing demand.
Now consider starting with more partitions than nodes. In this setup, each node gets
multiple partitions, as shown on the right side of figure 7.10. Here, the system has some
flexibility. If demand drops, we can decommission some nodes and reassign orphaned
partitions to remaining nodes. If demand rises, we can commission more nodes and
spread partitions among nodes.
Dataset
Machines
Dataset
Machines
Partitioning Overpartitioning
Figure 7.10 Partitioning (left) and overpartitioning (right)
There‚Äôs a limit, however: the number of partitions determines the maximum num-
ber of nodes. When all partitions are distributed, no more nodes can be added. So
although this approach is more flexible than the first, it is still constrained by the num-
ber of partitions.
105Summary
Summary
¬° Partitioning improves the scalability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Partitioning assigns items to partitions, whereas balancing assigns partitions to
nodes.
¬° Static partitioning uses a fixed number of partitions, offering simplicity but lack-
ing elasticity, whereas dynamic partitioning adapts to changing demands with a
variable number of partitions, adding complexity.
¬° Horizontal partitioning (or sharding) divides data by rows, and vertical partition-
ing divides data by columns. You can combine these strategies to manage differ-
ent data types and scale aspects of the application independently.
¬° Item-based assignment is a partitioning strategy that assigns each data item to a
partition based solely on its own characteristics. Directory-based assignment is a
partitioning strategy that uses a separate component called a directory or lookup
table.
¬° Consistent hashing minimizes uneven distribution and data relocation during
partition changes.
¬° Designing an adequate partitioning strategy requires consideration of the sys-
tem‚Äôs unique characteristics and requirements. The strategy may change as the
system evolves.
106
8Replication
This chapter covers
¬° Redundancy
¬° Replication
Some of the most complex aspects of distributed systems involve redundancy‚Äî
especially redundancy in the presence of failure. The four fundamental principles of
database transactions (which you should recall from chapter 5) are famously referred
to as ACID guarantees. ACID stands for atomicity, consistency, isolation, and durability.
Let‚Äôs recall the definition of durability. Durability guarantees that once a transac-
tion is committed, its effects are permanent, preventing the transaction from going
back on its word, so to speak.
The requirement that a transaction be unable to go back on its word, or back-
track, is essential for many business processes. When we receive a positive acknowl-
edgment of a promise, we rely on the durability of the promise to move forward.
In an e-commerce application, for example, we rely on the durability of a payment
processor‚Äôs promise to collect payment to ship goods or render services. If the pay-
ment processor backtracks, we will be unpleasantly surprised when we realize that we
shipped the goods but never received the payment.
107Redundancy
In a theoretical system model, components are assumed to be completely reliable and
not subject to any kind of failure, such as Crash-Stop or Crash-Recovery failures, so in that
model, backtracking is not possible. All promises made by the system are guaranteed to be
fulfilled because components cannot fail. (Recall that we are not considering Byzantine
failures, so the system must not lie.) But in a practical system model in which components
are subject to failures, backtracking is possible. If a component responsible for processing
transactions fails, the system may not be able to fulfill promises made to users. To prevent
this situation‚Äîto prevent a single point of failure‚Äîwe add redundancy to the system.
8.1 Redundancy
Redundancy refers to the duplication and coordination of subsystems so that an increase
in the duplication factor results in increased reliability and/or scalability. There are
two types of redundancy:
¬° Static redundancy‚ÄîThe set of components and the set of interactions between the
components do not change during the lifetime of a system. This type of redun-
dancy is present predominantly in hardware systems.
¬° Dynamic redundancy‚ÄîThe set of components and the set of interactions between
the components change during the lifetime of a system. This type of redundancy
is present predominantly in software systems.
Aha! moment: Redundancy and scalability
Redundancy improves the reliability of a distributed system. Nevertheless, redundancy
aids the scalability of a distributed system: it distributes data across multiple nodes.
Subsequently, load partitioning (such as round-robin load balancing) distributes the
load across replicas (see the figure). As we will see in subsequent chapters, redun-
dancy sometimes has the opposite effect, decreasing the scalability of a system. In
conclusion, the relationship between redundancy and scalability is not straightforward.
Client
Node
Data
Client
Node
Data
Client
Node
Data
Replication distributes the data.
Partitioning distributes the load.
Therefore, partitioning provides the scalability.
Replication
distributing data,
partitioning
distributing load
108 chapter 8 Replication
Aha! moment: Duplication and coordination
In my experience, our mental models often involve duplicating components but rarely
involve coordinating those components. But a collection of components is not a sys-
tem; it must be composed into a coherent system, meaning that the components
must be coordinated. This coordination can be as simple as a round-robin load-
balancing strategy or as complex as a consensus protocol, but it must exist. There-
fore, I emphasize the duality of duplication and coordination in my mental models.
Figure 8.1 illustrates the ‚ÄúHello World‚Äù
of redundant (hardware) systems: a
redundant logic gate.
A logic gate takes n inputs and pro-
duces 1 output, computing a Boolean
function f. For this exploration, the
actual function is irrelevant:
y = f(x1, x2, ... xn)
We assume that if the logic gate fails,
the system fails. In other words, the
logic gate is a single point of failure.
So how can we make the system more
reliable? One solution is to add redun-
dancy to the system in the form of
duplication and coordination.
We can replicate the logic gate and
use three logic gates instead of one.
To coordinate the replicas, we need
a majority gate. This way, if one logic
gate fails, the other logic gates can
compensate for the failure. To prevent the majority gate from becoming the single
point of failure, we also replicate the majority gate, which combines the outputs of the
replicas and selects the majority vote of the replicas:
R = (a ‚àß b) ‚à® (b ‚àß c) ‚à® (a ‚àß c)
This formula computes the logical majority of three inputs: a, b, and c. The formula
evaluates to true if at least two of the three inputs are true.
To summarize, we replaced a single logic gate with a group of logic gates and major-
ity gates. We ensured that the group of components behaves like the original single
component but can tolerate the failure of one component‚Äîthe magic of redundancy.
A common implementation of duplication is replication. Replication means having
multiple instances of the same thing. The remainder of this chapter focuses on replica-
tion; later chapters revisit general duplication.
Majority
gate
Majority
gate
Majority
gate
Logic
gate
Logic
gate
Logic
gate
Replication Coordination
Logic
gate
a
b
c
Adding redundancy
Figure 8.1 Redundancy as duplication and
coordination
109Thinking about replication and consistency
8.2 Thinking about replication and consistency
Before we explore the formal mental models of replication and consistency in software
systems, let‚Äôs consider the nuances behind what we think of as one thing. William Kent
offers a thought-provoking perspective in the first edition of his book Data and Reality:
A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World (Tech-
nics Publications, 2000):
What is ‚Äúone thing‚Äù? That appears at first to be a trivial, irrelevant, irreverent,
absurd question. It‚Äôs not. The question illustrates how deeply ambiguity and misun-
derstanding are ingrained in the way we think and talk.
Consider the example of a book, an author, and a library. An author wrote a book titled
Structure and Interpretation of Computer Programs (SICP for short). A library is filled with
a vast collection of books, including ten copies of SICP. What do we mean by the term
book? Does it refer to the work of the author (the logical item) or to the copies in the
library (the physical item)?
Now consider editions: The author wrote the first edition of SICP in 1984, the second
edition in 1996, and the JavaScript edition in 2022, as shown in table 8.1.
Table 8.1 Book editions
Name Edition Year
SICP First 1984
SICP Second 1996
SICP JavaScript 2022
Are the first, second, and JavaScript editions the same book or different books? The
examples in the first and second editions are written in the Scheme programming lan-
guage, whereas the examples in the JavaScript edition are written in the JavaScript
programming language. So are the first and second editions sufficiently similar to be
considered the same book, and the JavaScript edition is deemed different? Figure 8.2
illustrates the library‚Äôs inventory. How many copies of SICP are available?
1st SICP
1st SICP
1st SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
2nd SICP
js SICP
js SICP
Figure 8.2 Library
inventory of SICP
110 chapter 8 Replication
This question leads us to the core of replication and consistency. Replication and con-
sistency is about not only the number of physical items that exist but whether these
physical entities represent the logical item. The answer to this question depends on
your perspective:
¬° Some readers may argue that any edition covers the relevant concepts; therefore,
they claim that 10 books are in stock.
¬° Other readers may argue that the relevant concepts can be faithfully captured
only in the classic Scheme programming language; therefore, they claim that
eight books are in stock.
¬° Some readers may argue that the relevant concepts can be tangibly captured only
in the modern JavaScript programming language; therefore, they claim that two
books are in stock.
The exploration of ‚Äúone thing‚Äù forces us to scrutinize the underlying principles of
identity and equivalence: oneness and sameness. Understanding these principles is
foundational to replication and consistency, whether in the context of books or distrib-
uted software systems. The complexity and ambiguity of these principles are not just
challenges to overcome but also intrinsic to the reality of distributed software systems.
8.3 Replication
The term replication refers to representing a single logical object by multiple, identical
physical objects. Replication is a technique used to improve the reliability of a distrib-
uted system. By replicating a logical object into multiple physical objects, called repli-
cas, we can avoid reaching the reliability limits of a single object.
Central to the discussion of replication is replication transparency, which refers to the
system‚Äôs ability to hide the existence of multiple objects, providing the illusion of a sin-
gle object. In essence, a discussion of replication transparency is a discussion of how the
system balances concealing and revealing the details of replication. This concept chal-
lenges software engineers to balance various factors in the system, such as consistency,
availability, and latency (see figure 8.3).
Logical
object
Physical
object 1
Physical
object i
Physical
object n
‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢
Partitioning
Physical objects represent a part.
Replication
Physical objects represent the whole.
Previous chapter
Figure 8.3 Replication represents a single logical object by multiple, identical physical objects.
111The mechanics of replication
8.4 The mechanics of replication
Replication is not only applicable to
stateful components, which manage
state in their local storage, but also rel-
evant to stateless components. Logic
gates deployed redundantly, for exam-
ple, are stateless components. Their
outputs depend solely on their inputs.
In the context of software engineer-
ing, we often redundantly deploy state-
less services and load-balance between
them. This form of replication is so
straightforward that we rarely recog-
nize it as replication.
In this chapter, we focus on stateful
components because they harbor most
of the complexity related to this topic.
Consider a key-value store, illustrated in
figure 8.4, composed of a collection of
key-value items. Unlike in chapter 7, we
ignore partitioning and assume that the
entire dataset can fit on a single node.
In a distributed system, complexity in replication arises from change. When an object
remains static, replicating the object is straightforward. But when the object changes,
we must ensure that these changes propagate properly.
Aha! moment: Not all change is equal
Not all changes in a system have the same consequences for our knowledge of the
state of the system. Some changes potentially invalidate our current knowledge; oth-
ers preserve that knowledge. Consider a counter. We care only if its value exceeds 10:
¬° If the counter supports both increment and reset operations, any modifica-
tion could invalidate our current knowledge. Even if we know that the value
crossed 10, a subsequent reset operation takes us back to 0.
¬° If the counter supports only an increment operation, when its value crosses
10, further increments will not invalidate our knowledge that the counter is
above the threshold.
This property, in which a new change does not contradict current knowledge, is known
as monotonicity.
8.4.1 System model
A distributed system (as defined in chapter 1) consists of concurrent components that
communicate by sending and receiving messages over a network. It‚Äôs important to note
Partitioning
C1.2
x = 1
C2.2
y = 2
C3.2
z = 3
Partitioning
C1.1
x = 1
C2.1
y = 2
C3.1
z = 3
Partitioning
C1.3
x = 1
C2.3
y = 2
C3.3
z = 3
Replication Replication Replication
Figure 8.4 A replicated key-value store
112 chapter 8 Replication
that within this framework, exactly one component or the network takes a single step
at a time. We modeled the network as a central component to which every component
was connected. In this chapter, we will model the network as point-to-point communi-
cation links between components. This model enables concisely thinking about‚Äîand
visualizing of‚Äînetwork partitions.
A distributed system (as defined in chapter 2) is partially synchronous, consisting of
components subject to Crash-Stop failure, Omission failure, and Crash-Recovery failure
communicating over an unreliable network. In the context of replication, we attribute
message loss to a network partition‚Äîto a temporary, intermediate, or permanent fail-
ure of one or more communication links. The example in figure 8.5 shows one client,
three replicas, and a network partition between replica 1 and replica 2.
Node 1
Client
Node 2
Node 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
x = 1
y = 2
z = 3
Figure 8.5 The network as
point-to-point communication
links between components
8.4.2 Replication lag
Replication lag refers to the inherent delay in propagating changes across replicas,
caused by the impossibility of instantaneous updates in a distributed system. Because
only one component or the network can take a single step at a time, updates cannot be
applied simultaneously to all replicas. Instead, they must be applied sequentially, one
replica after another (see figure 8.6).
x1 = 1
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 1
x 3 = 1
x 1 = 2
x 2 = 2
x 3 = 1
x1 = 2
x 2 = 2
x 3 = 2
Figure 8.6 Replication lag.
Instantaneous propagation
of changes is impossible,
resulting in an inherent lag.
Replication lag is not merely a temporal delay but also an inherent aspect of replicated
distributed systems. This lag is the source of many challenges, such as potential loss of
replication transparency.
113The mechanics of replication
We can differentiate between inherent replication lag, which is a result of the nature
of distributed systems, and imposed replication lag, which is a result of network par-
titions and component failures. This dichotomy captures the distinction between
unavoidable replication lag and additional replication lag within a distributed system.
8.4.3 Synchronous vs. asynchronous replication
In synchronous replication, an operation is not considered complete until the changes
have been replicated and acknowledgments have been received from all other nodes.
In other words, the completion of an operation is tightly coupled with the replication
process. Replication takes place in the foreground (see figure 8.7).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.7 Synchronous replication
In asynchronous replication, the operation is considered complete as soon as it‚Äôs pro-
cessed on the initial node; there‚Äôs no wait for the changes to be replicated and acknowl-
edgments to be received from the other nodes. In other words, the completion of an
operation is loosely coupled with the replication process. Replication takes place in the
background (see figure 8.8).
Client
Node 1
Node 2
Node 3
Client
Node 1
Node 2
Node 3
Can execute
sequentially
or in parallel
Figure 8.8 Asynchronous replication
114 chapter 8 Replication
Synchronous replication ensures immediate consistency across all nodes, but it may
affect latency and availability due to its dependence on every node‚Äôs timely response. In
practice, many systems employ a hybrid strategy for replication: When a change is rep-
licated, a majority of replicas must acknowledge the operation. This majority is often
called the quorum. Essentially, replication in the quorum happens in the foreground.
When the quorum is achieved, however, changes to replicas outside the quorum can
be asynchronous. Replication outside the quorum may happen in the background.
8.4.4 State-based vs. log-based replication
Assuming that a system is a deterministic state machine, we have two primary options
for replicating change. We can replicate the current state of the system, or we can rep-
licate the sequence of operations that leads to the state:
step : State ‚ûû Operation ‚ûû State
For this discussion, the state machine must be deterministic. This means that multiple
replicas of the same state machine, beginning in the same start state and receiving the
same operations in the same order, will arrive at the same state.
Consider a counter with a single increment by operation that starts at state counter
= 0. For this example, we assume the client issues a increment by(2) followed by a
increment by(3) operation.
In state-based replication, changes are replicated by sending (a diff of) the current
state across the wire regardless of the series of operations that led to the current state.
In this example, change is propagated as a sequence of counter = 2 and counter = 5
messages.
In log-based replication, changes are replicated by sending the sequence of
operation across the wire. In this example, change is propagated as a sequence of
increment by(2) and increment by(3) messages.
In summary, state-based replication focuses on propagating the current state,
whereas log-based replication focuses on propagating the sequence of operations lead-
ing to the current state. In practice, log-based replication is emerging as the industry
favorite.
8.4.5 Single-leader, multileader, and leaderless systems
Replication strategies can be distinguished by the number of leader nodes (see figure
8.9). In systems with leadership, one or more leader nodes are authorized to accept
and handle requests, whereas follower nodes are restricted and may not process
requests directly.
In single-leader systems, one dedicated leader node coordinates operations. Only the
leader node accepts operations and propagates changes to its followers. This creates a
chain of command but can become a single point of failure, almost negating the reason
for replication.
In multileader systems, more than one dedicated node can act as a leader that coor-
dinates operations. Any leader node can accept an operation. This prevents a single
115Summary
Single-leader Multileader Leaderless
Figure 8.9 Single-leader, multileader, and leaderless configurations
point of failure, but without a chain of command, concurrent operations may conflict,
requiring conflict resolution.
In leaderless systems, no designated leader node coordinates operations. All nodes
can accept any operation. Conflicts that arise from concurrent operations are again
resolved by conflict resolution.
A wide array of conflict resolution strategies exists, ranging from timestamp-based
methods such as ‚Äúlast write wins‚Äù to techniques that use conflict-free replicated data
types (CRDTs). While these offer varied solutions for managing conflicts, their practical
application is not straightforward. The seemingly simple ‚Äúlast write wins‚Äù method, for
example, can result in unexpected overwrites from a client‚Äôs perspective, which high-
lights the need for careful consideration and understanding of the chosen conflict res-
olution strategy.
Aha! moment: Leaders are for writes and followers are for reads?
Sometimes, authors oversimplify leader-based replication and state that leader
nodes process writes and follower nodes process reads. Although that configuration
is indeed common, that statement neglects the issue of replication lag.
Even though distributing reads across follower nodes can optimize performance,
these nodes may not always reflect the most recent data due to the inherent delay in
propagating updates. To ensure access to the most up-to-date data, reads must be
directed to the leader node.
Summary
¬° Redundancy aims to improve the reliability of a system, growing beyond the reli-
ability limits of a single resource.
¬° Redundancy refers to the duplication and coordination of subsystems so that an
increase in the duplication factor results in increased reliability.
116 chapter 8 Replication
¬° Static redundancy refers to redundancy in which the set of components and their
interactions do not change; dynamic redundancy refers to redundancy in which
they do change.
¬° Replication‚Äîthe employment of multiple instances of the same thing‚Äîis the
most common implementation of duplication.
¬° Replication improves the reliability of distributed systems by distributing data
across multiple resources, overcoming the limitations of a single resource.
¬° Replication lag is an inherent aspect of distributed systems, complicating replica-
tion transparency and consistency.
¬° Synchronous replication ensures consistency but may affect latency and availabil-
ity, whereas asynchronous replication improves latency and availability but may
affect consistency.
¬° State-based replication propagates the current state of the system, whereas log-
based replication propagates the sequence of operations leading to the state.
117
9Consistency
This chapter covers
¬° Consistency
¬° Linearizability
¬° Eventual consistency
¬° The CAP conjecture and theorem
In database systems, consistency has a tangible interpretation: A transaction tran-
sitions the database from one valid state to another valid state, where validity is
defined by application-level constraints (ensuring, for example, that an account bal-
ance is always non-negative).
In distributed systems, components may fail and messages may be delayed, reor-
dered, or lost; consistency can take on multiple interpretations. In this chapter, we‚Äôll
explore several consistency models and discuss common scenarios in which they
arise.
9.1 Consistency models
To focus on consistency, in this chapter we reason about a distributed system as a col-
lection of concurrent processes operating on a collection of objects. A process is a
118 chapter 9 Consistency
sequence of operations on objects. The object‚Äôs type defines the set of possible values
and the set of possible operations to create and manipulate object instances. An opera-
tion is not instantaneous; instead, it is delineated by its invocation and completion. We
model an operation on an object as an invocation and completion pair:
Operation = Invocation ‚Ä¢ Completion
Processes are strictly sequential. That is, each process issues a sequence of operations to
objects, alternately issuing an invocation and waiting to receive the associated response
(see figure 9.1).
Invocation
event
Completion
event
Completion
event
Invocation
event
Op A.1 Op A.1
Op B.1
Process A
Process B
Process A
Process B
Figure 9.1 A process is a sequence of operations on objects. An operation is delineated by its
invocation and completion.
The history of a system is the sequence of operations, including their concurrent struc-
ture, the sequence of invocation and completion events ordered by their real-time
occurrence:
H = [I‚Ä¢A.1, I‚Ä¢B.1, C‚Ä¢A.1, I‚Ä¢A.2, C‚Ä¢B.1, C‚Ä¢A.2]
A consistency model is a set of histories. A consistency
model defines which histories are considered
good, legal, or valid and which histories are con-
sidered bad, illegal, or invalid. In other words, a
consistency model is a predicate on a history‚Äîon
a sequence of invocation and completion events
(see figure 9.2). A system is consistent with respect
to a consistency model if all possible histories of
the system conform to the valid histories defined
by that model.
Aha! moment: Violating consistency?
A common misconception among software engineering is the notion of violating con-
sistency. When we talk about a system violating consistency, what we really mean
Possible history
Good
history
Bad
history
Figure 9.2 A consistency model
classifies histories as good (valid)
and bad (invalid).
119Consistency models
is that the system failed to meet the guarantees of a specific consistency model. In
short, a system cannot violate consistency as a concept; a system can violate only
the terms of a specific consistency model.
9.1.1 Common consistency models
Recall from chapter 1 that correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable, including consistency models. You can define which histories
are good and which histories are bad. Over the past few decades, however, a set of com-
mon, well-defined consistency models has emerged, each with different virtues and
limitations (see figure 9.3).
Legend
Unavailable
Sticky available
Total available
Guarantees are available
on nonfaulty nodes if the
network is not partitioned.
Nodes must pause during
some partitions.
Guarantees are
available on nonfaulty
nodes, even if the network
is partitioned, if each client
continues communicating
with the same node.
Guarantees are available
on nonfaulty nodes, even
if the network is partitioned.
Strict serializable
Serializable Linearizable
Repeatable
read
Cursor
stability
Snapshot
isolation
Read
committed
Monotonic
atomic view
Read
uncommitted
Monotonic
reads
Monotonic
writes
Sequential
Writes
rollow
reads
Read
your
writes
Casual
PRAM
Figure 9.3 Common consistency models and their relationship (Source: https://jepsen.io/consistency)
9.1.2 Virtues and limitations
Consistency models may be disjoint (see the left side of figure 9.4), overlapping
(center), or subsets (right). In the context of consistency models, those that are sub-
sets (include fewer valid histories) are considered stronger; those that are supersets
(include more valid histories) are considered weaker. This creates a hierarchy between
consistency models based on the subset‚Äìsuperset relationship.
120 chapter 9 Consistency
Disjoint Overlapping Subset
Consistency
model A
Consistency
model B
Consistency
model A
Consistency
model B
Consistency model A
Consistency
model B
Figure 9.4 Disjoint, overlapping, and subset consistency models
Modern systems offer the developer a range of consistency levels to choose among.
Figure 9.5 illustrates the available consistency levels in Azure CosmosDB, Microsoft‚Äôs
globally distributed, multimodel database management system.
Strong Bounded staleness Session Consistent prefix Eventual
Stronger consistency Weaker consistency
Higher availability, lower latency, higher throughput
Figure 9.5 Consistency levels of Azure CosmosDB (https://mng.bz/AGRz)
How do you choose among consistency models? What are the tradeoffs between stron-
ger and weaker consistency models? Take a look at figure 9.6. When you shift to the
left‚Äîcommit to a stronger consistency model‚Äîyou lose availability and latency. But
what benefits do you gain in exchange? When you shift to the right‚Äîcommit to a
weaker consistency model‚Äîyou gain better availability and latency. But what might be
compromised in return?
Better developer experience
Stronger consistency
Higher avaiability, lower latency
Weaker consistency
Figure 9.6 Stronger and weaker consistency models
In essence, you have to trade availability and latency for developer experience. On
one hand, a stronger consistency model improves the developer experience by mitigat-
ing more of the effects of distribution. But this mitigation requires more coordination
among components, which limits availability and increases latency. On the other hand,
121Linearizability
a weaker consistency model exposes more of the effects of distribution but requires less
coordination between components, allowing for better availability and lower latency.
You have to strike a balance that aligns with your expectations and requirements.
Although it‚Äôs challenging to quantify, a good developer experience is intuitive and
easy to reason about, leaving no room for unexpected and (especially) unwanted out-
comes. Figure 9.7 illustrates a process A accessing a single read/write (r/w) register x.
Initially, x has the value 0. A sets x to the value of 10. At that point, we expect all subse-
quent reads of x to yield the value 10. In this example, however, the read of x yields the
value of 0.
write(x=10) OK OK(0)read(x)
x x
x=0 x=10
Process A Process A
Figure 9.7 Unexpected and unwanted surprises
Is this behavior correct? The answer depends on our definition of correctness:
¬° If we expect to read the most recent value, the answer is no, this behavior is
incorrect.
¬° If we expect to read any previous value, the answer is yes, this behavior is correct.
Reading the most recent value is formalized by linearizability, whereas reading any pre-
vious (more accurately, intermittent) value is formalized by eventual consistency.
9.2 Linearizability
Linearizability, also known as atomic consistency, stands out as one of the strongest
consistency models. Linearizability provides a recency guarantee: as soon as an opera-
tion completes, all subsequent operations will witness its effects.
Linearizability (https://dl.acm.org/doi/10.1145/78969.78972) was initially intro-
duced in the context of concurrent systems, providing a formal framework for specify-
ing the semantics of shared objects in a multiprocessor environment. When applied to
replicated distributed systems, linearizability can provide a formal framework for speci-
fying the semantics of shared objects in a replicated, distributed environment.
In essence, linearizability ensures that a replicated distributed system behaves like a
nonreplicated, nondistributed system. In other words, linearizability ensures replica-
tion transparency.
Linearizability is a real-time guarantee about single operations (one operation at a
time) on single objects (one object at a time) that is defined per object type (tailored
122 chapter 9 Consistency
to the semantics of the object type). In section 9.2.1, we‚Äôll explore queue and stack
examples.
9.2.1 Queue and stack
As I mentioned in chapter 1, correctness is application-specific. As a software engineer,
you can define the guarantees of your system and decide what behavior is desirable,
tolerable, or intolerable. This idea also applies to object types. Here, we will define two
types: a queue and a stack, each defining an insert operation and a remove operation.
A queue is an object type where the two operations insert and remove preserve first-in,
first-out (FIFO) order on its elements. Here, insert is often referred to as enqueue and
remove as dequeue. For a queue, table 9.1 shows a valid trace and an invalid trace: When
a process enqueues value a and then enqueues value b, we expect a dequeue operation
to yield a and a subsequent dequeue operation to yield b.
Table 9.1 Valid and invalid traces of a queue
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
A stack is an object type where the two operations insert and remove preserve first-in,
last-out (FILO) order on its elements. Here, insert is often referred to as push and
remove as pop.
For a stack, table 9.2 shows a valid trace and an invalid trace. When a process pushes
value a and then pushes value b, we expect a pop operation to yield b and a subsequent
pop operation to yield a. Unlike in a queue, the valid and invalid traces are reversed.
Table 9.2 Valid and invalid traces of a stack
Valid Invalid
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(b)
4. remove ‚Ä¢ return(a)
1. insert(a) ‚Ä¢ return
2. insert(b) ‚Ä¢ return
3. remove ‚Ä¢ return(a)
4. remove ‚Ä¢ return(b)
We define the correct behavior of an object in a single-process, single-object setting.
Linearizability extends this definition by defining a consistency condition that pre-
serves these single-process, single-object semantics across multiple processes interact-
ing with multiple copies of the object.
123Linearizability
9.2.2 Formal definition of linearizability
A history of operations is sequential or concurrent, depending on the relationships
between its operations. A history is said to be sequential if every invocation of an operation
is immediately followed by its corresponding completion. In other words, operations do
not interleave. By contrast, a history is said to be concurrent if it is not sequential‚Äîif the
invocation of one operation may occur before the completion of another operation
invoked earlier. In other words, operations interleave (see table 9.3).
Table 9.3 Sequential vs. concurrent operations
Sequential Concurrent
1. invoke(op1)
2. complete(op1)
3. invoke(op2)
4. complete(op2)
1. invoke(op1)
2. invoke(op2)
3. complete(op1)
4. complete(op2)
A history H is linearizable if
¬° Sequential equivalence‚ÄîThere exists a sequential history H‚Ä≤ such that H‚Ä≤ is equiva-
lent to H‚Äîthat is, H‚Ä≤ and H yield the same results.
¬° Real-time ordering‚ÄîThe ordering of operations in H‚Ä≤ respects the real-time order-
ing of nonconcurrent operations in H‚Äîthat is, any operation that is invoked
after a previous operation completes must witness the effects of the completed
operation.
Although linearizability is defined on a per-object-type basis, meaning that it is custom-
ized to the semantics of the object type, the core principles of sequential equivalence
and real-time ordering are unchanged.
Figure 9.8 illustrates a replicated, distributed, increment-only counter starting at 0.
The history is not linearizable. If the system were linearizable, the read operation of
process A would have returned a value of 2 because the read operation of process A fol-
lows the increment operation of process B. Instead, the system exposes the presence of
multiple replicas, including its replication lag. As a result, the read operation of process
A returns a stale value of 1.
Aha! moment: Linearizable history vs. linearizable system
Note that the term linearizability is used differently when applied to the history of
an execution and when applied to a system. If we say a history is linearizable, we
assert that an equivalent sequential history exists. In this case, the term linearizabil-
ity denotes a fact about a history. If we say a system is linearizable, we assert that
every possible history of the system is linearizable. In this case, the term linearizable
denotes a guarantee about a system.
124 chapter 9 Consistency
Replica 1 Replica 1
Process A Process A
Replica 2 Replica 2
Process B Process B
inc OK OK(1)r(x)
inc OK OK(2)r(x)
Figure 9.8 Nonlinearizable history of a replicated, distributed, increment-only counter
How can we implement a replicated, distributed system that guarantees lineariz-
ability? In chapter 10, we will look at one of the most popular options: consensus
algorithms.
9.3 Eventual consistency
Eventual consistency stands out as one of the weakest consistency models. Eventual
consistency provides a convergence guarantee: eventually, all nodes are in the same
state. Werner Vogels, chief technology officer of Amazon Web Services (AWS), pop-
ularized the term eventual consistency to describe the consistency model in Amazon‚Äôs
DynamoDB database system. To understand the practical implications of eventual
consistency, let‚Äôs explore a real-world example often cited by Vogels: the Amazon
shopping cart.
9.3.1 The shopping cart
Consider the shopping cart illustrated in figure 9.9. This cart is essentially a collection
of items, with operations to add and remove items.
Replica 1
Replica 2
Replica 1
Replica 2
A
A
B
A, C
A, B, C
A, B, C
remove (A) ‚Ä¢ add(B)
add(C)
Figure 9.9 Eventual-consistent shopping cart
125Eventual consistency
Initially, we have two identical replicas of the cart, each containing one item: A. Should
a network partition occur, these replicas lose communication. Consequently, any
change in one replica remains unseen by the other. In such a scenario, imagine that
a client removes item A and introduces item B to the first replica while another client
adds item C to the second. Upon restoration of the network connection, the replicas
strive to reach a consistent state.
A challenge arises, however: the same cart has been altered differently across repli-
cas, leading to a conflict requiring resolution. Finding a resolution falls on the software
engineers.
In Amazon‚Äôs approach to this dilemma, the priority is clear: retain items added by the
user over items they removed. This method ensures that no added items vanish, even
though removed items occasionally resurrect. This example highlights the tradeoffs
between availability and a delightful developer experience‚Äînot to mention a delight-
ful user experience devoid of surprises.
9.3.2 Variants of eventual consistency
Eventual consistency comes in two flavors: basic and strong.
basic eventual consistency
Basic eventual consistency is defined in terms of these two guarantees:.
¬° Eventual delivery‚ÄîAn update made to one nonfaulty replica is eventually deliv-
ered to every other nonfaulty replica.
¬° Weak convergence‚ÄîAfter no further updates occur, all replicas eventually con-
verge to the same state.
This version does not constrain system behavior when updates are made continuously
or the values that read operations may return before convergence.
strong eventual consistency
Strong eventual consistency is defined in terms of these two guarantees:
¬° Eventual delivery‚ÄîThis guarantee is the same as in basic eventual consistency.
¬° Strong convergence‚ÄîAny two replicas that received the same set of updates are in
the same state.
This version constrains the system‚Äôs behavior such that two replicas will be in the same
state if they see the same updates.
9.3.3 Implementation
Eventual consistency requires conflict resolution because clients can modify any
replica at any time, even when the replicas are not in communication. How are these
concurrent updates reconciled?
application-specific reconciliation
Historically, reconciliation has been managed at the application level. In the Amazon
shopping-cart example, the application gives precedence to newly added items over
removed items during conflict resolution.
126 chapter 9 Consistency
algorithmic reconciliation
Recently, a family of algorithms has gained prominence in automated conflict resolu-
tion: conflict-free replicated data types (CRDTs). CRDTs are data structures designed
to allow independent and concurrent updates across multiple replicas while ensuring
eventual state convergence.
Operation-based CRDTs, also known as commutative CRDTs, replicate individual opera-
tions across replicas. Replicas receive the updates and apply them locally. To guarantee
convergence, the system must ensure that operations are not lost, each operation is
applied effectively once, and the apply function is commutative.
State-based CRDTs, also known as convergent CRDTs, replicate the entire state across rep-
licas. Replicas receive updates and merge them locally. To guarantee convergence, the
merge function must be associative, commutative, and idempotent. A straightforward
example of a CRDT is the grow-only set, a set with one operation: add (see table 9.4).
Table 9.4 Operation-based and state-based grow-only set
Operation-based grow-only set State-based grow-only set
// apply adds an element to
// the set
on init do
set := { }
end
on receive (add x) do
set := set ‚à™ { x }
end
// merge takes the union of
// two sets
on init do
set := { }
end
on receive (set') do
set := set ‚à™ set'
end
Today, CRDTs are an active area of research. Many modern distributed systems and
databases, including Riak (https://riak.com/index.html) incorporate CRDT princi-
ples to achieve high availability and low latency.
9.4 Consistency, availability, and partition tolerance
Consistency, availability, and partition tolerance (CAP) is a framework for understanding
the tradeoffs between very specific definitions of these three key properties of repli-
cated, distributed systems.
CAP is possibly the best-known but most-misunderstood principle in distributed sys-
tems. In this section, we will discuss and explore CAP in great detail, so the next time
someone drops the CAP bomb on you, you‚Äôll be well prepared to withstand the meta-
phorical blast.
9.4.1 History
When talking about CAP, we have to discern the difference between the original CAP
conjecture and the subsequent CAP theorem. In 2000, Eric Brewer introduced the
127Consistency, availability, and partition tolerance
CAP conjecture during his keynote address ‚ÄúTowards Robust Distributed Systems‚Äù
(https://mng.bz/V96P) at the Principles of Distributed Computing (PODC) confer-
ence. Brewer posited that a distributed system cannot achieve all three of the following
properties simultaneously:
¬° Consistency
¬° Availability
¬° Partition tolerance
Although Brewer presented it as a theorem, at that time his interpretation of CAP was
merely conjecture because he offered no formal proof in its support. In 2002, Seth
Gilbert and Nancy Lynch published a formal proof in ‚ÄúBrewer‚Äôs Conjecture and the
Feasibility of Consistent, Available, Partition-Tolerant Web Services‚Äù (https://mng.bz/
xZYW), rendering their interpretation of CAP a theorem.
NOTE Brewer‚Äôs CAP conjecture is an attempt to formulate the conflict between
consistency and availability, whereas Gilbert and Lynch‚Äôs CAP theorem is an
attempt to formalize the conflict between consistency and availability.
Aha! moment: Pick two out of three
CAP was originally presented as a ‚Äúpick two out of three‚Äù framework, an interpretation
that has since fallen out of favor. This interpretation implies that network partitions
are optional. As discussed in chapter 1, however, network partitions are inevitable in
a realistic system model. Therefore, the ability to tolerate partitions is non-negotiable
rather than optional.
Because you have to account for network partitions, you have to choose between con-
sistency and availability if and when a partition occurs. In other words, CAP divides
the world into CP and AP systems.
9.4.2 Conjecture vs. theorem
CAP states that any replicated system can guarantee consistency or availability only
under partitioning. But the conjecture and the theorem are based on different inter-
pretations (see table 9.5). Both the conjecture and the theorem define consistency as a
safety property and availability as a liveness property of the system:
¬° Consistency (informally)‚ÄîEvery read request receives a response indicating suc-
cess and reflecting the value of the most recent write request or a response indi-
cating failure.
¬° Availability (informally)‚ÄîEvery read request eventually receives a response
indicating success (not necessarily reflecting the value of the most recent write
request).
128 chapter 9 Consistency
Also, both the conjecture and the theorem define a network partition as a failure mode
of the underlying system model. In a network partition (informally), a network parti-
tion divides the network into segments, and messages sent from nodes in one segment
to nodes in other segments are lost.
Table 9.5 Conjecture vs. theorem
Conjecture Theorem
Consistency Single-copy serializability Linearizability of r/w register
Availability Some node responds timely. Any node responds eventually.
Partitioning Temporary Permanent
9.4.3 CAP theorem
After Brewer published the CAP conjecture, Gilbert and Lynch published the CAP the-
orem, which provides a formal, mathematical articulation within the specific assump-
tions and constraints of their system model.
system model
Figure 9.10 illustrates Gilbert and
Lynch‚Äôs system model. The system con-
sists of three nodes: C, N1, and N2. Nodes
have no access to clocks and communi-
cate by sending and receiving messages
over an asynchronous network.
Let‚Äôs consider the three nodes, C, N1,
and N2. N1 and N2 start with the same
value, v0. Let‚Äôs also consider a perma-
nent network partition between N1
and N2. Communication is still possible
between C and both N1 and N2 but not
between N1 and N2.
the proof
To demonstrate the impossibility of achieving consistency, availability, and partition tol-
erance simultaneously, Gilbert and Lynch employed a proof by contradiction. Assume,
for the sake of contradiction, that an algorithm exists, allowing the system to be both
consistent and available under this network partition.
the algorithm‚Äôs failure
Under these conditions, if a write occurs on N1 and a read occurs on N2, the read
operation cannot return the most recent value (v1), thereby violating the assumed con-
sistency. Therefore, we reach a contradiction. Our initial assumption that this system
could be both consistent and available is proved to be false:
C
N 1 V0
V0N 2
Figure 9.10 Gilbert and Lynch‚Äôs system model
129Summary
¬° C sends a single write request to N1 setting the value to v1:
send(w, v1) receive(w, v1) send(ack) receive(ack)
¬° C sends a single read request to N2:
send(r) receive(r) send(v0) receive(v0)
NOTE In the case of a permanent partition, no information can flow from
one segment to another. Therefore, even the weakest form of consistency is
impossible in a system with a permanent partition. Martin Kleppmann pro-
vides a more careful formalization of CAP in ‚ÄúA Critique of the CAP Theorem‚Äù
(https://arxiv.org/abs/1509.05393).
critique
The conjecture and the theorem have key differences with far-reaching consequences:
¬° Brewer‚Äôs original interpretation of CAP is intuitive and practical, but it is not a
theorem.
¬° Gilbert and Lynch‚Äôs subsequent interpretation of CAP is a theorem, but it is not
intuitive or practical.
The CAP theorem has had both positive and negative effects on the distributed-
systems community. On the positive side, CAP has taught us to think about the inher-
ent tradeoffs in distributed systems. On the negative side, CAP is sometimes misused as
a definitive argument to shut down a conversation, even when CAP may not be applica-
ble to the design challenges at hand.
Although many software engineers cite the CAP theorem to justify their design deci-
sions, a comprehensive understanding reveals that the CAP theorem applies in theory
but may not apply to the design challenges at hand in practice.
Kleppmann said in his blog post ‚ÄúPlease stop calling databases CP or AP‚Äù (https://
mng.bz/Z94P), ‚ÄúThe CAP theorem is too simplistic and too widely misunderstood to be
of much use for characterizing systems. Therefore I ask that we retire all references to
the CAP theorem, stop talking about the CAP theorem, and put the poor thing to rest.
Instead, we should use more precise terminology to reason about our tradeoffs.‚Äù
Summary
¬° Consistency models allow developers to choose the guarantees that best align
with their application‚Äôs requirements.
¬° Consistency models separate system histories (traces) into valid (correct) and
invalid (incorrect) histories.
¬° Stronger consistency models improve the developer experience but reduce avail-
ability and increase latency; weaker models prioritize availability and latency at
the expense of developer experience.
130 chapter 9 Consistency
¬° Linearizability, one of the strongest consistency models, ensures that operations
on distributed and replicated systems look like the systems are not distributed or
replicated.
¬° Eventual consistency, one of the weakest models, guarantees that all repli-
cas eventually converge to the same state, often requiring conflict resolution
through application logic or algorithms like CRDTs.
¬° CAP describes the tradeoffs between consistency, availability, and partition toler-
ance, demonstrating that systems must compromise on consistency or availability
during network partitions.
¬° Brewer‚Äôs interpretation of CAP is intuitive and practical but not a theorem,
whereas Gilbert and Lynch‚Äôs interpretation is a theorem, but not intuitive or
practical.
131
10Distributed consensus
This chapter covers
¬° Consensus
¬° State machine replication
¬° Raft consensus protocol
Distributed consensus is a foundational abstraction of distributed systems. Long
believed impossible to achieve, distributed consensus serves as a cornerstone for
building reliable and scalable distributed systems. It allows a group of redundant
processes to advance in lockstep to act as one so that at any time, some processes in
the group can compensate for the failure of others.
Failing to reach consensus is often considered catastrophic for the application at
hand. Did the transaction commit or abort? Did operation A happen before opera-
tion B? Was the lock acquired by component 1 or component 2? Any disagreement
on these questions quickly results in incorrect behavior. Therefore, the consensus
problem has garnered outsize interest in both the theoretical and practical realms of
software engineering.
132 chapter 10 Distributed consensus
10.1 The challenge of reaching agreement
At first glance, the concept of distributed consensus appears straightforward. Distrib-
uted consensus requires a set of concurrent components‚Äîin this context often called
processes‚Äîthat communicate by sending and receiving messages over a network to
agree on a single value. Although the problem statement of distributed consensus is
simple, finding a solution in a realistic system model is notoriously difficult. Consensus
is trivial to achieve in a theoretical system model, where components are not subject
to failure and the network delivers messages exactly once and in order. But consensus
is difficult to achieve in a realistic system model, where components may fail and the
network may reorder, delay, lose, or duplicate messages.
In chapter 8, we encountered a consensus problem in the form of atomic com-
mitment: A set of resource managers must agree on whether to commit or abort a
transaction. When we discussed the Two-Phase Commit protocol and its behavior in
the presence of failure, we realized that a failure of one component‚Äîthe transaction
coordinator‚Äîmay block the protocol, guaranteeing safety but sacrificing liveness.
Consensus algorithms guarantee the safety of a system and aim to provide‚Äîbut do not
guarantee‚Äî the liveness of the system. No single component failure can bring the sys-
tem to a halt.
Aha! moment: Impossibility theorem
The Fischer, Lynch, Paterson (FLP) Impossibility Theorem (https://mng.bz/Rw2K),
credited to Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, asserts that
in an asynchronous distributed system without clocks, it is impossible to guarantee
consensus under all circumstances, even if only a single node can fail. Given the dire
result of this theorem, we may wonder how we can realistically solve the consensus
problem or expect consensus algorithms to succeed.
The theorem principally addresses the inherent limitations of ensuring liveness in
asynchronous distributed systems without clocks. It does not rule out the possibil-
ity of achieving consensus under more constrained system models. In an asynchro-
nous system model with clocks, for example, consensus is possible (https://mng
.bz/26w0).
10.2 System model
In this chapter, a distributed system is a collection of processes (Processes) and a set
of values (Values). A process p may fail. Here, a failure is a crash failure‚Äîthat is, a pro-
cess simply halts. A process p may propose a value v, represented as Propose(p, v), and
a process may decide on a value Decide(p, v)
A consensus algorithm is any algorithm that ensures the safety properties of validity,
integrity, and agreement and the liveness property of termination. Although validity
and termination feel a bit academic, agreement and integrity neatly capture our intu-
ition of consensus: all processes agree on the same value and do not go back on their
133State machine replication
word. Table 10.1 introduces the symbols ‚àÄ and ‚àÉ, used in the formal definition that
follows the table.
Table 10.1 Symbols in set theory
Symbol Meaning
‚àÄ For all
‚àÉ There exists
¬° Safety properties‚Äî
‚Äì Validity‚ÄîIf a process decides on value v, value v was proposed by some process:
‚àÄ p ‚àà Processes, ‚àÉ q ‚àà Processes, ‚àÄ v ‚àà Values:
Decide(p, v) ‚áí Propose(q, v)
‚Äì Integrity‚ÄîIf a process decides on value v, the decision is permanent:
‚àÄ p ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(p, v‚Ä≤) ‚áí v = v‚Ä≤
‚Äì Agreement‚ÄîNo two correct processes decide on different values:
‚àÄ p, q ‚àà Processes, ‚àÄ v, v‚Ä≤ ‚àà Values:
Decide(p, v) ‚àß Decide(q, v‚Ä≤) ‚áí v = v‚Ä≤
¬° Liveness properties‚Äî
‚Äì Termination‚ÄîEvery nonfailed process eventually decides on a value v:
‚àÄ p ‚àà { p ‚àà Processes ‚à£ alive(p) }, ‚àÉ v ‚àà Values:
‚óä ‚ñ° (Decide(p,v))
10.3 State machine replication
Why is distributed consensus so significant in the context of distributed systems? Dis-
tributed consensus forms the foundation of state machine replication, a technique
that mechanically transforms processes into fault-tolerant groups of processes. Con-
sensus algorithms enable processes to advance in lockstep, ensuring fault tolerance by
allowing the group to compensate for any failed member (see figure 10.1).
State machine replication operates on a simple principle: if two identical, determinis-
tic processes begin in the same state and receive the same inputs in the same order, they
will produce the same output and reach the same state. In other words, the challenge of
134 chapter 10 Distributed consensus
Make a collection of physical components
behave like one logical component.
The resulting logical component is able to
withstand the loss of one physical component.
Consensus
Figure 10.1 Transforming
a process into a fault-
tolerant process
ensuring that multiple processes progress in lockstep can be reduced to the challenge
of reaching consensus on the log that supplies their inputs.
Figure 10.2 illustrates state machine replication to implement a replicated key-value
store. A consensus protocol ensures that every replica receives the same commands in
the same order, resulting in a reliable key-value store that can compensate for the crash
of a node.
Process 2 Process 1Process 2
Platform level Application level
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
x := 0
y := 1
z := 2
State machine
replica 1
State machine
replica 2
State machine
replica 3
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
x = 0
y = 1
z = 2
Consensus
subsystem
Consensus
subsystem
Consensus
subsystem
Figure 10.2 State machine replication in action
10.4 The origin‚Äîand irony‚Äîof consensus
Leslie Lamport, who received the Turing Award in 2013 for his seminal contributions
to the theory and practice of concurrent and distributed systems, initially set out to
prove that achieving fault-tolerant consensus in a distributed system was impossible.
135Implementing consensus
But in a remarkable twist of fate, instead of proving that fault-tolerant consensus is
impossible, in 1989 Lamport devised and published what is often considered the first
fault-tolerant consensus algorithm: the Paxos protocol. This protocol presents consen-
sus as a single decision, ensuring that a set of write-once registers (the replicas) all get
set to the same value, even in the face of failures and communication over an unreli-
able network.
Although the protocol is groundbreaking, Paxos has limitations. Lamport aptly
demonstrated the fundamentals of consensus for a single decision, but real-world
distributed systems often require consistency across a series of decisions, not just iso-
lated instances. Multi-Paxos builds on the principles of Paxos to achieve consensus on
a series of decisions, rendering Multi-Paxos suitable for implementing state machine
replication.
Aha! moment: Viewstamped Replication
In 1988, Brian Oki and Barbara Liskov published the Viewstamped Replication algo-
rithm in their paper ‚ÄúViewstamped Replication: A New Primary Copy Method to Sup-
port Highly-Available Distributed Systems‚Äù (http://pmg.csail.mit.edu/papers/vr.pdf).
Although Viewstamped Replication did not garner the same level of recognition and
fame as Paxos, and the paper did not explicitly use the term consensus, the algorithm
presented addressed the same problem. Therefore, Viewstamped Replication‚Äînot
Paxos‚Äîcan be regarded as the first published algorithm that solved the problem of
distributed consensus.
10.5 Implementing consensus
Many well-known consensus algorithms, including Multi-Paxos, Raft, and Viewstamped
Replication, combine the idea of a leader with the idea of a quorum.
10.5.1 Leader-based consensus
Achieving consensus is straightforward in a theoretical system model, where compo-
nents never fail and the network reliably delivers messages once and in order. A simple
approach is to appoint a process known as the Benevolent Dictator for Life (BDFL), in
which a single individual has the final say in decision-making. This designated BDFL
processes each request, makes decisions, and broadcasts these decisions to all other
processes. In this ideal environment, where processes cannot fail and messages cannot
get lost, duplicated, or reordered, consensus is guaranteed.
By contrast, achieving consensus is challenging in a realistic system model, where
components may fail and the network may lose, duplicate, or reorder messages. Simply
appointing a BDFL is untenable: if the BDFL fails, the entire system is unable to make
any decisions. Furthermore, even with a functioning BDFL, network inconsistencies
such as a lost message could prevent decisions from being communicated to all pro-
cesses, resulting in system inconsistencies. We need a more robust approach.
136 chapter 10 Distributed consensus
10.5.2 Quorum-based consensus
A common approach to implementing consensus involves quorum-based algorithms.
In a quorum-based algorithm, consensus is reached only when a majority of nodes,
known as a quorum, acknowledges a given decision. These algorithms operate on the
principle of majority rule to ensure that decisions are made by and known to a majority
of processes.
In the context of distributed systems, a quorum is a subset of processes where the
cardinality of the subset is strictly greater than half the total number of processes: Q >
N/2. This ensures that any two quorums will always have at least one process in com-
mon, a property critical for maintaining system consistency. In practical terms, if we are
required to compensate for f failed processes, we need at least f+1 nonfailed processes
to establish a quorum. Summing these, we get N = f + (f+1), which simplifies to N =
2*f+1, the often-cited formula that represents the minimum total processes needed to
tolerate f failures and maintain the ability to achieve consensus.
The overlap of quorums serves to prevent splitbrain conditions. In a splitbrain sce-
nario, two or more subsets of the nodes in a distributed system operate simultaneously
but independently, leading to inconsistency. The quorum intersection property guar-
antees that at least one node will be aware of the decisions made in either of the over-
lapping quorums, thus serving as a single point of coordination to prevent splitbrain
conditions (see table 10.2).
Table 10.2 Possible quorums for { p1, p2, p3 }
{ p1, p2, p3 }
{ p1, p2 } { p3 }
{ p1, p3 } { p2 }
{ p2, p3 } { p1 }
10.5.3 Combining leader and quorum
In algorithms like Multi-Paxos, Raft, and Viewstamped Replication, the leader, elected
through a quorum, proposes values. The leader awaits acknowledgments from a quo-
rum of nodes before committing a value, ensuring that a majority is aware of and
agrees on the proposed value. This approach combines the decisiveness of a leader
with the reliability of a quorum, addressing the challenges associated with unreliable
processes and unreliable networks.
10.6 Raft
In this section, we explore Raft, which is a consensus protocol designed for managing
replicated logs across a cluster of nodes, forming the foundation of state machine rep-
lication. The protocol relies on a leader to orchestrate message sequencing (deciding
137Raft
the order of log entries) and a quorum to ensure message commitment (deciding the
inclusion of entries in the log).
The Raft consensus algorithm is popular for its emphasis on understandability. But
despite its design focus on understandability, Raft remains a complex algorithm. To deepen
our understanding of Raft, we will tackle three Raft puzzles at the end of this section.
NOTE This section discusses the Raft consensus algorithm as outlined in the
original paper (https://raft.github.io/raft.pdf). Variants exist; some of these
variants may differ from the protocol discussed here and the puzzles presented
at the end of this section.
10.6.1 The log
Raft‚Äôs objective is to guarantee log consistency across all nodes in the cluster. For any
pair of nodes n1 and n2, their logs, represented by l1 and l2, are considered consistent
if one log is a subsequence of the other, up to the commit index‚Äîthe index of the
highest log entry that is known to be replicated to a quorum of nodes and therefore
considered committed. Log consistency is a prerequisite for state machine safety, Raft‚Äôs
central safety guarantee. The state machine safety guarantee ensures that committed
log entries will never be lost and may safely be applied to the application-level state
machine (see figure 10.3).
Commit index
(must never be lost)
1
2
3
4
5
6
7
8
9
Index
(could be lost)
Figure 10.3 Raft‚Äôs
log abstraction
Raft uses the logs themselves to guarantee log consistency and state machine safety.
Specifically, in case of a leader election, only candidates with the most up-to-date logs
are eligible to become leader.
Aha! moment: Application level vs. platform level
The Raft consensus algorithm essentially has two layers of operation: platform level and
application level. The platform level is tasked with running the core Raft protocol, which
includes activities such as leader election, log replication, and commitment of entries
138 chapter 10 Distributed consensus
(continued)
to the log. When a log entry is committed, the platform level forwards or hands off this
entry to the application level. The application level in turn possesses its own state
machine, which applies the log entry to the application‚Äôs state. In this way, Raft ensures
not just consensus at the platform level but also consistent application-level behavior.
10.6.2 Terms
In Raft, time is delineated in monotonically increasing terms, creating a logical clock
that is central to the protocol‚Äôs correctness. Each term consists of a leader-election
phase and a log-replication phase:
¬° Leader election (Request Vote protocol)‚ÄîThe leader-election subalgorithm
ensures that one and only one leader is active at any given time and a new leader
is elected swiftly in the event of a leader failure.
¬° Log replication (Append Entries protocol)‚ÄîThe log-replication subalgorithm
handles client requests and manages the propagation of log entries across the
cluster.
In summary, per term, first Raft elects a leader; then it delegates authority for manag-
ing the replicated log to the leader. The leader accepts log entries from clients, rep-
licates the log entries to all other nodes, and determines when nodes may apply log
entries to their local state machines (see figure 10.4).
Term 1 Term 2 Term 3 Term 4 Term 5
Election
Log replication
Election
Log replication
Election
Log replication
Election
Log replication
Figure 10.4 The Raft consensus protocol advances in terms, with each term consisting of a leader-
election phase and a log-replication phase.
Aha! moment: Terms as fencing tokens
Although we often assert that Raft guarantees only one leader, we should clarify this
statement. Raft does not guarantee that only one node believes itself to be the leader
at any given time, but it does guarantee that only one node may act as the leader.
139Raft
If two nodes believe themselves to be the leader, a crucial distinction lies in their
term numbers. A previous leader holds a term number lower than the current lead-
er‚Äôs. The term number acts as a fencing token. Nodes compare the term number
of incoming messages with their own current term. Messages with lower term num-
bers are promptly rejected, preserving the guarantee that only one node may act as a
leader at one point in time. Term-based coordination is not only a technical detail but
also a fundamental cornerstone of Raft‚Äôs ability to guarantee correctness.
10.6.3 Leader Election protocol
In the Raft consensus algorithm, the Leader Election protocol is responsible for ensur-
ing that one leader is active at any time and a new leader is elected in the event of a
leader failure. We will explore the Leader Election protocol from the point of view of
one node. A Raft node can be in one of three states: Follower, Candidate, or Leader
(see figure 10.5).
Suspects leader
failure
term := term + 1
term < msg.term
term := term + 1
term < msg.term
term := msg.term
Timeout
term := term + 1
Receives votes
from quorumCandidate
Follower Leader
Figure 10.5 Node states and state transition of the Leader Election protocol
as a folloWer
When a Raft node boots, either for the first time or after recovering from a crash, it
starts as a Follower. In this state, the Follower listens for messages from the Leader.
Each Follower has an election timer set to a random timeout value. This timer resets
whenever the Follower receives a message. When the timer expires, the Follower sus-
pects that the Leader has failed and initiates an election.
as a candidate
To initiate an election, a Follower increments its current term and transitions to the
Candidate state. The Candidate votes for itself and issues RequestVote requests to each
other node in the cluster. Three possible outcomes can occur during the election:
140 chapter 10 Distributed consensus
¬° Quorum vote‚ÄîIf the Candidate receives votes from a quorum of the nodes in the
cluster, including itself, it becomes the Leader for the term.
¬° Split vote‚ÄîIf the Candidate does not receive votes from a quorum of nodes in the
cluster during the election, it does not become the Leader for the term. Instead,
the candidate initiates a new election, and the process repeats until the candi-
date becomes the Leader or learns about a new term and Leader, transitioning
back to being a Follower.
¬° Demotion‚ÄîIf the Candidate learns about a new term and Leader, it transitions
back to being a Follower.
as a leader
When a Raft node believes that it is the Leader and then learns about a new term and a
new Leader, it transitions back to being a Follower.
10.6.4 Log Replication protocol
The Log Replication protocol is responsible for handling client requests and manages
the propagation of log entries across the cluster (see figure 10.6):
1 Accept. The leader accepts a new log entry from a client.
2 Append. The leader appends the new log entry to its own log.
3 Propagate. The leader propagates the log entry to other nodes in the cluster via
Append Entries requests.
4 Commit. Followers receive the Append Entries request, append the entries to
their logs, and send an acknowledgment response to the leader. When the leader
has received acknowledgment responses from a quorum (including itself), the
leader advances the commit index.
AppendCommit
Request r
Ack
Ack
AppendEntries r
Append
Ack
AppendEntries r
Leader Follower Follower
Leader knows r is
replicated to
quorum.
1
2
3
3
4
Append
Figure 10.6 Message flow of the Log Replication protocol
141Raft puzzles
10.6.5 State machine safety
The key safety property for Raft is state machine safety. If a process has applied a log entry
at a given index to its state machine, no other process will ever apply a different log
entry for the same index.
Raft guarantees state machine safety by placing restrictions on which process may be
elected leader: only a candidate with the most up-to-date log may be elected leader. Raft
determines which of two logs is more current by comparing the index and term of the
last entries in the logs:
¬° If the logs have different terms for their last entries, the log with the later term is
considered more up-to-date.
¬° If the logs have the same term for their last entries, then the longer log is consid-
ered more up-to-date.
Given these rules, if a quorum of processes has a log entry, no candidate without that
log entry can become the leader. Consequently, when an entry is added to the logs of a
quorum, that entry will always be present in the logs of any future leaders for that term
or any higher term. This ensures that when a log entry is committed, the entry will
never be lost or overwritten (see section 10.7.3).
10.7 Raft puzzles
In this final section, we will deepen our understanding of the Raft consensus protocol
by exploring three Raft puzzles. Notably, grasping the importance of the third puz-
zle will elevate your comprehension of Raft to that of a true Raft wizard. Each puzzle
involves a Raft cluster with three nodes, and the cluster is on its third term.
Aha! moment: Thinking in algorithms
I enjoy thinking in systems but not in algorithms. For me, thinking in algorithms is akin
to being so focused on the brushstrokes that I fail to see the painting. Thinking in dis-
tributed systems is not only about understanding the algorithms that a system uses
but also about understanding how these algorithms contribute to the system‚Äôs overall
behavior. Many systemic behaviors are implicit consequences of an algorithm and its
interaction with the broader system, which may not be apparent solely from studying
the algorithm itself. Puzzle 3 (section 10.7.3) is a helpful example.
10.7.1 Puzzle 1
Figure 10.7 illustrates the current state of the cluster. Which node is the leader for term
3, and what indicates its leadership?
Answer: Node 2. Node 2 is the leader for term 3. We can infer this from the fact that
the leader is responsible for accepting client requests, adding them to its log, and then
propagating the request to the followers. In this case, node 2‚Äôs log is more advanced,
indicating that it must be the leader for term 3.
142 chapter 10 Distributed consensus
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.7 Puzzle 1
10.7.2 Puzzle 2
Figure 10.8 illustrates the current state of the cluster. Node 2 is the leader in term 3 and
has successfully accepted client request 9, which has been added to its log. But node 2
has not yet propagated this request to any follower. Is it possible that request 9 was lost?
Answer: Yes. If node 2 loses leadership for any reason (because it crashes, for exam-
ple), the new leader in term 4 has no knowledge of request 9. Consequently, if leader-
ship changes, request 9 is lost. Losing request 9, however, does not violate state machine
safety because the request has not been committed yet.
10.7.3 Puzzle 3
Figure 10.9 illustrates the current state of the cluster. Node 2 is the leader in term 3
and has successfully accepted client request 9, which has been added to its log. Node
2 has propagated this request to node 1, which has also been added to its log. Node
1 has sent an acknowledgment, but node 2 has not received the acknowledgment or
advanced its commit index. Node 2 crashes. Is it possible that request 9 was lost?
Answer: No. Losing request 9 does not violate state machine safety because the request
has not been committed yet, but request 9 must not be lost. To understand why, we must
assume the point of view of node 1. Node 1 has sent an acknowledgment and cannot
know whether node 2 crashed before or after advancing its commit index and sending
143Raft puzzles
Commit index
1
2
3
4
5
6
7
8Node 1
Commit index
1
2
3
4
5
6
7
8
9Node 2
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
Figure 10.8 Puzzle 2
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 3
8
Figure 10.9 Puzzle 3
144 chapter 10 Distributed consensus
an acknowledgment to the client. To guarantee state machine safety in all scenarios,
node 1 must assume that request 9 has been committed.
When a new leader is elected, only node 1 is eligible to assume the leadership role
because it has the most up-to-date log. After assuming leadership, node 1 will propagate
the missing entries to node 3 and advance its commit index, including request 9 (see
figure 10.10).
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Commit index
1
2
3
4
5
6
7
8
9Leader
Node 2
Commit index
1
2
3
4
5
6
7
8Eligible
Node 1
9
Commit index
1
2
3
4
5
6
7Node 3
Term 1 Term 2 Term 38
Precommit Postcommit
Figure 10.10 (Left) Leader crashes before commit. (Right) Leader crashes after commit.
Summary
¬° Distributed consensus allows a group of redundant processes to advance in lock-
step via state machine replication.
¬° State machine replication achieves identical outputs by applying identical inputs
in identical order to a group of identical processes.
¬° Achieving consensus in realistic systems with process and network failures is
notoriously challenging. Consensus algorithms such as Viewstamped Replica-
tion, Paxos, and Raft address these challenges.
¬° The Raft protocol is a popular consensus protocol, often praised for its emphasis
on understandability, but it remains complex.
¬° Raft divides finding consensus into leader election and log replication.
145
11Durable executions
This chapter covers
¬° Short-running versus long-running processes
¬° Failure-free definitions
¬° Failure-tolerant executions
¬° Sagas versus durable executions
Durable executions, an emerging concept in software engineering, are to distrib-
uted systems what transactions are to databases: an abstraction concealing the pos-
sibility of failure.
11.1 The pitfalls of partial executions
In the presence of partial failure, even the most basic rules for reasoning about
computations do not hold.
‚ÄîAndrew P. Black, Vincent Cremet, Rachid Guerraoui, and Martin Odersky,
‚ÄúAn Equational Theory for Transactions,‚Äù (https://mng.bz/158j)
Imagine a user registering for a streaming platform for video or music streaming.
During the registration process, the platform handles the user‚Äôs credit card payment
146 chapter 11 Durable executions
and then grants access to its content library. The following listing displays the steps
involved in the signup function.
Listing 11.1 User signup function
async function signup(user) {
const charge = await Payment.create({
...
});
const account = await Account.create({
...
});
}
At first glance, the function may appear to be fine. On closer inspection, however,
we notice a problem: the function handles only the happy path, naively ignoring the
possibility of failure. If the function crashes after charging the credit card but before
updating the database‚Äîthat is, it executes partially‚Äîthe user will be charged but will
not have access to the content.
We‚Äôve run into a fundamental problem in software engineering, illustrated in figure
11.1. The sequential composition of two atomic actions is not itself atomic, leaving us
vulnerable to crash failure that results in partial execution:
t | ‚ö°Ô∏è ‚â° prefix t
Ideally, the execution of a process is failure-transparent, meaning that a failure-free
execution is indistinguishable from a failed execution that was subsequently recovered:
t | ‚ö°Ô∏è ‚â° t or nothing
ba
a ‚Ä¢ b
Figure 11.1 The
sequential composition
of two atomic actions is
not itself atomic.
Aha! moment: Atomicity
These are the two flavors of atomicity:
¬° Concurrency‚ÄîA process is failure-atomic if the process executes observ-
ably equivalent to uninterrupted. Intermediary states are not observable by
Potential crash failure
147System model
other processes. (In the world of databases, this property is referred to as
isolation).
¬° Failure‚ÄîA process is failure-atomic if the process executes observably equiva-
lent to all or nothing. But intermediary states are observable by other processes.
Although concurrency atomicity is often desirable, failure atomicity is often required
to transition the system from a correct state to a correct state (see the figure). In this
chapter, I use the term atomic, which is synonymous with failure-atomic.
CreateCharge
Correctness
predicate holds.
Correctness
predicate holds.
Correctness predicate
does not hold.
State transition, from correct state to correct state via an incorrect intermediary state
11.2 System model
Although the programmer‚Äôs activity ends when they have constructed a correct defi-
nition, the execution taking place under control of their definition is the true subject
matter of their activity, for it is this execution that has to accomplish the desired effect.
‚ÄîE.W. Dijkstra
In our day-to-day conversations, we often blur the lines between definition (the code)
and execution (the running code). When we‚Äôre talking about a function, for example,
we don‚Äôt always specify whether we mean the function definition or the function execution.
To have clear mental models, it is important to distinguish between the terms (see fig-
ure 11.2). Failure-aware and failure-agnostic are related to definitions, whereas failure-free,
failure-tolerant, and failure-transparent are associated with executions.
Definition Execution
Static Dynamic
Development Operations
Interpretation
Figure 11.2 Definition
versus execution
148 chapter 11 Durable executions
11.2.1 Process definition
In this chapter, I define a process definition P as a sequence of steps A, B, and so on,
where each step is a failure-atomic action:
P = A ‚Ä¢ P‚Äô | Œµ
¬° A ‚Ä¢ P‚Äô represents a process that performs an atomic action A and then behaves
like its successor process P‚Äô.
¬° Œµ represents a process that terminates‚Äîessentially only a successful termination.
11.2.2 Process execution
For a process definition P, we define a process execution as the trace t where each step
A of P is an observable event a of t. We assume Crash-Stop failures (a process execution
may stop at an arbitrary moment):
If P = A ‚Ä¢ B ‚Ä¢ Œµ then t = √ó or a ‚Ä¢ √ó or a ‚Ä¢ b ‚Ä¢ ‚úì
¬° ‚úì represents a successful execution.
¬° √ó represents a crash failure.
Trivially, if a process definition consists of a single step, a process execution inherits the
failure atomicity of its step (see figure 11.3).
a
Figure 11.3 A process P consists of one step: a.
But if a process definition consists of multiple steps, a process execution does not
inherit failure atomicity. Failure atomicity does not compose (see figure 11.4).
ba
Figure 11.4 A process P consists of two steps: a and b.
Now that we have a model for process definitions and process executions, let‚Äôs discuss
how executions recover from failures.
149The concept of failure-transparent recovery
Aha! moment: Short- vs. long-running executions
In their paper ‚ÄúSagas‚Äù (https://mng.bz/Pwjn), Hector Garcia-Molina and Kenneth
Salem introduce the concept of short-running and long-running executions. They
define short-running (or short-lived) as execution that consists of a single step and
long-running (or long-lived) as execution that consists of multiple steps. Therefore,
short-running or long-running is not a statement about physical time but a statement
about logical time, where time is delineated by steps (see the figure).
a
a
t
a b
a b
tShort-running Long-running
Short-running vs. long-running
11.3 The concept of failure-transparent recovery
Recovery of a process is considered failure-transparent if a failure-free execution pro-
duces a sequence of events equivalent to that of a failed, recovered, and continued exe-
cution. In other words, transient or intermittent failures cannot prevent an execution
from running to completion (though permanent failures can).
NOTE This chapter considers only forward recovery. For a discussion of back-
ward recovery and compensation, see chapter 3.
All discussion of failure-transparent recovery occurs in the context of a specific equiva-
lence function. The equivalence of two sequences of events is determined by an equiva-
lence function. This function is specific to the application, and as software engineers, we
have the authority to decide when two sequences of events are equivalent. Consider the
following function that prints the letters a to f, where each print statement is a step.
Listing 11.2 Printing letters a to f
function alphabet() {
print("a");
print("b");
print("c");
print("d");
150 chapter 11 Durable executions
print("e");
print("f");
}
¬° The process definition is
P = A ‚Ä¢ B ‚Ä¢ C ‚Ä¢ D ‚Ä¢ E ‚Ä¢ F ‚Ä¢ Œµ
¬° A failure-free execution is
t = a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
A failure-transparent execution is equivalent to a failure-free execution, as determined
by the chosen equivalence function. Ideally, the equivalence function is the identity func-
tion, meaning that the sequence of events produced by a failed, recovered, and contin-
ued execution is the same as the sequence of events produced by a failure-free execution:
t = a ‚Ä¢ b ‚Ä¢ c üí• d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The identity function, however, amounts to exactly-once processing, and as we saw in
chapter 6, exactly-once processing is not always achievable. An apt and popular equiva-
lence function is the valid to duplicate last event equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
Another apt and popular equivalence function is the valid to duplicate last n events equiv-
alence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
The most extreme variant of valid to duplicate last n events equivalence function is the
restart equivalence function:
t = a ‚Ä¢ b ‚Ä¢ c üí• a ‚Ä¢ b ‚Ä¢ c ‚Ä¢ d ‚Ä¢ e ‚Ä¢ f ‚Ä¢ ‚úì
11.4 Strategies of failure-transparent recovery
In practice, to ensure that a system behaves correctly in case of event duplication,
actions must be idempotent. Idempotence refers to the property of an action when per-
forming the action multiple times yields the same result as performing the action once
(see chapter 6). When a failure interrupts a process execution, the system must be able
to recover and continue the process execution. There are two prevalent patterns for
recovering and continuing an execution: restart and resume.
11.4.1 Restart
Process executions recover from failures by restarting the process execution from the
beginning. A process restart is simple to implement if the process steps are idempo-
tent. Restarting has some limitations, however. In the initial user signup scenario, for
example, we intended to collect payment and create the account at the same time, as
shown in the following listing.
151Strategies of failure-transparent recovery
Listing 11.3 Creating account and collecting payment
async function signup(user) {
const payment = await Payment.create(...);
const account = await Account.create(...);
}
If we intended to create the account but delay collecting payment, a simple restart
would not be sufficient. If the system restarted the entire signup process due to a failure,
the restart would reset the one-week delay for payment, as shown in the next listing.
Listing 11.4 Creating account and delaying payment collection
async function signup(user) {
const account = await Account.create(...);
setTimeout(async () => {
const payment = await Payment.create(...);
}, OneWeek);
}
Another example in which restarting may not be ideal is nondeterministic action. The
difference between retrieving the time of day and generating a random number, for
example, may not yield the correct behavior. Restarting may result in incorrect behav-
ior when a process involves nondeterministic actions, such as retrieving the current
time or generating a random number, potentially leading to a different outcome.
11.4.2 Resume
Process executions recover from failures by restarting the process execution from their
most recent save point. By capturing sufficient information between process steps, a
process execution effectively resumes after a failure (see figure 11.5).
a b
Load
state
Save
state
Load
state
Save
state
Figure 11.5
Resuming process
execution after failure
152 chapter 11 Durable executions
11.5 Implementation of failure-transparent recovery
Implementing failure-transparent reco-
very can be achieved at two levels: appli-
cation and platform. Application-level
recovery requires process definitions
to be failure-aware, explicitly han-
dling potential failures. By contrast,
platform-level recovery allows process
definitions to remain failure-agnostic,
containing failure handling within the
platform and abstracting failure han-
dling from developers (see figure 11.6).
11.5.1 Application-level implementation:
Sagas
When failure transparency is imple-
mented at the application level, the
result is failure-aware process defini-
tions. In other words, the developer
of the process is responsible for fail-
ure detection and mitigation and the
resulting process definition‚Äîthe code‚Äîaccounts for the possibility of failure.
Sagas are failure-aware process definitions resulting in failure-transparent process
executions. Although there is no canonical approach to implementing sagas, state-
machine-based implementations are common. In this approach, the state of the saga
and the trigger for the next step are persisted after each step, enabling eventual execu-
tion, as shown in the next listing.
Listing 11.5 Creating an account as a saga
async function signupSaga(state) {
switch (state.label) {
// Step 1: Create Account
case 'createAccount':
const account = await AccountCreate();
return { label: 'createPayment', account };
// Step 2: Create Payment
case 'createPayment':
const payment = await PaymentCreate();
return { label: 'done', payment };
// Handle unknown states
default:
throw new Error('Unknown state');
}
}
Failure-handling ladder
Escalation
Escalation
Failure-oblivious
levels
Failure-oblivious
level
Failure-aware
levels
Failure
containment
Failure
origin
Figure 11.6 Failure handling
153Implementation of failure-transparent recovery
In my opinion, this failure-aware implementation, which accounts for the possibility
of failure, obfuscates the core business logic of the signup process, making it harder to
understand and maintain. By contrast, durable executions do not change the nature of
definitions because they do not need to account for failure explicitly.
11.5.2 Platform-level implementation: Durable execution
When failure transparency is implemented at the platform level, the result is failure-
agnostic or failure-oblivious process definitions. In other words, the developer of the
process is not responsible for failure detection and mitigation, and the resulting pro-
cess definition‚Äîthe code‚Äîdoes not account for the possibility of failure.
Durable executions are failure-agnostic process definitions resulting in failure-
transparent process executions. Today, there are two implementation strategies for
durable executions: log-based and state-based.
log-based implementation
In log-based implementations, the system records the output of each step in a durable
log to replay the process execution. In the event of an execution failure, the run time
restarts the execution but deduplicates previously executed events.
Consider the example of the signup process. In this example, the function signup
is a coroutine function that yields two steps: account creation and payment processing
(see listing 11.6).
Listing 11.6 Creating account and collecting payment
function* signup(user) {
const account = yield function() {
Account.create({...});
});
// Potential Crash Stop
const payment = yield function() {
Payment.create({...});
});
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable log. The run time iteratively executes each step of the coroutine, storing the
result in the durable log. If a step was executed previously (indicated by a nonunde-
fined (that is, defined) entry in durableLog), the run time reuses that value. In effect,
in case of recovery, the run time replays and fast-forwards to the failure point, as shown
in the following code.
Yields a function to execute and
resumes the coroutine with the
result of the function execution
Yields a function to execute and
resumes the coroutine with the
result of the function execution
154 chapter 11 Durable executions
Listing 11.7 Durable execution: Log-based implementation
async function runtime(coroutine, args, durableLog) {
const execution = coroutine(args);
let stepResult = execution.next();
let i = 0;
while (!stepResult.done) {
let val;
if (durableLog[i] !== undefined) {
stepResult = execution.next(durableLog[i]);
} else {
durableLog[i] = await stepResult.value();
execution.next(durableLog[i]);
}
i++;
}
}
This implementation is simple and does not require any specific language or run-time
support. But software developers face notable drawbacks and burdens when they use
log-based implementation. First, log-based implementation works only with determin-
istic execution: on replay, the execution must follow the same path for the execution
and the log to match. Second, the log grows continuously, and for executions with
many steps, the size of the log may become a limiting factor.
state-based implementation
In state-based implementation, the system records the state of the execution (more accu-
rately, its continuation) after each step. In the event of an execution failure, the run time
restores the continuation and continues execution, as shown in the following listing.
Listing 11.8 Durable execution: State-based implementation
async function runtime(coroutine, args, durableState) {
let execution;
// If a durable state exists, load and resume the coroutine,
‚û•otherwise start a new one.
if (durableState) {
execution = load(durableState);
} else {
execution = coroutine(args);
}
// Execute the coroutine steps.
let stepResult = execution.next();
Iteratively executes each step of the
coroutine. stepResult.done is set to
true when the coroutine terminates.
If a step was
executed
previously, use
that value and
resume the
coroutine.If a step was not executed, execute
the step, store the result in the log,
and resume the coroutine.
155Summary
while (!stepResult.done) {
// Save the current state.
durableState = save(execution);
// Execute the current step and get the result.
const value = await stepResult.value();
// Move to the next step.
stepResult = execution.next(value);
}
}
Here, the run-time function takes a coroutine, arguments for the coroutine, and a
durable state. The run time begins execution by either starting a new coroutine or
resuming a previously saved one. As each step is executed, the run time serializes and
saves the entire coroutine state to durableState. In effect, in the case of recovery, the
run time doesn‚Äôt need to replay or fast-forward; it simply reloads the saved state and
then resumes execution where it left off before the failure.
This implementation eliminates the need for determinism and a growing log. But
it requires language and run-time support in the form of serializable continuations‚Äîa
requirement that most run times do not fulfill today. I expect that with the growing
popularity of durable execution, over time, many mainstream run times will offer serial-
izable continuation, either natively via a compiler extension or via libraries. Early exam-
ples include Grain (https://grain-lang.org) and Unison (https://www.unison-lang.org).
Summary
¬° Failure transparency refers to the property of a system in which failure-free execu-
tions are indistinguishable from failed and subsequently recovered executions.
¬° Failure transparency can be achieved at the application level and the platform
level.
¬° At the application level, failure transparency relies on failure-aware process defi-
nitions, resulting in failure-transparent process executions.
¬° At the platform level, failure transparency is achieved through failure-agnostic
process definitions, enabling failure-transparent process executions.
¬° Durable execution is an emerging approach to implementing failure transpar-
ency at the platform level.
¬° Durable execution follows two implementation strategies:
‚Äì Log-based‚ÄîIn log-based implementation, the system records the output of
each step in a durable log and, upon execution failure, replays the process
while deduplicating previously executed events.
‚Äì State-based‚ÄîIn state-based implementation, the system records the state (con-
tinuation) after each step and, upon failure, restores the continuation to
resume execution without replaying steps.
156
12Cloud and services
This chapter covers
¬° Cloud computing
¬° Cloud-native computing
¬° Serverless computing
¬° Services
In the landscape of contemporary distributed systems, cloud computing, cloud-
native computing, serverless computing, and microservices have risen to promi-
nence. But a paradox of popularity emerges: despite the ubiquity of the terms, their
meanings remain somewhat vague and unclear. Therefore, in this final chapter, I
present my mental models for cloud computing, cloud-native computing, serverless
computing, and microservices.
Accurate and concise definitions are essential for conveying complex concepts
effectively. In software engineering, however, there‚Äôs a frequent issue with defini-
tions that are neither accurate nor concise, even for common concepts. With this
in mind, the definitions and descriptions in this chapter strive to be accurate and
concise while being consistent with current industry understanding. It is important
to note that they do not necessarily reflect industry standards.
157Cloud computing
12.1 From proactive to reactive
Back in 2008, when I started working at SAP, one of the world‚Äôs largest software com-
panies, acquiring a single virtual machine was a slow manual process. This process
involved submitting a request for a virtual machine through a support ticket on the
SAP portal. Then I had to wait for management approval, followed by waiting for the
provisioning of the machine. Even when the machine was online, it demanded contin-
uous configuration and maintenance until I eventually requested its decommissioning
in a few months.
By the time I left SAP a decade later, the landscape had changed dramatically. Acquir-
ing a set of virtual machines had become an automated and nearly instantaneous pro-
cess. I could create virtual machines quickly on SAP‚Äôs cloud platform, constrained only
by my allocated budget. Machines were no longer permanent fixtures that needed
configuration and maintenance. Instead, machines appeared and disappeared on an
as-needed basis.
The transition from static to
dynamic resource allocation has
redefined our approach to sys-
tem design. Systems became elas-
tic. Elasticity refers to the ability of
a system to achieve scalability and
reliability by acquiring and releas-
ing resources on demand to meet
demand (see figure 12.1).
Before the advent of the cloud,
our systems consisted of a limited
number of resources that were long-lived and well-known. These resources had long-lived
and well-known properties, including long-lived and well-known identities and network
addresses. Software engineers tried to predict load and failure requirements and address
them proactively. In other words, the topology of our systems was static.
Increasingly, our systems consist of many short-lived resources with short-lived prop-
erties, most prominently short-lived network addresses. Software engineers attempt to
observe load and failure requirements and address them reactively‚Äîthat is, systems
can self-regulate. In other words, the topology of our systems is dynamic. This dynamic
environment serves as not only the backdrop but also the bedrock of cloud computing.
12.2 Cloud computing
In essence, cloud computing separates the world into resource consumers and resource
providers. A resource consumer can acquire and release virtually unlimited resources on
demand from a resource provider: the cloud platform. A cloud platform might enable
a resource consumer to acquire compute resources such as virtual machines or storage
resources such as hard disk drives. When a resource is no longer needed, the resource
consumer may release it.
Elasticity
By acquiring and releasing resources on demand
Scalability Reliability
Figure 12.1 Elasticity in terms of scalability and reliability
158 chapter 12 Cloud and services
A cloud platform can be a public cloud platform (a resource provider that offers
resources outside its own organization) or a private cloud platform (a resource provider
that offers resources only inside its own organization). Well-known examples of public
cloud platforms include Amazon Web Services (AWS; https://aws.amazon.com), Google
Cloud Platform (https://cloud.google.com), and Microsoft Azure (https://azure
.microsoft.com). Well-known examples of private cloud platforms include company-run
OpenStack (https://www.openstack.org), Cloud Foundry (https://www.cloudfoundry
.org), and Kubernetes (https://kubernetes.io) instances.
The hallmark of cloud computing is its distinct division between resource consumers
and resource providers, coupled with the capability to acquire and release resources
on demand. These fundamental aspects are the core primitives of cloud computing. In
the following sections, we will use these characteristics to establish accurate and concise
definitions for cloud-native and serverless computing.
12.3 Cloud-native computing
A cloud application is any applica-
tion hosted on a cloud platform.
By this definition, a vanilla Word-
Press (https://wordpress.org)
installation hosted on a cloud
platform is a cloud application.
But not every cloud application
is a cloud-native application (see
figure 12.2).
A cloud-native application is a
cloud application that is scalable
and reliable by construction,
using the cloud‚Äôs capability to
acquire and release resources
on demand. The concept of by
construction stands in clear con-
trast to by requirement. Due to the
design and implementation of
a cloud-native application, scal-
ability and reliability are guar-
anteed, not merely desired. In
other words, these qualities are
assured rather than requested
(see figure 12.3).
Cloud platforms provide a range of building blocks to enable scalability and reliabil-
ity by construction such as supervisors, scalers, and load balancers. A supervisor mon-
itors a set of resources and acquires a new resource if an existing one is believed to
Applications
Not hosted on a
cloud platform
Noncloud
application
Cloud
application
Hosted on a
cloud platform
Figure 12.2 Noncloud application vs. cloud application
Cloud applications
Not built for a
cloud platform
Non-cloud-native
application
Cloud-native
application
Built for a
cloud platform
Figure 12.3 Non-cloud-native application vs. cloud-native
application
159Serverless computing
have failed. A scaler monitors the set of resources and acquires or releases resources
to match fluctuations in demand. Finally, a load balancer manages the distribution of
requests across available resources. In Kubernetes, for example, a K8s Deployment can
act as the supervisor, a K8s HorizontalPodAutoscaler as the scaler, and a K8s Service as
the load balancer.
Merely placing an existing application on a cloud platform equipped with tools for
scalability and reliability doesn‚Äôt automatically render the application scalable and reli-
able. As discussed throughout this book, in discussions of topics such as failure toler-
ance, partitioning, and redundancy, designing for scalability and reliability requires
a holistic approach; it happens not only at the platform level but also at the applica-
tion level. By this definition, the aforementioned vanilla WordPress instance is not a
cloud-native application because it lacks load and failure detection and mitigation
facilities.
Aha! moment: Lift and shift
Lift and shift describes the process of transferring an existing application, originally
not hosted on a cloud platform, to a cloud environment. This migration typically
involves few or no modifications to the application itself. But the benefits of such a
move can be limited. To capitalize fully on the capabilities of a cloud platform, espe-
cially its elasticity, an application needs to be specifically designed with elasticity in
mind.
Aha! moment: Cloud-native ‚Äútechnologies‚Äù
Common definitions (https://mng.bz/JwBV) of the term cloud-native emphasize that
cloud-native applications are container-packaged, microservice-oriented, and dynam-
ically orchestrated, decorating the result with terms like declarative and immutable.
Although cloud-native applications often possess these characteristics, I believe that
these characteristics alone do not effectively define them. Container packaging, for
example, is a technical choice, whereas being microservices-oriented is an architec-
tural choice. Choosing a different technology or architectural style does not prevent a
cloud application from being scalable and reliable by construction.
12.4 Serverless computing
As in section 12.3, I will define serverless computing in terms of cloud computing and the
cloud primitives. This discussion is based on a minimal model of computation, illus-
trated in figure 12.4.
A system is composed of a set of resources, such as computational resources, network
resources, and storage resources. Also, a system handles a set of application events, here
referred to as requests.
160 chapter 12 Cloud and services
Software
system
Requires
ResourceEvent Figure 12.4 Minimal model
of computation to reason
about serverless computing
You must consider four relevant actions. A system must be able to do all of the following:
¬° Acquire a set of resources
¬° Release a set of resources
¬° Receive events
¬° Process events
As a universal constraint, an event cannot be processed if the required resources have
not been acquired.
12.4.1 Traditional
In a traditional computing environment, the operator of a system must acquire the
necessary resources a priori‚Äîbefore the application event enters the system (see fig-
ure 12.5).
Process
event.
Receive
event.
Acquire
resource.
Release
resource.
Figure 12.5 Order of events in traditional computing. Resource acquisition happens proactively.
12.4.2 Serverless
In a serverless computing environment, the system may acquire the necessary resources
on demand‚Äîafter the application event has entered the system (see figure 12.6).
Process
event.
Acquire
resource.
Receive
event.
Release
resource.
Figure 12.6 Order of events in serverless computing. Resource acquisition happens reactively.
161Service
Consequently, in a serverless environment, the system must be able to determine the
necessary set of resources based on the application event itself.
12.4.3 Cold path vs. hot path
Typical implementations of a serverless environment do not release resources immedi-
ately after processing an application event. Instead, after they are acquired, resources
are held in anticipation of additional application events for some period:
¬° Cold path refers to the situation in which receiving an event and processing an
event are interleaved by acquiring resources (see figure 12.7, left).
¬° Hot path refers to the situation in which receiving an event and processing an
event are not interleaved by acquiring resources (see figure 12.7, right).
Process
event.
Acquire
resource.
Receive
event.
Receive
event.
Process
event.
Release
resource.
Cold path Hot path
Figure 12.7 Cold path vs. hot path
12.5 Service
In the landscape of cloud-based distributed systems, microservices have risen to prom-
inence, eliciting as much enthusiasm as they do debate. When we encounter micro-
services, however, we encounter a paradox of popularity. Despite the ubiquity of the
term microservices, we have only a fuzzy, nebulous mental model of what constitutes a
microservice.
NOTE Invoking the terms microservices and microservices-based architecture often
sparks intense discussions centered on the defining characteristics of excellent
microservices and architectures. My objective is not to critique but rather to
clarify‚Äîto discuss what fundamentally constitutes a microservice. Therefore, I
will use the less emotionally charged term service.
Almost any discussion of a service-based architecture begins with a whiteboard ses-
sion, with rectangles representing services and lines representing interactions‚Äîa
familiar ritual. As the discussion progresses, however, this clarity dissipates. Rectangles
get duplicated, and other rectangles with labels such as Load balancer and Autoscalar
emerge. New shapes that represent databases, queues, and timers are added.
Stepping back, the whiteboard is no longer a model of simplicity but a complex
tapestry. Where are the service boundaries? Is each white rectangle truly an autono-
mous service? When multiple rectangles share a label, do they collectively represent
162 chapter 12 Cloud and services
an autonomous service? Are databases, queues, and timers inside or outside service
boundaries? What roles do load balancers and autoscalers play?
12.5.1 Global view vs. local view
At the beginning of this book, I briefly outlined the ideas of global view and local
view. Recall that we are considering the system from the perspective of an all-knowing
observer. That means we can observe the state of the entire system; we have a global
view (see figure 12.8).
State of
C 1
C 1 C 2
State of
C 2
Network
Figure 12.8 Global point of view: C1 and C2
A component does not have the same luxury. A component can observe only its own
state and its own channel to the network, giving it a limited local view (see figure 12.9).
State of
C 1
C 1 C 2
State of
C 2
Network C2
State of
C2
Network
There is only me... ...and the rest of the system.
Figure 12.9 From C1‚Äôs point of view, there are only C1 and the rest of the system.
When it takes a limited local view, the component C1 does not talk to a service; it talks to
the rest of the system. Simply mapping a service to a component or set of components
does not make sense from that point of view. Instead, from the point of view of C1, a ser-
vice is a contract, specifying the interaction between C1 and the rest of the system.
Consider an echo service. When a component sends an HTTP POST request to
http://echo.io, the component receives a response containing the body of the
request. From our newfound perspective, the component is not addressing another
component via http://echo.io; it‚Äôs invoking a systemwide contract. The component
isn‚Äôt communicating with a specific component located at echo.io; it‚Äôs requesting a
service from the system as a whole.
The service is the contract between the component and the rest of the system; the
service is not the set of components implementing the contract. In other words, we
focus on a logical view instead of a physical point of view.
163Service
12.5.2 Example recommendation service
To develop intuition about a service, let‚Äôs examine a movie-recommendation ser-
vice. The contract of the recommendation service consists of one operation: Get. Get
accepts a single movie id and returns a list of movie ids:
Get: ID -> [ID]
How could we implement this service? We could implement it as a Python program
connecting to a database to retrieve recommendations specific to an id (see figure
12.10).
Although this is functionally correct, this service is neither scalable nor reliable
because one process is neither scalable nor reliable. To improve scalability and reliabil-
ity, we employ redundancy‚Äîthat is, duplicate the process (see figure 12.11).
Recommendation
process
Recommendations
Figure 12.10
Initial model of the
recommendation service
What can we do, however, if the database is overloaded or experiencing a failure? To
address this situation, we can deploy two groups of processes:
¬° Primary‚ÄîReturns dynamic (real) recommendations in case the database is
healthy
¬° Backup‚ÄîReturns static (fake) recommendations in case the database is not
healthy
So although the primary group retrieves recommendations from a database that is
sometimes unavailable, the backup group retrieves recommendations from a local file
that is always available (see figure 12.12).
To distribute requests among processes, we employ a load balancer. Because we are
dealing with two process groups, however, not all processes are equal. Therefore, we
need to employ a smart load balancer. The load balancer may default a request to a
Recommendations
Recommendation
process
Recommendation
process
Figure 12.11 Redundant implementation of the
recommendation service
164 chapter 12 Cloud and services
‚ÄúReal‚Äù
recommendations
Recommendation
process
Recommendation
process
‚ÄúFake‚Äù
recommendations
Recommendation
process
Recommendation
process
Primary BackupPrimary Backup
Figure 12.12 Primary and backup process groups
process in the primary group, but if that process returns an error, it might retry the
request to a process in the backup group. Also, to mitigate load and failure, both pro-
cess groups employ autoscaling (see figure 12.13).
Load balancer
Autoscaler
Recommendation
process
Recommendation
process
Primary Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Figure 12.13 Load balancer and autoscaler
There‚Äôs a challenge, however: the load balancers and autoscalers themselves may not
be immune to load issues or failures. Once again, we address this situation with redun-
dancy (see figure 12.14).
165Service
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Autoscaler
Smart load balancer
Figure 12.14 Redundant load balancers and autoscalers
Figure 12.14, however, suggests a 1:1 relationship between parts of the system that may
not exist. Some instances of an autoscaler, for example, may be responsible for more
than one process group, even across service boundaries (see figure 12.15).
Smart load balancer
Autoscaler
Recommendation
process
Primary
Recommendation
process
Primary
Autoscaler Autoscaler
Recommendation
process
Recommendation
process
Backup Backup
Smart load balancer
Autoscaler
Figure 12.15 Final model of the recommendation service
166 chapter 12 Cloud and services
The recommendation service is implemented by a dynamic, ever-changing set of compo-
nents and their interactions. Some of these components contribute solely to the imple-
mentation of the recommendation service; others contribute to the implementation of
multiple services. Yet the service is still perfectly embodied by its contract of returning a
set of recommendations‚Äîthe contract as the only constant. The consumer may ignore
all the complexity and simply request a set of recommendations from the rest of the
system‚Äîthe collective entity implementing all services available in the system.
12.6 Final thoughts
Imagine a band in which each musician interprets the same musical sheet differently.
When one musician reads a note as a C and another reads it as an F, their combined
efforts, meant to produce harmony, create dissonance.
This analogy mirrors our experience with technology terms like cloud, cloud-native,
serverless, and microservices. Despite the frequent use of these terms, our understanding
of these concepts tends to remain shallow and not aligned with the understanding of
our peers. This limited understanding can undermine our effectiveness as both engi-
neers and communicators.
Figure 12.16 illustrates this challenge. Each of us has a mental model of a concept.
The diagram shows Reality, My Mental Model, and Your Mental Model as overlapping
circles, with the intersection representing our shared, accurate understanding. When
this overlap is small, communication and collaboration become difficult, leading to
misalignment and misunderstandings. When the overlap is large, communication and
collaboration become almost effortless.
...aims to maximize
the intersection.
Thinking in
distributed systems...
My
Mental Model
Your
Mental Model
Reality
Figure 12.16 Thinking in distributed systems aims to maximize the intersection of our mental models
with reality and with one another‚Äôs mental models.
I never feel more empowered or more confident‚Äîas neither engineer nor commu-
nicator‚Äî than when I fully understand a concept. Therefore, I encourage you to
relentlessly explore the core of any concept you encounter. Never stop digging!
167Summary
Summary
¬° Elasticity refers to a system‚Äôs ability to ensure scalability and reliability by dynami-
cally acquiring or releasing resources to match demand.
¬° Cloud computing divides the world into resource consumers and providers,
allowing consumers to acquire and release virtually unlimited resources on
demand through public or private cloud platforms.
¬° Cloud computing fundamentally transformed computing, replacing static, long-
lived system topologies with dynamic, on-demand system topologies.
¬° A cloud application is any application hosted on a cloud platform; a cloud-native
application is a cloud application that is elastic by construction.
¬° Traditional systems acquire resources that are necessary to process events proac-
tively, (before receiving the event).
¬° Serverless systems acquire resources that are necessary to process events reac-
tively (after receiving the event).
¬° Services are contracts between components and the system, focusing on logi-
cal interactions that remain constant instead of physical interactions that may
change over time.
168
index
A
abort trace 77
abstractions 68‚Äì70
accuracy 51
ACID (atomicity, consistency, isolation,
durability) 45, 106
administrative actions 95
agnostic concepts 69
analogies, defined 5
application-level failures 48‚Äì52
apply function 126
asynchronous
replication 113
system 51
asynchronous distributed systems 25‚Äì26
atomicity 72
aware concepts 69
AWS (Amazon Web Services) 124, 158
B
backtrack 106
backward recovery 52
BDFL (Benevolent Dictator for Life) 135
big data 93
BLOBs (binary large objects) 96
blocking protocols 82
Bos, Herbert 68
by construction, concept, defined 158
by requirement, concept, defined 158
C
CAP (consistency, availability, and partition
tolerance) 42, 126‚Äì129
Chandra, Tushar Deepak 50
CHECK clause 73
cloud and services 156
serverless computing
cold path vs. hot path 159, 161
traditional 160
cloud application 158
Cloud Foundry 158
cloud-native application 158
cloud platform 158
cold path 161
Commit phase, Two-Phase Commit protocol 84
commit trace 77
commutative CRDTs 126
completeness 4, 45, 51
complexity of distributed system 2
components 27‚Äì28
concurrency 71, 147
concurrent history 123
169index
conjectures, CAP 127
consensus
algorithm 133
origin of 135
consistency 72, 117
linearizability 121‚Äì124
models 75, 118‚Äì121
consistent hashing 103
convergence guarantee 124
convergent CRDTs 126
correctness 4, 9
CouldNotConnect failure message 48, 51
crash semantics 18
CRDTs (conflict-free replicated data types) 115,
126
D
Data and Reality (Kent) 109
databases 73
demotion, defined 140
directory-based assignment strategy 98
distributed consensus 131
challenges of reaching agreement 132
implementing 135‚Äì136
Raft 137‚Äì141
state machine replication 133
system model 132
distributed systems 1, 8, 56, 92, 118
big ideas 13‚Äì16
mental models for 16
navigating complexity 18‚Äì20
partitioning, overview 92
thinking about 8‚Äì12
distributed transactions 79
atomic commitment 79‚Äì82
essence of 82
Two-Phase Commit protocol 83‚Äì87
duplicate messages 18
duplication, coordination and 108
durability 72, 106
durable executions 145
failure-transparent recovery 149, 150, 152‚Äì155
partial executions 145
system model 147‚Äì148
dynamic partitioning 95
dynamic redundancy 107
E
echo service 162
elasticity 157
encyclopedias and volumes 90
equality 62
equivalence 62
equivalence function 149
Erlang/Open Telecom Platform (OTP) 51
error
budget 12
rate 12
eventual consistency 124‚Äì126
implementation 125‚Äì126
shopping cart example 124
variants of 125
exactly-once processing semantics 55, 62
exchanging messages 56‚Äì58
extended absences 18
external steps 8
F
fail-safe failure tolerance 43
failure, defined 71
failure classification 47‚Äì50
application-level failures 48
intermittent failures 49
permanent failures 50
platform-level failures 48
spatial dimension 47
temporal dimension 48
transient failures 49
failure detection 46, 50
failure mitigation 46, 52‚Äì53
failure recovery 42
failure tolerance 40
in practice 44‚Äì53
theory of 41
types of 42‚Äì44
failure-transparent recovery 149
FIFO (first-in, first-out) 122
FILO (first-in, last-out) 122
FLP (Fischer, Lynch, Paterson) Impossibility
Theorem 132
forward recovery 52
170index
G
Get operation 163
global transaction 82
global view 15
Google Cloud Platform 158
H
hash partitioning 100, 102
heartbeats 51
Helland, Pat 69
history, defined 74
holarchies, defined 14
holon, defined 14
horizontal partitioning 95
hot path 161
I
idempotence 63, 78
if statement 61, 62
immutable, defined 159
insert operation 122
InsufficientFunds failure message 48
intermittent failures 49
internal steps 8
IoT (Internet of Things) 94
isolation, defined 72
item-based assignment strategies 97, 99
hash partitioning 100
range partitioning 100
K
K8s
Deployment 159
HorizontalPodAutoscaler 159
Service 159
Kent, William 109
Kubernetes 18, 158‚Äì159
L
Lamport, Leslie 7, 9
layered architecture 47
leader-based consensus 135
leader election (Request Vote protocol) 138
leaderless systems 115
lift and shift 159
linearizability 121‚Äì124
formal definition of 123
queues and stacks 122
liveness 46
guarantee 82
property 10
local transactions 82
local view 15
log-based
implementation 153
replication 114
logical objects 92
log replication (Append Entries protocol) 138
long-running executions 149
lost messages 18
M
masking failure tolerance 42
means of abstraction 68
means of combination 68
mental model 3‚Äì4
complete 4
correct 4
of software systems 4
overview 3
merge function 126
message delivery and processing 55, 58
case study 64
challenges 60‚Äì62
exactly-once processing semantics 62
exchanging messages 56‚Äì58
idempotence 63
uncertainty principle of 58‚Äì59
message-delivery semantics 18
Microsoft Azure 158
models 3, 5‚Äì8
describing different aspects 6
describing same aspects 5
thinking above code 21
Modern Operating Systems (Tanenbaum and Bos) 68
multileader systems 115
N
networks, component and network behavior
27‚Äì28
nodes 92
nonblocking protocols 82
171index
nonmasking failure tolerance 42
Noop (no operation) 78
O
OpenStack 158
operation-based CRDTs 126
overpartitioning 103
P
partial application 45
partially synchronous systems 26, 51
partitioning 90
consistent hashing 103
encyclopedias and volumes 90
item-based assignment strategies 99‚Äì100
overpartitioning 103
overview 92
rebalancing 103
repartitioning 94‚Äì99, 101
types of 94‚Äì95
Paxos 14, 46
permanent departures 18
permanent failures 50
Petri nets 56
physical objects 92
platform-level failures 48‚Äì53
pneumatic tubes 16
POST request 70, 162
Prepare phase, Two-Phase Commit protocol 84
primitive expressions 68
process 21, 45
definition 148
execution 148
pull scenario 51
push scenario 51
Q
queues 122
quorum 114
quorum-based consensus 136
quorum vote 140
R
race conditions 21
Raft (replicated state machine consensus
protocol) 14, 46, 137‚Äì141
Leader Election protocol 139‚Äì140
log 137
Log Replication protocol 140
puzzles 141‚Äì142
state machine safety 141
terms 138
range partitioning 100, 101
real-time ordering 123
rebalancing 103
receiver component 56
Redo 77
redundancy 107
reliability 12
relocation 97
remove operation 122
reordered messages 18
repartitioning 94‚Äì99, 101
data item to partition assignment strategies
97‚Äì98
hash partitioning 102
range partitioning 101
replication 106, 110
consistency and 109
lag 112
mechanics of 111‚Äì115
redundancy 107
transparency 110
RequestVote requests 139
resource consumer 157
responsiveness 12
restart equivalence function 150
resume 151
RM (resource manager) 79
failure 85
transaction on multiple 81
transaction on single 81
r/w (read/write) register 121
S
safety 46
guarantee 81
property 10
sagas, defined 152
scalability 12
redundancy and 107
semantics 61
172index
sender component 56
sequential
equivalence 123
history 123
serial history 75
serializability 73, 75
serverless computing 159‚Äì161
service-level
indicator 12
objective 12
services 161‚Äì166
example recommendation service 163
global view vs. local view 162
shopping cart example 124
short absences 18
short-running executions 149
SICP (Structure and Interpretation of Computer
Programs) 109
side effect 56
single-leader systems 114
software engineering 2‚Äì4
source identifier 70
spatial dimension 47
split vote 140
stacks 122
state action machine 8
state-based
CRDTs 126
implementation 154
replication 114
state machine replication 133
static partitioning 95
static redundancy 107
Stripe 65
subsystems 13
supervision strategies 51
synchronous
distributed systems 25
replication 113
system 51
system, defined 4
system models 23‚Äì30, 44, 112, 132, 147‚Äì148
asynchronous distributed systems 25‚Äì26
CAP 128
component and network behavior 27‚Äì28
partially synchronous systems 26
realistic system models 30
synchronous distributed systems 25
theory and practice 24
systems of systems, defined 13
T
Tanenbaum, Andrew 68
target identifier 70
TCP (Transmission Control Protocol) 59
TC (transaction coordinator), failure 86
temporal dimension 48
theorems, CAP 127‚Äì129
total application 44
Toueg, Sam 50
Transaction Commit protocol 7
transactions 67, 70‚Äì78
abstractions 68‚Äì70
application-level abort 78
atomic commitment 79‚Äì82
completeness 77
concurrency 71
correctness 74
distributed, Two-Phase Commit protocol 83‚Äì87
failure 71
model of 72‚Äì78
platform-level abort 78
serializability 75
Transaction Undo Log 77
/transfer endpoint 70
transient failures 49
Two-Phase Commit protocol 7, 83‚Äì87
improvement 87
in absence of failure 83‚Äì84
in presence of failure 85‚Äì86
U
uhashring Python package 103
Undo 77
UUID (universally unique identifier) 63
V
variance 97
variants of eventual consistency 125
vertical partitioning 95
Viewstamped Replication 14, 46
W
witness (predicate) 50
A component can observe only its own state and channel to the network.
This gives the component a limited local view.
Certain
Send
request
Uncertain
Certain
Receive
request
Receive
response
Send
response
Process
message
Message
delivery
State
before processing
Message
processing
Message
delivery
State
after processing
Possible
failures
Sender Receiver
This limited view causes the sender to transition from a state of certainty to a state
of uncertainty, with the goal of returning to a state of certainty.
Dominik Tornow
ISBN-13: 978-1-63343-617-6
Almost all modern software is distributed. To create
production-quality applications, you need to think
differently about failure, performance, network services,
latency, resource usage, and much more. This clearly-written
book equips you with the skills and the mindset you need to
design, develop, and deploy scalable and reliable distributed
systems.
In Think Distributed Systems you‚Äôll find a beautifully illustrated
collection of mental models for:
‚óè Correctness, scalability, and reliability
‚óè Failure tolerance, detection, and mitigation
‚óè Message processing
‚óè Partitioning, replication, consensus and more!
This practical book delivers both the big picture view and
ground-level details you need to understand the distributed
systems you‚Äôll encounter on the job. Author Dominik Tornow
breaks down distributed system design into useful categories
like component and network failures, transactions, durable
executions, and distributed consensus. You‚Äôll love how the el-
egant analogies, examples, illustrations, and definitions clarify
even the most difficult concepts.
Dominik Tornow has studied and practiced software systems
engineering over 20 years. He is the founder and CEO of
Resonate HQ, Inc.
For print book owners, all digital formats are free:
https://www.manning.com/freebook
THINK Distributed Systems
OPERATIONS & CLOUD
M A N N I N G
‚ÄúHelps you understand the
key concepts by building the
right mental models. ‚Äù‚ÄîMikolaj Pawlikowski
author of Chaos Engineering
‚ÄúPractical and easily
accessible.
‚Äù‚ÄîJames Cowling, Convex
‚ÄúOff ers realistic models,
engaging stories, and
thoughtful insights.
‚Äù‚ÄîLenny Blum
JPMorgan Chase & Co.
‚ÄúA masterclass in clarity,
precision, and symmetry.
‚Äù‚ÄîJoran Dirk Greef,
Creator of TigerBeetle
‚ÄúDemystifies complex
concepts without
oversimplifying.
‚Äù‚ÄîRam Alagappan, University
of Illinois Urbana Champaign